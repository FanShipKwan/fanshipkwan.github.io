{"meta":{"title":"因缺斯汀","subtitle":"一个怪石的呓语","description":null,"author":"FanShipKwan","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"","slug":"PYTHON数据分析（PANDAS-III-合并数据集）","date":"2016-05-16T14:33:10.906Z","updated":"2016-05-16T14:33:14.663Z","comments":true,"path":"2016/05/16/PYTHON数据分析（PANDAS-III-合并数据集）/","link":"","permalink":"http://yoursite.com/2016/05/16/PYTHON数据分析（PANDAS-III-合并数据集）/","excerpt":"PYTHON数据分析（PANDAS III –合并数据集）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程《PYTHON数据分析》 ##一、实验简介 数据分析和建模方面的大量编程工作都是用在数据准备上：加载、清理、转换以及重塑。有时候，存放在文件或数据库中的数据并不能满足你的数据处理应用的要求。许多人都选择使用普通编程语言（如 Python、Perl、R或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和 Python 标准库提供了一组高级的、灵活的、高效的核心函数和算法，它们使你能轻松地将数据规整化为正确的形式。","keywords":null,"text":"PYTHON数据分析（PANDAS III –合并数据集）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程《PYTHON数据分析》 ##一、实验简介 数据分析和建模方面的大量编程工作都是用在数据准备上：加载、清理、转换以及重塑。有时候，存放在文件或数据库中的数据并不能满足你的数据处理应用的要求。许多人都选择使用普通编程语言（如 Python、Perl、R或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和 Python 标准库提供了一组高级的、灵活的、高效的核心函数和算法，它们使你能轻松地将数据规整化为正确的形式。 ##二、pandas 内置方法 pandas 对象中的数据可以通过一些内置的方式进行合并： pandas.merge 可根据一个或多个键将不同 DataFrame 中的行连接起来。SQL 或其他关系型数据库的用户对此应该会比较熟悉，因为它实现的就是数据库的连接操作。 pandas.concat 可以沿着一条轴将多个对象堆叠到一起 实例方法 combine_first 可以将重复数据编接在一起，用一个对象中的值填充另一个对象中的缺失值 ##三、数据库风格的 DataFrame 合并 数据集的合并（merge）或连接（join）运算是通过一个或多个键将行链接起来。这些运算是关系型数据库的核心。pandas 的 merge 函数是对数据应用这些算法的主要切入点。 我们以一个简单的例子开始： In [68]: df1 = DataFrame({&#39;key&#39;:[&#39;b&#39;,&#39;b&#39;,&#39;a&#39;,&#39;c&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;], &#39;data1&#39;:range(7)}) In [69]: df2 = DataFrame({&#39;key&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;d&#39;], ....: &#39;data2&#39;:range(3)}) In [70]: df1 Out[70]: data1 key 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 a 6 6 b In [71]: df2 Out[71]: data2 key 0 0 a 1 1 b 2 2 d 这是一种多对一的合并。df1中的数据有多个被标记为 a 和 b 的行，而 df2 中 key 列的每个值则仅对应一行。对这些对象调用 merge 即可得到： In [73]: pd.merge(df1,df2) Out[73]: data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 注意，我并没有指明要哪个列进行连接。如果没有指定，merge 就会重叠列的列名当做键。不过最好显示指定一下。 In [74]: pd.merge(df1,df2,on=&#39;key&#39;) Out[74]: data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 如果两个对象的列名不同，也可以分别进行指定： In [75]: df3 = DataFrame({&#39;key1&#39;:[&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;,&#39;a&#39;,&#39;a&#39;,&#39;c&#39;], ....: &#39;data1&#39;: range(7)}) In [76]: df4 = DataFrame({&#39;key2&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;d&#39;], ....: &#39;data2&#39;: range(3)}) In [77]: pd.merge(df3, df4, left_on=&#39;key1&#39;,right_on=&#39;key2&#39;) Out[77]: data1 key1 data2 key2 0 0 a 0 a 1 1 a 0 a 2 4 a 0 a 3 5 a 0 a 4 2 b 1 b 5 3 b 1 b 可能你已经注意到了，结果里面 c 和 d 以及与之相关的数据消失了。默认情况下，merge 做的是inner连接，结果中的键是交集，其他方式还有left、right以及outer。外连接求取得是键的并集，组合了左连接和右连接的效果： In [78]: pd.merge(df1,df2, how = &#39;right&#39;) Out[78]: data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 6 NaN d 2 In [79]: pd.merge(df1,df2, how = &#39;left&#39;) Out[79]: data1 key data2 0 0 b 1 1 1 b 1 2 2 a 0 3 3 c NaN 4 4 a 0 5 5 a 0 6 6 b 1 In [80]: pd.merge(df1,df2, how = &#39;outer&#39;) Out[80]: data1 key data2 0 0 b 1 1 1 b 1 2 6 b 1 3 2 a 0 4 4 a 0 5 5 a 0 6 3 c NaN 7 NaN d 2 In [81]: # 多对多的合并操作非常简单，无需额外的工作 In [82]: df2 = DataFrame({&#39;key&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;b&#39;,&#39;d&#39;], ....: &#39;data2&#39;:range(5)}) In [83]: df1 Out[83]: data1 key 0 0 b 1 1 b 2 2 a 3 3 c 4 4 a 5 5 a 6 6 b In [84]: df2 Out[84]: data2 key 0 0 a 1 1 b 2 2 a 3 3 b 4 4 d In [85]: pd.merge(df1,df2,on=&#39;key&#39;,how=&#39;left&#39;)Out[85]: data1 key data20 0 b 1 1 0 b 3 2 1 b 1 3 1 b 3 4 2 a 0 5 2 a 2 6 3 c NaN 7 4 a 0 8 4 a 2 9 5 a 0 10 5 a 2 11 6 b 1 12 6 b 3 In [86]: # 多对多连接产生的是行的笛卡尔积 In [87]: #要根据多个键进行合并，传入一个由列名组成的列表即可：In [88]: left = DataFrame({&#39;key1&#39;:[&#39;a&#39;,&#39;a&#39;,&#39;b&#39;], ....: &#39;key2&#39;:[&#39;c&#39;,&#39;d&#39;,&#39;c&#39;], ....: &#39;data1&#39;:[1,2,3]})In [89]: right = DataFrame({&#39;key1&#39;:[&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;], ....: &#39;key2&#39;:[&#39;c&#39;,&#39;c&#39;,&#39;c&#39;,&#39;d&#39;], ....: &#39;data2&#39;:[4,5,6,7]}) In [90]: pd.merge(left,right,on=[&#39;key1&#39;,&#39;key2&#39;],how=&#39;outer&#39;) Out[90]: data1 key1 key2 data2 0 1 a c 4 1 1 a c 5 2 2 a d NaN 3 3 b c 6 4 NaN b d 7 对于合并运算需要考虑的最后一个问题是对重复列名的处理。虽然你可以手工处理列名重叠的问题（稍后将会介绍如何重命名轴标签），但 merge 有一个更实用的 suffixes 选项，用于指定附加到左右两个 DataFrame对象的重叠列名上的字符串： 好了，我们来看一看 merge 函数的参数吧： 参数 说明 left 参与合并的左侧 DataFrame right 参与合并的右侧 DataFrame how “inner”、”outer”、”left”、”right”其中之一。默认为”inner” on 用于连接的列名。必须存在于左右两个 DataFrame 对象中。如果未指定，且其他连接键也未指定，则以 left 和 right 列名的交集作为连接键 left_on 左侧 DataFrame 中用做连接键的列 right_on 右侧 DataFreme 中用作连接键的列 left_index 将左侧的行索引用作其连接键 right_index 类似于 left_index sort 根据连接键对合并后的数据进行排序，默认为 True。有时在处理大数据集时，禁用该项可获得更好的性能 suffixes 字符串值元组，用于追加到重叠列名的末尾，默认为（’_x’,’_y’）。例如，如果左右两个 DataFrame 对象都有“data”，则结果就会出现”data_x”和”data_y” copy 设置为 False，可以在某些特殊情况下避免将数据复制到结果数据结构。默认总是复制 ##四、索引上的合并 有时候，DataFrame 中的连接键位于其索引中。在这种情况下，你可以传入 left_index = True 或 right_index = True(或两个都传)以说明索引应该被用作连接键 In [96]: left1 = DataFrame({&#39;key&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;c&#39;], ....: &#39;value&#39;:range(6)}) In [97]: right1 = DataFrame({&#39;group_val&#39;:[3.5,7]},index=[&#39;a&#39;,&#39;b&#39;]) In [98]: left1 Out[98]: key value 0 a 0 1 b 1 2 a 2 3 a 3 4 b 4 5 c 5 In [99]: right1 Out[99]: group_val a 3.5 b 7.0 In [100]: pd.merge(left1,right1,left_on=&#39;key&#39;,right_index=True) Out[100]: key value group_val 0 a 0 3.5 2 a 2 3.5 3 a 3 3.5 1 b 1 7.0 4 b 4 7.0 In [101]: # 由于默认的 merge 方法是求取连接的交集 In [102]: #因此你可以通过外连接的方式得到它们的并集 In [103]: pd.merge(left1,right1,left_on=&#39;key&#39;,right_index=True,how=&#39;outer&#39;) Out[103]: key value group_val 0 a 0 3.5 2 a 2 3.5 3 a 3 3.5 1 b 1 7.0 4 b 4 7.0 5 c 5 NaN In [104]: #对于层次化索引的数据，就有点复杂了 In [105]: lefth = DataFrame({&#39;key1&#39;:[&#39;Ohio&#39;,&#39;Ohio&#39;,&#39;Ohio&#39;,&#39;Nevada&#39;,&#39;Nevada&#39;], .....: &#39;key2&#39;:[2000,2001,2002,2001,2002], .....: &#39;data&#39;:np.arange(5)}) In [106]: righth = DataFrame(np.arange(12).reshape((6,2)), .....: index = [[&#39;Nevada&#39;,&#39;Nevada&#39;,&#39;Ohio&#39;,&#39;Ohio&#39;,&#39;Ohio&#39;,&#39;Ohio&#39;], .....: [2001,2000,2000,2000,2001,2002]], .....: columns = [&#39;event1&#39;,&#39;event2&#39;]) In [107]: lefth Out[107]: data key1 key2 0 0 Ohio 2000 1 1 Ohio 2001 2 2 Ohio 2002 3 3 Nevada 2001 4 4 Nevada 2002 In [108]: righth Out[108]: event1 event2 Nevada 2001 0 1 2000 2 3 Ohio 2000 4 5 2000 6 7 2001 8 9 2002 10 11 In [109]: # 这种情况，你必须以列表的形式指明用作合并键的多个列 In [110]: pd.merge(lefth,righth,left_on=[&#39;key1&#39;,&#39;key2&#39;],right_index=True) Out[110]: data key1 key2 event1 event2 0 0 Ohio 2000 4 5 0 0 Ohio 2000 6 7 1 1 Ohio 2001 8 9 2 2 Ohio 2002 10 11 3 3 Nevada 2001 0 1 DataFrame 还有一个 join 实例方法，它能更为方便地实现按索引合并。它还可用于合并多个带有相同或相似索引的 DataFrame 对象，而不管它们之间有没有重叠的列： In [119]: left2.join(right2,how=&#39;outer&#39;) Out[119]: Ohio Nevada Missouri Alabama a 1 2 NaN NaN b NaN NaN 7 8 c 3 4 9 10 d NaN NaN 11 12 e 5 6 13 14 In [120]: left1.join(right1,on = &#39;key&#39;) Out[120]: key value group_val 0 a 0 3.5 1 b 1 7.0 2 a 2 3.5 3 a 3 3.5 4 b 4 7.0 5 c 5 NaN In [121]: # 对于简单的索引合并，我们还可以向 join 传入一组 DataFrame In [122]: another = DataFrame([[7,8],[9,10],[11,12],[16,17]], .....: index = [&#39;a&#39;,&#39;c&#39;,&#39;e&#39;,&#39;f&#39;],columns = [&#39;New York&#39;,&#39;Oregon&#39;]) In [123]: left2.join([right2,another]) Out[123]: Ohio Nevada Missouri Alabama New York Oregon a 1 2 NaN NaN 7 8 c 3 4 9 10 9 10 e 5 6 13 14 11 12 In [124]: left2.join([right2,another],how=&#39;outer&#39;) Out[124]: Ohio Nevada Missouri Alabama New York Oregon a 1 2 NaN NaN 7 8 b NaN NaN 7 8 NaN NaN c 3 4 9 10 9 10 d NaN NaN 11 12 NaN NaN e 5 6 13 14 11 12 f NaN NaN NaN NaN 16 17 ##五、轴向连接 另一种数据合并运算也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）。NumPy 有一个用于合并原始 NumPy 数组的 concatenate 函数： In [126]: arr = np.arange(12).reshape((3,4)) In [127]: arr Out[127]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) In [128]: np.concatenate([arr,arr],axis=1) Out[128]: array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 对于 pandas 对象（如 Series 和 DataFrame），带有标签的轴使你能够进一步推广数组的连接运算。具体点说，我们还需要考虑以下这些东西： 如果各对象其他轴上的索引不同，那些轴应该是做并集还是交集？ 结果对象中的分组需要各不相同吗？ 用于连接的轴重要吗？ pandas 的 concat函数提供了一种能够解决这些问题的可靠方式。我将给出一些例子来讲解其使用方式。假设有三个没有重叠索引的 Series： In [133]: s1 = Series([0,1],index=[&#39;a&#39;,&#39;b&#39;]) In [134]: s2 = Series([2,3,4],index = [&#39;c&#39;,&#39;d&#39;,&#39;e&#39;]) In [135]: s3 = Series([5,6],index = [&#39;f&#39;,&#39;g&#39;]) In [136]: # 对这些对象调用 concat 可以将值和索引粘合在一起 In [137]: pd.concat([s1,s2,s3]) Out[137]: a 0 b 1 c 2 d 3 e 4 f 5 g 6 dtype: int64 In [138]: # 默认情况下，concat 是在 axis=0上工作的，最终产生一个新的 Series In [139]: #如果出入 axis=1，则会变成一个 DataFrame In [140]: pd.concat([s1,s2,s3],axis=1) Out[140]: 0 1 2 a 0 NaN NaN b 1 NaN NaN c NaN 2 NaN d NaN 3 NaN e NaN 4 NaN f NaN NaN 5 g NaN NaN 6 concat函数的参数： 参数 说明 objs 参与连接的 pandas 对象的列表或字典。唯一必需的参数 axis 指明连接的轴向，默认为0 join “inner”、”outer”其中之一，默认为”outer”。指明其他轴向上的索引是按交集（inner）还是并集（outer）进行合并 join_axes 指明用于其他 n-1 条轴的索引，不执行并集/交集运算 keys 与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、数组列表（如果将 levels 设置成多级数组的话） levels 指定用作层次化索引各级别上的索引，如果设置了 keys 的话 names 用于创建分层级别的名称，如果设置了 keys 或 levels 的话 verify_integrity 检查结果对象新轴上的重复情况，如果发现则引发异常。默认（False）允许重复 ignore_index 不保留连接轴上的索引，产生一组新索引 range（total_length） ##六、合并重叠数据 还有一种数据组合问题不能用简单的合并（merge）或连接（concatenation）运算来处理。比如，我们可能有索引全部或部分重叠的两个数据集. In [170]: a = Series([np.nan,2.5,np.nan,3.5,4.5,np.nan], index = [&#39;f&#39;,&#39;e&#39;,&#39;d&#39;,&#39;c&#39;,&#39;b&#39;,&#39;a&#39;]) In [171]: b = Series(np.arange(len(a),dtype = np.float64), index = [&#39;f&#39;,&#39;e&#39;,&#39;d&#39;,&#39;c&#39;,&#39;b&#39;,&#39;a&#39;]) In [172]: b[-1]=np.nan In [173]: a Out[173]: f NaN e 2.5 d NaN c 3.5 b 4.5 a NaN dtype: float64 In [174]: b Out[174]: f 0 e 1 d 2 c 3 b 4 a NaN dtype: float64 In [175]: np.where(pd.isnull(a),b,a) Out[175]: array([ 0. , 2.5, 2. , 3.5, 4.5, nan]) In [176]: #Series 有一个 combine_first 方法，实现的也是一样的功能，而且会进行数据对齐 In [177]: b[:-2].combine_first(a[2:]) Out[177]: a NaN b 4.5 c 3.0 d 2.0 e 1.0 f 0.0 dtype: float64 In [178]: # 对于 DataFrame 也是如此 In [179]: df1 = DataFrame({&#39;a&#39;:[1,np.nan,5,np.nan], .....: &#39;b&#39;:[np.nan,2,np.nan,6], .....: &#39;c&#39;:range(2,18,4)}) In [180]: df2 = DataFrame({&#39;a&#39;:[5,4,np.nan,3,7], .....: &#39;b&#39;:[np.nan,3,4,6,8]}) In [181]: df1.combine_first(df2) Out[181]: a b c 0 1 NaN 2 1 4 2 6 2 5 4 10 3 3 6 14 4 7 8 NaN","raw":null,"content":null,"categories":[],"tags":[]},{"title":"","slug":"PYTHON数据分析（PANDAS-II-使用数据库）","date":"2016-05-16T14:32:55.130Z","updated":"2016-05-16T14:33:00.012Z","comments":true,"path":"2016/05/16/PYTHON数据分析（PANDAS-II-使用数据库）/","link":"","permalink":"http://yoursite.com/2016/05/16/PYTHON数据分析（PANDAS-II-使用数据库）/","excerpt":"PYTHON数据分析（PANDAS II –使用数据库）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] ##一、实验简介 在许多应用中，数据很少取自文本文件，因为用这种方式存储大量数据很低效。基于 SQL 的关系型数据库（如 SQL Server、PostgreSQL 和 MySQL 等）使用非常广泛，此外还有一些非 SQL（即所谓的 NoSQL）型数据库也变得非常流行。","keywords":null,"text":"PYTHON数据分析（PANDAS II –使用数据库）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] ##一、实验简介 在许多应用中，数据很少取自文本文件，因为用这种方式存储大量数据很低效。基于 SQL 的关系型数据库（如 SQL Server、PostgreSQL 和 MySQL 等）使用非常广泛，此外还有一些非 SQL（即所谓的 NoSQL）型数据库也变得非常流行。 ##二、数据库简介 将数据从 SQL 加载到 DataFrame 得过程很简单，此外 pandas 还有一些能够简化该过程的函数。例如，我们将使用一款嵌入式的 SQLite 数据库（通过 Python 内置的 sqlite3 驱动器） In [42]: import sqlite3In [43]: query = &quot;&quot;&quot; ....: CREATE TABLE test ....: (a VARCHAR(20),b VARCHAR(20), ....: c REAL, d INTEGER ....: );&quot;&quot;&quot; In [44]: con = sqlite3.connect(&#39;:memory:&#39;) In [45]: con.execute(query) Out[45]: &lt;sqlite3.Cursor at 0x104321030&gt; In [46]: con.commit() In [47]: data = [(&#39;Atlanta&#39;,&#39;Georgia&#39;,1.25,6), ....: (&#39;Tallahassee&#39;,&#39;Florida&#39;,2.6,3), ....: (&#39;Sacramento&#39;,&#39;California&#39;,1.7,5)] In [48]: stmt = &quot;INSERT INTO test VALUES(?,?,?,?)&quot; In [49]: con.executemany(stmt,data)Out[49]: &lt;sqlite3.Cursor at 0x104316260&gt; In [50]: con.commit() 从表中选取数据时，大部分 Python SQL 驱动器（PyDBC、psycopg2、MySQLdb、pymssql等）都会返回一个元组列表 In [51]: cursor = con.execute(&#39;select * from test&#39;) In [52]: rows = cursor.fetchall() In [53]: rowsOut[53]: [(u&#39;Atlanta&#39;, u&#39;Georgia&#39;, 1.25, 6), (u&#39;Tallahassee&#39;, u&#39;Florida&#39;, 2.6, 3), (u&#39;Sacramento&#39;, u&#39;California&#39;, 1.7, 5)] 我们可以将这个元组列表传给 DataFrame 的构造器，但还需要列名（位于游标的 description 属性中） In [55]: cursor.description Out[55]: ((&#39;a&#39;, None, None, None, None, None, None), (&#39;b&#39;, None, None, None, None, None, None), (&#39;c&#39;, None, None, None, None, None, None), (&#39;d&#39;, None, None, None, None, None, None)) In [56]: DataFrame(rows,columns = zip(*cursor.description)[0]) Out[56]: a b c d 0 Atlanta Georgia 1.25 6 1 Tallahassee Florida 2.60 3 2 Sacramento California 1.70 5 这种数据规整操作相当多，你肯定不想每查一次数据库就重写一次。pandas 有一个可以简化该过程的 read_sql 函数（位于 pandas.io.sql 模块）。只需传入 select 语句和连接对象即可 In [61]: import pandas.io.sql as sql In [62]: sql.read_sql(&#39;select * from test&#39;,con) Out[62]: a b c d 0 Atlanta Georgia 1.25 6 1 Tallahassee Florida 2.60 3 2 Sacramento California 1.70 5 ##三、存取 MongoDB 中的数据 noSQL 数据库有许多不同的形式。有些是简单的字典式键值对存储（如 BerkeleyDB 和 Tokyo Cabinet），另一些则是基于文档的（其中的基本单元是字典型的对象）。本例选用得是 MongoDB。我们先在自己的电脑上启动一个 MongoDB 实例，然后用 pymongo（MongoDB 的官方驱动器）通过默认端口进行连接 In [34]: import pymongoIn [35]: con = pymongo.MongoClient(&#39;localhost&#39;,port = 27017) 存储在 MongoDB 中的文档被组织在数据库的集合（collection）中。MongoDB 服务器的每个运行实例可以有多个数据库，而每个数据库又可以有多个集合。假设你想保存之前通过 GeoNames API 获取的数据。首先我们可以访问 citys 集合（暂时还是空的） In [39]: citys = con.db.citys 然后，我将那组 city 加载进来并通过 citys.save（用于将 Python 字典写入 MongoDB）逐个存入集合中： In [40]: import requests,json In [41]: url = &#39;http://api.geonames.org/citiesJSON?north=44.1&amp;south=-9.9&amp;east=-22.4&amp;west=55.2&amp;lang=de&amp;username=demo&#39; In [42]: data = json.loads(requests.get(url).text) In [43]: for city in data[&#39;geonames&#39;]: ....: citys.save(city) 现在，如果我想从该集合中取出我自已想要的 city（如果有的话），可以用下面的代码对集合进行查询： In [55]: cursor = citys.find({&#39;name&#39;: &#39;Hong Kong&#39;}) 返回的游标是一个迭代器，它可以为每个文档产生一个字典。跟之前一样，我们可以将其转换为一个 DataFrame。此外还可以只获取各 city 的部分字段： In [56]: city_fields = [&#39;countrycode&#39;, &#39;name&#39;, &#39;population&#39;] In [57]: result = DataFrame(list(cursor),columns=city_fields) In [58]: result Out[58]: countrycode name population 0 HK Hong Kong 7012738","raw":null,"content":null,"categories":[],"tags":[]},{"title":"","slug":"PYTHON数据分析（PANDAS-I-读写文本数据）","date":"2016-05-16T14:32:40.747Z","updated":"2016-05-16T14:32:47.027Z","comments":true,"path":"2016/05/16/PYTHON数据分析（PANDAS-I-读写文本数据）/","link":"","permalink":"http://yoursite.com/2016/05/16/PYTHON数据分析（PANDAS-I-读写文本数据）/","excerpt":"PYTHON数据分析（PANDAS I –读写文本数据）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程《PYTHON数据分析》 ##一、实验简介 本实验将学习逐块读取文本文件、将数据写出到文本格式以及手工处理分隔符格式等知识。 输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用 Web API 操作网络资源","keywords":null,"text":"PYTHON数据分析（PANDAS I –读写文本数据）标签： PYTHON科学计算 转载自实验楼作者 木木同学 的课程《PYTHON数据分析》 ##一、实验简介 本实验将学习逐块读取文本文件、将数据写出到文本格式以及手工处理分隔符格式等知识。 输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用 Web API 操作网络资源 ##二、逐块读取文本文件 由于本节实验需要使用到python的lxml库，所以先在环境中安装： pip install python-lxml Python 在文本和文件的处理以其简单的文本交互语法、直观的数据结构，以及诸如元组打包解包之类的便利功能，深受人们喜爱。 pandas 提供了一些用于将表格型数据读取为 DataFrame 对象的函数。 函数 说明 read_csv 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号 read_table 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符\\t read_fwf 读取定宽列格式数据（也就是说，没有分隔符） read_clipboard 读取剪贴板中的数据，可以看做 read_table 的剪贴板版。在将网页转换为表格时很有用 这些函数的选项可以划分为以下几大类： 索引：将一个或多个列当做返回的 DataFrame 处理，以及是否从文件、用户获取列名 类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等 日期解析：包括组合功，比如将分散在多个列中的日期时间信息组合成结果中的单个列 迭代：支持对大文件进行逐块迭代 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西（比如由成千上万个逗号隔开的数值数据） 类型推断（typeinference）是这些函数中最重要的功能之一，我们不需要指定列的类型到底是数值、整数、布尔值，还是字符串。日期和其他自定义类型的处理需要多花点功夫才行。 首先我们来看一个以逗号分隔（我这里用的是 CSV 文件，同学们用 txt 文件是一样的）的文本文件： 我们也可以用read_table，只不过需要指定分隔符而已： data = pd.read_table(r&#39;...\\test.csv&#39;) 在上面的例子中，我们在文本文件中写了标题行（即 message 那一行），但是不是所有文件都会有标题行，读入这种文件有两个办法。 层次化索引 有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其他字符串）。对于这种情况，我们可以编写一个正则表达式来作为 read_table 的分隔符。 正则表达式 上面的文本中 read_table发现列名的数量比列的数量少1，于是推断第一列应该是 DataFrame 的索引。 我们现在就来看一看 read_csv/read_table 函数的参数 参数 说明 path 表示文件系统位置、URL、文件型对象的字符串 sep/delimiter 用于对行中各字段进行拆分的字符序列或正则表达式 header 用作列名的行号。默认为0（第一行），如果没有 header 行就应该设置为 None index_col 用作行索引的列编号或列名。可以是单个名称/数字或由多个名称/数字组成的列表（层次化索引） names 用于结果的列名列表，结合 header= None skiprows 需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始） na_values 一组用于替换 NA 的值 comment 用于将注释信息从行尾拆分出去的字符（一个或多个） parse_datas 尝试将数据解析为日期，默认为 False，如果为 True，则尝试解析所有列。此外，还可以指定需要解析的一组列号或列名。如果列表的元素为列表或元组，就会将多个列组合到一起再进行日期解析工作 keep_data_col 如果连接多列解析日期，则保持参与连接的列。默认为 False converters 由列号/列名跟函数之间的映射关系组成的字典。例如，{‘foo’ : f}会对 foo 列的所有值应用函数 f dayfirst 当解析有歧义的日期时，将其看做国际格式（例如，7/6/2012 → June 7,2012）。默认为 False data_parser 用于解析日期的函数 nrows 需要读取的行数（从文件开始处算起） iterator 返回一个 TextParser 以便逐块读取文件 chunksize 文件块的大小（用于迭代） skip_footer 需要忽略的行数（从文件末尾处算起） verbose 打印各种解析器输出信息，比如“非数值列中缺失值的数量”等 encoding 用于 unicode 的文本编码格式 squeeze 如果数据经解析后仅含一列，则返回 Series thousand 千分位分隔符，如,或者. 在处理很大文件或找出大文件中的参数集以便后续处理时，我们可能只想读取文件的一小部分或逐块对文件进行迭代。 写入数据 规定 nrows 逐块读 ##三、手工处理分隔符格式 大部分存储在磁盘上的表格型数据都能用 pandas.read_table进行加载。然而，有时还是需要做一些手工处理。由于接收到含有畸形行的文件而使用 read_table 出毛病的情况并不少见。接下来我们来看一个例子： 手工处理 CSV文件的形式有很多。只需定义 csv.Dialect 的一个子类即可定义出新格式（如专门的分隔符、字符串引用约定、行结束符等） In [61]: class my_dialect(csv.Dialect): ....: lineterminator = &#39;\\n&#39; ....: delimiter = &#39;;&#39; ....: quotechar = &#39;&quot;&#39; ....: In [62]: reader = csv.reader(f, dialect=my_dialect, quoting = csv.QUOTE_ALL) In [63]: reader Out[63]: &lt;_csv.reader at 0x1074c29f0&gt; In [64]: with open(&#39;mydata.csv&#39;,&#39;w&#39;) as f: ....: writer = csv.writer(f,dialect = my_dialect,quoting = csv.QUOTE_ALL) ....: writer.writerow((&#39;one&#39;,&#39;two&#39;,&#39;three&#39;)) ....: writer.writerow((&#39;1&#39;,&#39;2&#39;,&#39;3&#39;)) ....: writer.writerow((&#39;4&#39;,&#39;5&#39;,&#39;6&#39;)) ....: writer.writerow((&#39;7&#39;,&#39;8&#39;,&#39;9&#39;)) ....:（mumu 发现仅截图，同学们学习起来不是特别方便，所以今后我会把代码写出来） ##四、JSON 数据 JSON（JavaScript Object Notation 的简称）已经成为通过 HTTP 请求在 Web 浏览器和其他应用程序之间发送数据的标准格式之一。它是一种比表格型文本格式（如 CSV）灵活得多的数据格式。如： obj = &quot;&quot;&quot; { &quot;name&quot;:&quot;Limei&quot;, &quot;places_lived&quot;:[&quot;China&quot;,&quot;UK&quot;,&quot;Germany&quot;], &quot;pet&quot;:null, &quot;siblings&quot;:[{&quot;name&quot;:&quot;Liming&quot;,&quot;age&quot;:23,&quot;pet&quot;:&quot;Xiaobai&quot;}, {&quot;name&quot;:&quot;Lifang&quot;,&quot;age&quot;:33,&quot;pet&quot;:&quot;Xiaohei&quot;}] } &quot;&quot;&quot; 除其空值 null 和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON 非常接近于有效的 Python 代码。基本类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及 null。对象中所有的键都必须是字符串。许多 Python 库都可以读写 JSON 数据。我们将使用 json，因为它是构建与 Python 标准库中的。通过 json.loads 即可将 JSON 字符串转换成 Python 形式 In [70]: import json In [71]: result = json.loads(obj) In [72]: result Out[72]: {u&#39;name&#39;: u&#39;Limei&#39;, u&#39;pet&#39;: None, u&#39;places_lived&#39;: [u&#39;China&#39;, u&#39;UK&#39;, u&#39;Germany&#39;], u&#39;siblings&#39;: [{u&#39;age&#39;: 23, u&#39;name&#39;: u&#39;Liming&#39;, u&#39;pet&#39;: u&#39;Xiaobai&#39;}, {u&#39;age&#39;: 33, u&#39;name&#39;: u&#39;Lifang&#39;, u&#39;pet&#39;: u&#39;Xiaohei&#39;}]} 相反，json.dumps则将 Python 对象转换成 JSON 格式： In [78]: asjson = json.dumps(result) In [79]: asjson Out[79]: &#39;{&quot;pet&quot;: null, &quot;siblings&quot;: [{&quot;pet&quot;: &quot;Xiaobai&quot;, &quot;age&quot;: 23, &quot;name&quot;: &quot;Liming&quot;}, {&quot;pet&quot;: &quot;Xiaohei&quot;, &quot;age&quot;: 33, &quot;name&quot;: &quot;Lifang&quot;}], &quot;name&quot;: &quot;Limei&quot;, &quot;places_lived&quot;: [&quot;China&quot;, &quot;UK&quot;, &quot;Germany&quot;]}&#39; 如何将（一个或一组）JSON 对象转换为DataFrame 或其他便于分析的数据结构呢？最简单方便的方式是：向 DataFrame 构造器传入一组 JSON 对象，并选取数据字段的子集。 In [80]: siblings = DataFrame(result[&#39;siblings&#39;],columns = [&#39;name&#39;,&#39;age&#39;]) In [81]: siblings Out[81]: name age 0 Liming 23 1 Lifang 33五、XML 和 HTML Python 有许多可以读写 HTML 和 XML 格式数据的库。lxml 就是其中之一，它能够高效且可靠地解析大文件。lxml 有多个编程接口。首先我要用 lxml.html 处理 HTML，然后再用 lxml.objectify 做一些 XML 处理。 许多网站都将数据放到 HTML 表格中以便在浏览器中查看，但不能以一种更易于机器阅读的格式（如 JSON、HTML 或 XML）进行下载 首先，找到我们希望获取数据的 URL，利用 urllib2 将其打开，然后用 lxml 解析得到的数据流，如下所示： In [4]: from lxml.html import parse In [5]: from urllib2 import urlopen In [6]: parsed = parse(urlopen(&#39;http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices&#39;)) In [7]: doc = parsed.getroot() 通过这个对象，我们可以获取特定类型的所有 HTML 标签，比如含有所需数据的 table 标签。给这个简单的例子加点启发性，假设我们想得到该文档中所有的 URL 链接。HTML 中的链接是a标签。使用文档根节点的 findall 方法以及一个 XPath（对文档的“查询”的一种表示手段）： links = doc.findall(&#39;.//a&#39;) In [9]: links[:3] Out[9]: [&lt;Element a at 0x10db065d0&gt;, &lt;Element a at 0x10db06628&gt;, &lt;Element a at 0x10db06680&gt;] 但是这些是表示 HTML 元素的对象。要得到 URL 和链接文本，你必须使用各对象的 get 方法（针对 URL）和 text_content 方法（针对显示文本）： In [11]: lnk = links[5] In [12]: lnk Out[12]: &lt;Element a at 0x10db06788&gt; In [13]: lnk.get(&#39;href&#39;) Out[13]: &#39;http://finance.yahoo.com/&#39; In [14]: lnk.text_content() Out[14]: &#39;Finance&#39; 因此编写下面这条列表推导式即可获取文档中的全部 URL： In [16]: urls = [lnk.get(&#39;href&#39;) for lnk in doc.findall(&#39;.//a&#39;)] In [17]: urls[:3] Out[17]: [&#39;https://www.yahoo.com/&#39;, &#39;https://mail.yahoo.com/?.intl=us&amp;.lang=en-US&amp;.src=ym&#39;, &#39;https://search.yahoo.com/search&#39;] 现在，从文档中找出正确表格的办法就是反复试验了。有些网站会给目标表格加上一个id属性。mumu 试验了好久，终于在这个网站上找到了我们需要的表格： In [70]: tables = doc.findall(&#39;.//table&#39;) In [121]: #从70行试到了121行也是不容易的啊 In [122]: calls = tables[13] In [123]: puts = tables[14] In [124]: from pandas.io.parsers import TextParser In [125]: #由于数值型数据任然是字符串格式 In [126]: #所以我们希望将部分列转换为浮点数格式 In [127]: #pandas 恰好就有一个 TextParser 类 In [128]: #用于自动类型转换 In [129]: def parse_options_data(table): .....: rows = table.findall(&#39;.//tr&#39;) .....: #每个表格都有一个标题行，然后才是数据行 .....: header = _unpack(rows[0],kind=&#39;th&#39;) .....: #对于标题行，就是 th 单元格 .....: #数据行，就是 td 单元格 .....: data = [_unpack(r) for r in rows[1:]] .....: return TextParser(data,names=header).get_chunk() In [130]: call_data = parse_options_data(calls)In [131]: put_data = parse_options_data(puts) In [132]: call_data[:3] Out[132]: Date Open High Low Close Volume \\ 0 Jul 29, 2015 123.15 123.50 122.27 122.99 35,914,200 1 Jul 28, 2015 123.38 123.91 122.55 123.38 33,448,900 2 Jul 27, 2015 123.09 123.61 122.12 122.77 44,274,800 Adj Close*\\n 0 122.99 1 123.38 2 122.77 XML 是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。现在我们来学习另一种用于操作 XML 数据的接口，即 lxml.objectify 这里mumu 给同学们一个 xml 文件的内容，同学们用文本编译器生成一个 xml 文件即可： ` 373889 Metro-North Railroad Escalator Availability Percent of the time that escalators are operational systemwide. The availability rate is based on physical observations performedthe morning of regular business days only. This is a new indicator the agencybegan reporting in 2009. 2011 12 Service Indicators M U % 1 97.00 97.00 好了，接下来我们就进入lxml.objectify` 的学习吧 In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用： In [4]: from lxml import objectify In [5]: path = &#39;Performance_MNR.xml&#39; In [6]: parsed = objectify.parse(open(path)) In [7]: root = parsed.getroot()In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用： In [4]: from lxml import objectify In [5]: path = &#39;Performance_MNR.xml&#39; In [6]: parsed = objectify.parse(open(path)) In [7]: root = parsed.getroot() In [8]: data = [] In [9]: skip_fields = [&#39;PARENT_SEQ&#39;&#39;PARENT_SEQ&#39;, &#39;INDICATOR_SEQ&#39;, &#39;DESIRED_CHANGE&#39;, &#39;DECIMAL_PLACES&#39;] In [10]: for elt in root: ....: el_data = {} ....: for child in elt.getchildren(): ....: if child.tag in skip_fields: ....: continue ....: el_data[child.tag] = child.pyval ....: data.append(el_data) In [11]: data Out[11]: [{&#39;AGENCY_NAME&#39;: &#39;Metro-North Railroad&#39;, &#39;CATEGORY&#39;: &#39;Service Indicators&#39;, &#39;DESCRIPTION&#39;: &#39;Percent of the time that escalators are operational \\nsystemwide. The availability rate is based on physical observations performed \\nthe morning of regular business days only. This is a new indicator the agency \\nbegan reporting in 2009.&#39;, &#39;FREQUENCY&#39;: &#39;M&#39;, &#39;INDICATOR_NAME&#39;: &#39;Escalator Availability&#39;, &#39;INDICATOR_UNIT&#39;: &#39;%&#39;, &#39;MONTHLY_ACTUAL&#39;: u&#39;&#39;, &#39;MONTHLY_TARGET&#39;: 97.0, &#39;PARENT_SEQ&#39;: u&#39;&#39;, &#39;PERIOD_MONTH&#39;: 12, &#39;PERIOD_YEAR&#39;: 2011, &#39;YTD_ACTUAL&#39;: u&#39;&#39;, &#39;YTD_TARGET&#39;: 97.0}]","raw":null,"content":null,"categories":[],"tags":[]},{"title":"","slug":"2016-4-23-dotatalking","date":"2016-05-16T14:31:20.465Z","updated":"2016-05-16T14:31:24.748Z","comments":true,"path":"2016/05/16/2016-4-23-dotatalking/","link":"","permalink":"http://yoursite.com/2016/05/16/2016-4-23-dotatalking/","excerpt":"DOTA职业圈，被黑出翔的那些经典名人名言标签（空格分隔）： DOTA","keywords":null,"text":"DOTA职业圈，被黑出翔的那些经典名人名言标签（空格分隔）： DOTA ##NO.1 拉谁!说话!拉谁! 出自怒吼天尊XB。 XB在某次比赛中，使用自己擅长的蝙蝠骑士(现已冠名XB骑士)，非常嚣张的嘲讽道：“拉谁!说话!拉谁!” 于是…… 他拉了一个小兵，全世界震惊了。从此XB骑士的美名享誉全球，XB的机智打动了在场的所有人，掌声经久不息 ##NO.2 给我幽鬼，输了砍手! 出自伊人HAO(砍手HAO) HAO娘刚出道没多久的时候。俗话说，初生牛犊不怕虎，面对强悍的对手，HAO娘自信的要求队友给他PICK幽鬼 ，“输了砍手!” 从此砍手HAO的外号不胫而走。 江湖上盛传，我有上将HAO，可斩ZHOU神! 不过HAO娘随着这几年的成长，越发的成熟，已经成为了最顶级C之一!砍手HAO的美名也逐渐被伊人HAO所代替. ##NO.3 就是在，有时候比赛输了，我会觉得队友不给力。 出自徐志雷(BURNING,大B神,BB) 最近版本变更，加上DK人员调整，以及之前的所谓B神“一踢四”的风波，再加上最近BB的表现不尽如意。让我们的BB站在了风口浪尖，之前在G1冠军联赛上说过的这句话又被很多人拿出来旧事从提~ BB号称是能够1V9的男人，所以输了比赛，当然怪队友不给力咯~ 其实啦，你们如果继续看这个视频，会发现BB之后有说到，“但是，DOTA是5个人的游戏。。。。。。”总而言之，就是输赢是要大家一块承担的意思。 不过，想黑人~断章取义可是最容易的手段哦~ ##NO.4 打个比赛像农民一样! 出自ZHOUZHOU~ 某次比赛，当时的大ZHOU神由于不满XB的无尽嘲讽，为了发泄，说出了这样一句让我大中华广大农民愤慨的话。 结果呢，这句话并没有影响到XB，反而给ZHOUZHOU自己扣上了农民ZHOU的帽子…… 突出一个惨。 ##NO.5 真替狗哥不值! 出自怒吼天尊XB 紧接着NO.4，当ZHOUZHOU不满XB嘲讽进行回敬之后，XB再次使出杀手锏! 当时，XB指着看台反嘲一句：“谁先骂的，昨天比赛是谁骂的狗哥(zhou前队友兄弟，ig合并时被踢出去的三生)，我真替狗哥不值!狗哥，奈斯不奈斯!?” 结果台下竟然一片“nice”回应，ZHOUZHOU嘲讽大败而归。 之后，真替XX不值这句话就成了适用范围极广的一句话。 例如，当YAO接受采访表示要打一直以来暗恋自己的花花(性别未知)的时候，广大网友纷纷表示“真替花花不值!” ##NO.6 来DOTA1不把你打哭? 出自某不知名非职业选手NL_KS 当时我们的NL_KS大神还在90016DOTA1战队，他们的战队名叫GREENDY。由于入驻DOTA2比较晚，(其实就是技不如人。。。。。。)在进行一场DOTA2比赛的时候，惨遭血虐，被广大网友嘲讽。 我们的KS大神按耐不住心中的怒火，一句“来DOTA1不把你打哭?”硬是强行登基!GREENDY成功成为DOTA1宇宙第一强队! 衍生版本，来DOTA1卜把妮打哭(为什么要这么写…懂的自然懂)。 ##NO.7 对不起，这个比赛我要赢。 出自09(牛肉9，大菊观9) 若干年前的SOLO大赛，决赛的对阵双方是09和LONGDD，当09的炼金在河道处击杀了龙DD的小J的时候，09霸气的打出了这样一句话“对不起，这个比赛我要赢。”当然，最终他的确赢得了比赛，获得了冠军。 尽显装B本色，广大网友不服不行，唏嘘不已。 ##NO.8 我又不是最菜的! 再次出自怒吼天尊XB(话题小王子啊，不是我想黑你的) 今年的DK人员大调整，直接让旧DK除BURNING以外的所有人全部离队，作为劣势路抗压，总被网友诟病冲动爱送爱崩盘的XB，在接受一次采访的时候表示，非常不能够接受这样一种被离队的结果，明确表示“我又不是最菜的!” 不过之后XB SAMA的一次次“惊艳”表现再次印证了你就是最菜的那个!(玩笑话，XB就是这么个打法)。 每当XB被各种姿势艹翻的时候，网友们都翻出这句话来，毕竟，不是最菜的。 ##NO.9 我TM又不是SB，我亲自送她上的飞机! 出自PIS(达蒙皮，P神) P神的绿帽事件，我就不再科普了，毕竟公共场合，有伤风化!说多了都是泪。 总而言之，当广大网友将诸多证据摆在PP面前的时候，PP选择在YY直播，做出澄清，对于师母到底飞没飞海南这件事，PP斩钉截铁的表示“我女朋友我还不知道么?我亲自送她上的飞机!我TM又不是SB!”尽显男儿本色，霸气外漏! 不幸的是，仅仅两小时之后，PP就再次回到YY表示，网友的爆料，情况基本属实，绿帽已戴，好聚好散。 ##NO.10 这个比赛白送了! 出自怒吼天尊XB(哥你上镜太多了点吧) 当时的老队长XB已经来到了VG战队，同样打3号位，作为1号位的TUTU，之前表现不佳，不过状态逐渐回升，VG战队越战越勇，不过当TUTU得知自己如果表示不好，可能会被SYLAR换掉的时候，再加上自己家中有急事(记得好像是父亲重病)，无法集中精力比赛，状态不是很好，VG战队在NEST比赛中也失败而归。 而爱冲动的老队长在没有完全了解TUTU情况之下，在微博上说出了这些气话“这个比赛白送了!”认为TUTU是导致VG的落败原因，这个比赛算白送给别人了。 于是我大XB SAMA再次被黑，之后的比赛，一旦VG落败，网友都会在聊天频道调侃“这个比赛白送了!” ##NO.11 不要放海涛切假腿。 出自海涛 海涛作为很多DOTA玩家的启蒙老师，其对DOTA做出的贡献无可厚非，再加上对DOTA2也是极力推广，不可不谓良心涛，虽然他被网友诟病“解说不专业，打的菜，话痨”。不过LZ本人现在对他还是持肯定态度的! 不过，千万，千万，不要放海涛切假腿。假腿就是动力鞋，科普一下，切假腿，是因为动力鞋有三个形态，可以切换。 传说中，当你对面有海涛的时候，必须全场GANK他，不要放海涛打钱，因为，一旦海涛打出假腿的钱，合成了假腿，你们就已经输了。 海涛切起假腿来，APM瞬间可以达到600以上，曾经仅靠一个假腿冰女通过超高频率的切假腿，秒杀6龙心人马，成为一段神话! 其实是海涛在自己的视频中，总是喜欢强调切假腿的妙用，这一个小小的假腿似乎爆发出了不可估量的作用，于是被网友黑出了翔! ##NO.12 我们打比赛从不20投 出自ZHOUZHOU(农民ZHOU，智障ZHOU) 在一次接受采访的时候，记者刁钻的问起了DOTA与LOL的区别，智障ZHOU突然大脑短路，苦思冥想，得出结论，微微一笑“我们打比赛从不20投!” 悲剧的是，就在接下来的比赛之中，由ZHOUZHOU带领的IG，在坚持了16分钟之后，就无奈打出了GG，创造了史无前例的16投的传说!狠狠的自己扇了自己一巴掌。20投也成为了扣在ZHOUZHOU头上一顶摘不下来的帽子。。。。。。 其实呢，ZHOUZHOU后来在微博上澄清，这句话是别人怂恿他说的，并不代表他的观点，不过，WHO CARE? ##NO.13 影魔已经不适合这个版本了。 单车(单车武士) 游戏风云直播，前职业选手单车，打水友局，对面水友霸气的手选了影魔，手选影魔的意思就等于，欠艹，求虐求侮辱，不服来SOLO!这能忍?单车思考了一下，选择了自认为拿手，并且可以针对影魔的神灵武士! 说了一句可能最让他后悔的一句话“我要证明，影魔已经不适合这个版本了。” 结果不到6级，单车的神灵武士就已经连续被对面影魔艹翻几次，看着0杀5死的数据，单车表示“这个Z炮怎么可能压到我?”“我不是那种一崩到底的选手!”接着单车武士继续被杀，继续着他的传说。 旁边的BBC意味深长的说“看来影魔还是挺适合这个版本的吧” 从此神灵武士冠名单车，正式更名为单车武士! ##NO.14 除了B神，其他C都差不多。 出自XIAO8(下面8) 在刚刚结束的WPC-ACE联赛上，LGD战队由于之前SYLAR的赌气离队，找来了小兔几当C，经过短暂的磨合，效果竟然出奇的好，得意于LGD战队的执行力与团结! 一次赛后，作为队长，XIAO8接受采访，主持人问到，LGD的C位变动，有没有导致战术等等方面的连锁变化，又或者这两个不同的C，风格，打法上有什么不同吗?XIAO8想了想，说出了“除了B神，其他C都差不多!” 这句话显然是对B神的认可，B神，代表了C位的标杆。更是对LGD战队的自信，LGD战队可以扶起任何一个C! 不过呢……由于B神最近的低迷表现，这句话又被拿出来赋予了新的含义，即，其他C都差不多，只有B神是最腿的，B神是C位的下限。。。 ##NO.15 我叫战神7，和你一换一! 出自战神7(教练) 战神7的个人能力，看过他打路人的都知道，不用我赘述了，国F几根最粗大腿之一!突出一个虎! 不过正如他的ID(上头猫)，这名选手打起职业来，脑子不是很好使，虽然自己打的中单2号位，却经常做出1V3，1V4的举动(当然跪了)，而且尤其喜欢跟对面的辅助一换一，突出一个谁不服我就跟你一换一! 从此，我叫战神7，和你一换一的美名就传播开了，战神7也迫于压力没有继续打职业，不过他仍然活跃在职业圈，希望大家支持这样有干劲，有能力的选手! ##NO.16 干干干!救救救!阿西吧! 出自宝哥 宝哥是以前老DK战队的5号位，专职辅助B神，老好人，现在没打职业了，在YY90016做直播，有时候会切换成“支付宝”形态，强行“支付”送人头! 大家脑补一下以下场景。 宝哥身先士卒，杀入敌阵，口中喊着“干干干!”，结果发现自己不是B神不能1V9，快要体力不支倒下了，焦急的宝哥大喊“救救救!”，结果队友一看宝哥又切换“支付宝”形态了，毅然选择了放弃宝哥，宝哥终于壮烈牺牲，宝哥长吁一口气“阿西吧!” ##NO.17 我们的猪肉粒含有牛肉成分! 出自09(牛肉9，翔9，偷钱9) 我们都知道著名大忽悠09先森，通过自己在DOTA取得的一定成绩，将自己包装成了一个全民偶像，聪明的09利用自己的名声做起了淘宝生意，食品店那叫一个火爆! 其中很著名的产品就是OX牛肉粒，结果细心的网友调查发现，该牛肉粒根本不是牛肉做的而是猪肉做的!结果却明目张胆的起着牛肉粒的名字! 我大酒神迫于压力站出来辟谣，我们的猪肉粒，是含有牛肉成分的! 至于你信不信，我反正信了! ##NO.18 我就问你满足不满足! 出自XB 某次DK跟TONGFU比赛，线下赛，没有隔音室，双方可以尽情嘲讽。这不正中老队长XB的下怀吗?怒吼天尊不是白叫的，于是XB在比赛中，就让对面明白了。 送!送!送!会不会玩! 杀我，杀我!咬你就是两口! 我就问你满足不满足! 满足不满足! 可怜的香蕉和HAO娘根本没见过这个阵势，直接给吓傻了…… 满足不满足，这句话也成为了嘲讽的必备! 当你打路人艹翻全场的时候，一句，我就问你满足不满足，尽显霸气本色! ##NO.19 打的跟NM要赢了一样! 出自LONGDD(霍比特龙，AKK龙，翔龙，DOUBLE DICK LONG，喝汤龙，长沙赵子龙，吹B龙) 著名单口相声演员LONGDD在90016的单口相声，相信很多吧友都听过。像我们龙神这种混黑社会的，一把砍刀从东边砍到西边，血浮血海的，免不了结了很多仇人。俗话说，常在河边走，哪能不湿鞋! 龙神直播的时候，经常会匹配到自己的仇家，仇人相见，分外眼红，对面经常打的十分凶残，满场干!无奈我大龙神的护龙山庄，高手如云，几大护法武功高强，对面路人也是有心杀贼无力回天!虽然打的很有血性，可惜却仍旧败下阵来! 这时候龙神喝一口汤，嚼一口槟榔，轻蔑的一句“打的跟NM要赢了一样!”，霸气十足! ##NO.20 FEN SHOU KUAI LE, ZHAO JIE SHI WO DE LE。 分手快乐，赵洁是我的了! 出自DENDI(乌克兰老司机) DENDI一直以搞怪出名，不过他所在的NAVI战队成绩却是十分傲人的!一次与LGD的比赛临近结束，NAVI战队获得胜利，DENDI当着几万人看直播的面，在公共频道打出了fen shou kuai le ,zhao jie shi wo de le !这样一句话。 要知道，赵洁当时还是LGD战队队长小8的女朋友，两人之前因为一些摩擦闹了矛盾，甚至要分手。结果DENDI竟然在比赛中来了这么一句话!当时小8就忍不住回了句SHA BI! 事后，DENDI道了歉，表示是别人教他这么说的，他也不知道什么意思。至于到底谁教的?有人说就是赵洁本人，有人说是赵洁的好友狐狸妈(430前女友)，至于到底是谁?笑笑就好~~ ##NO.21 最多55开! 出自LONGDD 我们都知道，55开用来形容双方形势差不多，比赛难以预料，旗鼓相当。 可是DOTA比赛上出现真正55开的局面并不很多见，于是，当局面一边倒的时候，为了稳定观众情绪，活跃气氛，使比赛充满悬念，著名相声演员LONGDD只能强行55开了! 某比赛，20分钟出头，比分已经26比3，劣势方外塔掉光，只能高地打麻将。 狗哥：“龙神，你说现在这个局面，几几开，给我们分析分析?” LONGDD思索了一下：“最多55开，不能再多了!” 狗哥：“这还55开啊?真的假的?” LONGDD：“强行55开，不能再多了!”","raw":null,"content":null,"categories":[],"tags":[]},{"title":"","slug":"2016-4-13-test","date":"2016-04-13T15:02:34.956Z","updated":"2016-04-13T15:02:35.034Z","comments":true,"path":"2016/04/13/2016-4-13-test/","link":"","permalink":"http://yoursite.com/2016/04/13/2016-4-13-test/","excerpt":"","keywords":null,"text":"在此处输入标题标签（空格分隔）： 未分类 在此输入正文","raw":null,"content":null,"categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2016-04-12T11:46:46.162Z","updated":"2016-04-12T11:46:46.162Z","comments":true,"path":"2016/04/12/hello-world/","link":"","permalink":"http://yoursite.com/2016/04/12/hello-world/","excerpt":"","keywords":null,"text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","raw":null,"content":null,"categories":[],"tags":[]}]}