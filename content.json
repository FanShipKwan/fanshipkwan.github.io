{"meta":{"title":"因缺斯汀","subtitle":"Interesting","description":null,"author":"善能菌","url":"http://yoursite.com"},"pages":[],"posts":[{"title":"DOTA大型攻略--山岭巨人","slug":"山岭巨人攻略","date":"2016-05-25T12:53:58.202Z","updated":"2016-06-18T03:49:23.466Z","comments":true,"path":"2016/05/25/山岭巨人攻略/","link":"","permalink":"http://yoursite.com/2016/05/25/山岭巨人攻略/","excerpt":"","text":"转自DOTA2贴吧《Dota2大型攻略 山岭巨人 -听啊，这来自大山深处的回响》 作者 Villidan_Dota2 第零章:6.87版本提要 6.87版本，大部分中单遭到削弱，卡尔，黑鸟，宙斯等等英雄都遭到了或多或少的削弱，尤其是卡尔，主要体现在对线期不再拥有充足的蓝量，黑鸟则直接淡出了历史舞台。当前版本，中单肉核开始崛起，代表英雄龙骑士炼金术士。 有关小小的改动： 英雄的初始魔法值从0提高到了50点 英雄的初始血量从180提高到了200点 每一点力量提高的血量从19提高到了20点 近战的初始攻击距离从128提高到了150 小兵的仇恨持续时间和刷新时间都提高到了2.5秒 一塔提供了1点的护甲加成 远程兵经验从41增加到了90 白银之锋（SilverEdge）更改了配方，现在提高15点全属性 新道具回音战刃（echo sabre）的加入 为什么以上改动导致了小小的崛起，我会在接下来的攻略中慢慢讲述 第一章：基础要了解一个英雄，首先要从它的模型开始说起。 由于178的数据库还没有更新6.87的基础血量法力值的改动，以下内容均由游戏中截出，如有不对，还请指出 山岭巨人（小小） 英雄介绍：以一团石头的形式出现的生命体，小小不断思索他的起源，但这始终是个谜。现在的他是个石巨人，但过去是什么呢？从土傀儡的脚后跟掉落的碎片？从制造石像鬼的工房被打扫出来的碎屑？神圣预言石的表层之砂？受到强烈的好奇心驱使，他不知疲倦的环游世界，寻找着他的起源，他的出身，和他的种族。在旅途中，他变得越来越庞大，不过路上的风雨吹打掉了他身上的石头，所以他不停的吸收新的岩石，永远在长大。 初始属性：生命值：720法力值：254 （大于240点，意味着小小不需要属性出门就可以放二连）初始护甲：0 （1%）初始移动速度：285初始攻击力：66攻击距离：150A帐攻击距离：235初始力量26+3.0 （60血量）初始敏捷9+0.9 （每8级提高1点护甲）初始智力17+1.6 (19.2法力值) 分析：首先小小的初始蓝量的提升，使得之前需要带至少3点智力（3树枝），才能在2级放出的2连，可以直接放出，小小并不需要智力的回蓝，那么购买这三点属性的150块钱，可以购买小圆盾／补刀斧／提前购买魔瓶 （有关对线期的装备会在对线一章提出，这里暂时略过） 那么作为中单英雄，我们要注意的是什么呢： 补刀，控F和击杀／被击杀 关于补刀，小小初始高攻，出手手感良好，加上这版本近战攻击距离提升，勾兵的仇恨时间提高（意味着你可以把兵勾回来更远，远程英雄在兵线上强行点你受到小兵攻击更多），小小可以很容易的进行正反补，而且远程兵经验提高，可以利用A-V晕眩敌方英雄-A来很轻松的反补掉自家的半血远程兵，有效压制敌方经验 关于击杀，小小的击杀是通过vt完成的，vt连击的输出很高，将在之后的伤害一章里仔细阐述，这里暂且略过 关于被击杀，也就是防被单杀和被游走，那么由于当前6.87版本每点力量提供的血量增加，初始血量增加。我们可以看到一个6级小小的裸装血量已经达到了1000点，在敌方物理输出不高的时候，前期5分钟6级小小的1000点血量满血的情况下，是很难击杀的，更何况小小有v可以晕眩，t可以把近战的敌人丢远，配合魔瓶魔棒，占线可以说很稳。有了力量假腿之后血量可以达到1200点，这个血量有魔瓶魔棒的小小是很难击杀的。 第二章：技能山崩 ( V )用岩石轰炸一块区域，对地面上的敌人造成伤害并眩晕。技能行为：群体效果，需要指定位置施法距离：600山崩半径：275山崩持续时间：2s山崩晕眩时间：1s伤害类型：法术伤害：100/180/260/300魔法消耗：120恒定冷却时间：17秒恒定技能描述：当受到山岭巨人召唤时，毫无生气的石头也会很有杀伤力。 投掷（T）抓起小小附近275范围内最近的单位（敌我皆可），然后投向目标单位或神符，对落地点周围的敌人造成伤害。如果被投掷的是敌方单位，那么这个单位会受到20%%的额外伤害。投掷造成的伤害随长大等级的提升而提高。技能类型：需要目标群体效果施法距离：1300伤害范围：275伤害类型：法术魔法消耗：120恒定冷却：9秒恒定对建筑物伤害：是，并造成33%的伤害如果一个目标在被山崩击中后又被投掷，在其落地前若山崩效果还未结束，他将再次受到山崩的伤害。这句话的解释会在下一章的伤害中给以解释技能描述：小小那巨大的身形让他能够随意抛起最强壮的勇士。 崎岖外表（C）使小小受到的伤害反弹给攻击者。300范围内攻击小小的敌人有一定概率被眩晕。技能行为：被动晕眩概率：10％／15%／20%／25%提高护甲值：1／1.25/1.5／1.75护甲提升：2/3/4/5作用范围:300恒定伤害值：25/35/45/55伤害类型：法术（意味着打小小出bkb不会被晕眩）最近改动：山岭巨人的崎岖外表在攻击完成时才触发（换言之打小小的攻击打到身上才开始计算触发被动与否，而不是攻击抬手的时候判定）技能描述：近战英雄会发现攻击山岭巨人是毫无用处的。 长大（w）技能类型：终极技能技能行为：被动小小崎岖的身体体重不断增长，以降低攻击速度的代价提高攻击力。提高对被投掷单位的伤害并提高移动速度。攻击力提升：50/100/150攻击速度降低：－20/－35/－50移动速度提高：40/50/60投掷伤害提高：35%／50%／65%神仗分裂范围，前方范围400半径扇形神仗投掷伤害提高：50%／65%／80%额外建筑伤害75%神仗攻击距离提升：85 第三章：伤害小小这个英雄，前期的优势在于高攻，高魔法爆发，有大招之后高移动速度； 中期在于跳刀／隐刀单杀压制打钱空间，打团先手后手，有了a仗带线推塔牵制，甚至队友打团小小偷塔，bkb飞鞋偷塔等等 后期在装备了bkb，强袭，银月，大炮，撒旦之类的装备后，正面能力几乎不虚任何主流carry。一个稳定的a仗投掷可以打出1.2 x（80% x 300+300）+两下到三下普通攻击的输出，伤害非常可观。 那么本章将围绕着小小前期加点的伤害进行讨论。 那么玩小小的朋友的困惑多就在于： 主V还是主T？ VT还是TV？ 我多少级能打出多少伤害？ 我多少装备能打多少伤害？ 本帖就将分析一下小小的伤害输出。给大家一个稍微具体的数值概念并且一一解决以上几个问题 问题1:我到底主山崩（V)还是主（T)这个问题相信都遇到过，那么我们来看看小小的技能伤害成长 我们可以看到 山崩 的成长曲线：100/180/260/300我们再来看看 投掷 的成长曲线：75/150/225/300 投掷1级大招加成35%投掷目标额外受到20%投掷伤害山崩时被投掷山崩造成双倍伤害 那么可能直接说起来不够直观，接下来是一组小小在不同时期伤害的截图 1. 在小小1级v1级t的时候vt造成的伤害小小在一级投掷一级山崩的时候造成的伤害对于正常魔法抗性的斧王造成的伤害是700-483，就是217点伤害 2. 小小在7级的时候（点大），主t的伤害，附加1下普攻我们可以看到魔法伤害造成了575点伤害（1000-348-109＋34）总共造成了接近700点伤害 3. 小小在7级的时候（点大），主v的伤害，附加1下普攻我们可以看到小小总共造成了700出头的伤害，其中魔法伤害为608（1000-281-111）略大于主t的输出 4. 小小在7级的时候（点大），3级V 3级T的伤害，附加一下普攻 总共造成了731点伤害，其中魔法伤害为622点（1000-269-109），略大于之前两种的伤害 那么通过以上我们可以得出： 在7级的时候，3级V3级T造成的vt伤害输出最高。 但是我们要考虑到这么一件事情，v不一定能打满4段双倍伤害，因为我们在游戏中不一定能完美二连，换句话说，v的伤害不一定这么高，而且V单独使用的时候，伤害不如T能丢起人的伤害高，T的冷却更短，输出距离更远（KS距离更远）还可以推塔，所以楼主推荐的加点主T。 问题2：VT还是TV:？其实呢，仔细读过更新日志的同学会知道。在短时间内的连击，同样完美重合的施放的时候，TV和VT的伤害是一样的。但是实际的时候还是略有差别的。 以下以4级V4级T,1级大的小小作为试验，同时给出vt在10级（测试的时候不小心升级了）的时候的爆发值 1. TVTV加一下普攻造成的伤害为827点，最后一跳的山崩没有享受到投掷的加成，理论上只要投掷先使用，山崩最后一跳应该都吃不到T的加成，满级v的损失是57点伤害 2. VT由于V的第一段伤害在使用v之后的一小段时间才能造成，VT可以打出满额的双倍V的伤害。在实战中略高于TV 但是以上内容并不代表你要刻意的去追求vt或者tv，这是没有任何意义的 什么时候用TV，在于敌方可能通过吹风／puck相位转移／跳刀／黑鸟毒狗禁锢之类的躲避技能躲过这套连招时使用。或者你怕v空了。。。什么吃个隐身加速v空了的同学，应该不少。 问题3、4：在多少装备多少级的时候能造成多少伤害？假腿的意义在于vt的时候，原来只能a一下，现在可以多a一下。跳刀回音刀的输出是和回音刀一样的。这里先不做测试。 本节可以告诉大家，我多少血的时候能爆发秒掉什么样的人。测试目标为满血2020点，10点护甲的斧王 隐刀 的输出，测试体小小为11级，2级大，假腿隐刀 隐刀 回音刀 的输出 测试体同上 白银之锋 回音刀 的输出 测试体同上 A仗 3级大 白银之锋 回音刀的输出 测试体为16级 A仗 3级大 白银之锋 回音刀 强袭装甲 的输出 测试体为16级 A仗 3级大 回音刀 的输出 测试体为16级 1. 假腿 隐刀在地方护甲10点（38%）和正常物理抗性的时候，假腿隐刀可以在vt的持续时间多a出一下，总共造成1185点伤害.其实落地马上可以多a一下，隐刀一套可以秒掉大约1300血量的敌人。 2. 隐刀 回音刀在装备回音刀之后，整套的输出提高到了1371。其实落地马上可以多a一下，总共理论上可以击杀1500＋的敌人，而且这只是一瞬间，理论上不可能开出任何技能。 3. 白银之锋 回音刀由于有属性加成，这时11级的小小已经来到了2000血，我们可以看到一套可以打出1600＋的血量。配合队友更是可以轻松秒杀2000+血量的敌人。 4. A仗 3级大 白银之锋 回音刀大约可以造成接近1900点的伤害。非常恐怖。 5. A仗3级大白银之锋回音刀强袭装甲的输出不幸的是我们的受体已经无法承受这套小小的输出，一套vt基本可以打出2100+的伤害了而且此时小小非常的肉，2800的血量，26点护甲，抵御61%的物理伤害，这只小小只有16级 6. A仗3级大回音刀的输出大约为1500点输出。这里是给出跳的情况下的概念。跳刀回音刀11级的话伤害大约在1000出头。 总结:小小的隐刀回音刀套路可以秒掉大部分当前版本的核心，有效压制敌方打钱空间，在需要小小出跳的时候也是可以的，但是往往隐刀第一波很难被察觉，可以有效提款地方核心。中期单纯一个投掷加两下回音刀的伤害也是爆表，基本可以提款不到1000血的辅助。 这里注意，小小投掷的是275范围内的最近敌人，隐刀一定要贴近敌人使用。t不丢到人伤害打折很多很多。 第四章：有关对线期对线这个东西很微妙，三言两语很难说清楚，楼主这里具体介绍一下自己的经验，有问题欢迎在本楼回复。 首先我们要理解 小小是一个是高攻的英雄，我们应该尽量在 补刀上压制对手。 小小是一个很脆很脆的英雄，抗兵抗普攻都很吃力，要 学会尽量勾兵 小小v的距离很远很远，600的施法距离加275的半径，最远可以晕到875距离的人，耗血＋耗人，耗血的同时通过晕眩反补远程兵是基本功 小小是一个等级英雄，当你实在因为实力或者敌方中路刚3的而毫无办法塔下挂机的时候，不要担心，你只要有等级，就有中期，实在不行还可以丢队友（ks） v的晕眩时间你走过去就可以t到人，不用担心vt连不到人，你的输出主要靠的是t丢到人＋a一下，如果a人有危险，耗血的方法就是最远距离v，走过去t起来，赶快跑，1点几秒的t的滞空时间够你跑出去很远了。 那么对线的思路是什么呢，这里简单介绍一下出门：不买小圆盾，不买补刀斧，买少量属性甚至不买。 出门楼主一般2个树枝，管队友疯狂求吃树，一般乞求方法是这样的：我一个小小中单打卡尔，求个吃树啊，不然崩了啊，好不好好不好。 然后这两个树枝实在艰难就种了吃掉，不艰难就用来合成大魔棒。 出门525余钱，吃到第一个f走到线上就可以买到瓶子，这个时候最好与队友沟通让队友卡下兵线，让兵线在自己高坡。如果兵险在自己高坡交界处，先站在高坡看看对方有没有线眼，有的话自己想办法买个真眼排了，200块钱回100块的本，而且还能让你少被点很多很多下，别舍不得。 如果兵险在对方高坡，那么第一波兵最好是通过v来推线把兵险推进对方的塔，这样卡尔这种低初始攻击英雄很难补刀，尽量不要让远程兵被反补，用v来补，因为你的瓶子马上在路上了，用两次v，对方没有控制，比方1级卡尔这种，还可以走上去a一下，把蓝打空，开始第一次运瓶子。 这个时候尽量和打野和其他路的队友商量下，因为小小真的很需要用J。一直运瓶耗血，才有可能出现击杀机会。一般来说，中单很少有补给带的特别多的英雄，打炼金，卡尔这种英雄，多耗血，耗几次他自然很虚，你就可以通过勾兵补到更多的兵。不要怕运大药，尽量不回家。 然后2分钟出头的时候瓶子应该回来了，第一次运瓶可以考虑运大药／大魔棒／补刀斧。面对炼金卡尔这种带个补刀斧，自己正补全收难度不大，然后如果出大魔棒的话，就意味着过一段时间可以多放一个t或者一个v，面对宙斯基本露头就可以去v一下耗血，不要怕。 不停运瓶，能控到f可以不运，状态好可以不运，但是最好是一直运，一直用蓝，一个一直运瓶的小小对于对面中单的血量压制是巨大的。我很多打的很亮的局，单杀7000分qop之类的，都是通过耗血，抓住一个机会，在对面400多血的时候一个v，走过去a一下，t起来再a一下完成了击杀。耗血是你自己能发育的基本。要把对方打怂。 第一个晚上的时候可以叫队友来中路击杀，因为4分钟小小基本有5级，伤害很高，加上耗血对面不一定满血，通过投掷有控制的队友，或者自己v先手，很容易完成击杀。并且最好在黑夜的时候问队友要一个中路的线眼，很关键。 这里非常推荐补一个补刀斧，中期刷的很快。 假腿也是很重要的，切假腿喝魔瓶，吃魔棒，vt的时候多a一下，撑血，刷的快。 4级t的时候每个兵有补刀斧的情况下，近战兵a一到两下，然后把远程兵丢向近战兵就可以完美清掉一波兵了。 以上就是对期的基本。 顺带一提，如果有小精灵的话，一直用投掷敌方小兵的方法耗血更简单更安全，还可以补刀。 第五章：有关出装出装这个东西其实很神奇，因为Dota这个游戏千变万化，局势阵容变化多端，没有一套出装可以解决所有问题的。但是基本的核心的东西不会变，叫做见招拆招。 什么叫做见招拆招呢，叫做分析，对面有什么，我该做什么，出什么，才能赢。 这里先不提打3号位或者某些秘法跳刀大根小小，说实话当前版本这种出装非常不稳定，物理小小中期25－30分钟赢一波团多少能推很多线，推1-2个塔，甚至在对面没有买活的时候直接破路，而秘法跳刀大根吹风一类的小小，首先很难躲过对方眼位，其次杀完人不肉可能刚不起来，再其次真的推不了塔。 那么物理小小的出装思路是什么呢鞋类物品：由于当前版本对力量加血的提升，我首推假腿，多的这几点血量很关键，并且能够在vt的时候比相位鞋多a一下。这多a一下的输出肯定比那24-8的攻击力高的多。同时假腿补刀斧刷的很快。一波野怪一个t，a两三下就没了。 有关魔法：carry小小出门可以带个芒果省的一直用j，补给可以多管辅助要，中单小小的出门应该以速瓶为主，瓶子魔棒好好使用的话应该蓝已经很够了，多叫队友tp灌瓶子，多运瓶，只有你耗对面血的，很少有人耗的过小小，思路和龙骑士是一样的，只不过小小由于有vt的存在，6级之前击杀机会会比龙骑士多的多。同时，回音刀也可以提高魔法恢复速度，虽然是百分比回蓝，但是不乱用的话已经够了。 蓝够了，有鞋子了，接下来应该是小小的切入装备 跳刀／隐刀这两个选择其实很简单 当你方能打到对方的输出足够的时候，不需要你去躲过眼位，不需要你打出更高爆发的时候，出跳，打个比方 对面全是脆皮，1000血不到的暗牧毒狗，1000血出头的卡尔影魔，你可以出跳，直接一套打死 当你能用跳刀切入队友能跟上的时候，比方说队友有一个隐形刺客，你们一起去抓敌法师，你的输出不够，sa的输出也不够，但是你能跳刀vt到敌法的时候，你们俩输出就够了，但是这并不代表什么，可能到中期对面眼位更加到位，抱团更紧，防范意识更强的时候，你的队友就不一定能过去了，跳刀的输出不足很快就会暴露出来 跳刀比隐刀刷的更快，跳刀补刀斧，跳过去a一下一个t再a一下基本一个大野怪就死了。如果你出跳刀可以打出经济压制，也可以出跳。 那么什么时候出隐刀呢？ 你可以通过隐刀单杀更重要的人，不要用隐刀冒着风险去单提一些没意义的人，如果你的隐刀可以躲过眼位，比方说大隐刀回音刀的小小可以单杀一个假腿狂战单刀的敌法师，这种时候，你的隐刀会极大缩小敌法师的带线空间，价值远高过你这把大隐刀的钱，这点是最重要的。 去除被动，这个很好理解，大隐刀破被动，bkb也解不了，自己注意就行。 大隐刀给予你15点全属性。多一点属性多一点被抓的时候反杀的可能，蓝也更多，攻速更快。 不要觉得隐刀就是虐菜的，隐刀可以躲过眼位，并且隐刀会给敌方辅助带来经济压制。 不适用于什么情况呢，敌方速推，你不是很顺的时候，你不能做到用隐刀无线单杀，带线，然后马上做出a仗强袭之类的正面装备回来刚团。你家都15分钟就掉了，你12分钟的隐刀能做什么呢。代表情况，敌方无解肥3级书兽王，我方大哥可以几波之后翻盘，如幽鬼 核心装备: 回音刀，提高回蓝，刷钱速度，10点力量，提高爆发，并且5秒的cd可以完美配合第二波进来t起来人进行输出。减速也非常适合追击战。我们在第三章里面可以看到，装备越好，回音刀的效果越好，当你有a仗的时候，每多一下普通攻击多的伤害是非常多的。在打到中期的时候，一个丢起来人的t加上回音刀多a一下的输出，可以打出过千的伤害，这是非常恐怖的。 A杖，具体的神仗提升已经写在第二章的技能介绍里，神仗的提升非常巨大。最重要的是，赋予了小小一个非常强的推塔能力，后期小小带盾拍塔逼塔防，一个8-10秒bkb直接拆完一路的情况并不少见。神仗配合回音刀可以两刀一个t一波兵，非常快，如果有补刀斧可以两刀直接一波兵。 面对不同情况对应出的装备: bkb＋飞鞋：偷塔用，后期打不过了，在破了对面路的情况下，很容易线带过去，对方出来打团，给小小30秒不到就可以拆掉一路，回来人杀不掉你直接无视继续拆塔，如果杀的掉你直接隐刀／bkb／飞鞋逃走，甚至飞到别的路。一般在有小小而且破一路的情况下，对面的线很难很难再过来了。 大炮银月bkb，在无论你多少护甲多少血量都会死的情况下（类如doom＋卡尔），你想赢的正面团的方法就是巨大的输出，大炮银月bkb，切入直接把你能打的输出打出来，吃掉对面所有的技能，等待我方carry收割。 林肯：当对面有个只盯着你大的兽王／蝙蝠／祸乱之源／vs，并且他们拥有视野优势，且你不可能躲避他们的大招时，可以考虑出林肯，如果对方还有很多可以破林肯的装备的时候，强袭撒旦bkb扛过大招反打，或者靠切入大炮银月bkb打爆炸输出，或者bkb加飞鞋强袭带线牵制带塔，都是可以考虑的，具体情况需要具体分析。 金箍棒：在面对PA／蝴蝶／风行者，且你可以切入碰到他们的时候出。 面对刃甲／物理输出：请出强袭，甚至强袭＋冰甲，护甲越高，受到刃甲反弹伤害越低，并且不需要你出bkb的情况下，请把bkb换成冰甲或者撒旦，类如地方无解肥刚被斧王，刃甲玲珑心疯狂冲脸。 其他的装备我出的很少，一个正常的小小应该做到尽快结束战斗，在a仗的时候追求带盾，带盾的时候追求破路。 附带若干出装思路图6分钟左右应该有： 如果能在十多分钟的时候出到跳刀回音刀，小小将会很凶残 这套装备单杀很凶残。切入爆发都很爆炸。大隐刀cd24秒，持续14秒，自己注意一点，基本上哪怕杀不掉人也可以走。 当然神仗是必不可少的这套装备的小小，已经可以在10秒左右拆掉对面的高地塔了。 无解肥小小自然最好不要死，那么这个时候要上高最好的方法就是带个盾。想办法打盾上高地。队友出个骨灰推推救一下，在后面看就好，很安全！ 如果有钱买bkb或者强袭并且对面不能一波直接反推的情况下就直接更了吧。推得稳一点。 极限输出装，打doom这样基本上切进去把你所有的输出打出来，剩下的就是看大哥了。这套装备已经有4100多血了。。 强袭正面能力很强，减甲加甲，对拼。这套装备我认为对拼最强，正面最刚。而且强袭可以抵抗刃甲。 在没办法的时候，带线偷塔也是没办法的，bkb tp＝无敌的情况不少，隐刀这里可以换跳，大炮可以换强袭，拆塔更快。 差不多就是这些装备了。冰甲林肯的局不多，国服doom多一点，我推荐打doom别出林肯，直接出撒旦大炮强袭，刚特么的 第六章：前中后期思路其实思路在出装里面体现了很多。 前期：运瓶，控f，耗血，寻求单杀。被压制了就追求等级，别把对面送的太肥，对面去打架了你就赶紧用t去刷点基本装备就好，别再去打架一蹦到底。这个版本对线要多勾兵，小兵仇恨时间提高，对面很难压制近战中单，塔给你加1点甲，别人点你少掉血。远程兵经验多，下半血a一下，一个v控制对面英雄，再a一下基本都能反补掉。再想办法把对面的高坡眼反掉，注意自己血量以及反gank，这个中路基本占的很稳。 中期：通过跳刀隐刀去带节奏，通过回音刀提高爆发和刷钱速度，尽量少回家多带塔。想办法击杀敌方carry中单，大部分都是能通过大隐刀＋回音刀单杀的，也基本不存在能开出技能的，除非提前准备好。尽量避免那种满地真眼的大范围团战，如果你看到对面辅助身上10个真眼，那就安稳的刷就是了，真眼不是无限的，也不是不要钱的。等待我方carry崛起或者自己carry想办法带队友偷盾或者赢肉山团，带盾bkb/强袭上高地。一般来说打出这种节奏破了一路的局，tiny自己留好买活钱，很难输很难输。 后期：看准情况出装，有的阵容你们就是打不了正面团，别管队友，分数是自己的，去偷塔，去偷，去换家。你是小小。你买活飞鞋几秒钟对面家就没了，不要怕。如果可以刚正面，当然是正面装备带队友一起赢最开心啦。具体后期出装看上一章，这里不再阐述。 第七章：Dota永远不是一个人的游戏，你有4个队友（不是4只meepo）和5个对手。一个人对阵容的理解，不能通过一篇这样简单的攻略提高很多很多，但是却可以通过不停游戏的体验，看高分replay的观察慢慢积累，这种累积造成了实力的差距。我一直不是一个操作多好反应多快的选手，dota对我来说是通过积累提高的。 一局Dota，当你选择英雄之前，你就要开始分析阵容，当一个阵容拿到手，你就要有判断： 这个阵容，我出什么装备能单杀谁，我出什么装备能赢，我们这个阵容正面打不打的过，我要吃哪些技能，我不能吃哪些技能，我这局能不能死 有的阵容看上去很完美，lion 暗牧 劣势路ds 中单小小，carry电魂。每条路看上去都可以打出优势，lion可以抓人，暗牧可以救人 ，电魂线上可以单，小小中路可以配合lion打出优势。但是你很快发现，过了对线期，你们打不过团，为什么，因为小小没装备的时候容易死，暗牧薄葬加了奶不回来，整个阵容无限缺输出，ds走不上去，电魂被控制只有一个f和大，输出不够了，lio没跳好难留人。那么这个时候你就要站出来做一号位了，你要刷起来，补刀斧疯狂刷，刷他妈的6神，我不能死，我不能吃doom大，我不能吃卡尔一套满技能，我bkb没了我就要走。看上去很自私，但是这是赢的唯一方法。 同理，换一局，我方中单小小劣势路宙斯大哥幽鬼辅助毒狗打野军团。这也是一种很常见的阵容，这种阵容分析下来叫做，劣势路宙斯输出很高，军团和你可以切入打先手给幽鬼收割，毒狗可以救切入进去的你。每个人都有用，你这局要做的就是撑住场面，想办法给幽鬼拖住时间，当然你自己刷起来也可以，但是一局游戏，不那么自私的话，乐趣会更多不是么。双跳刀干特么的。 上面吹的有点多了。接下来说一下小小和其他英雄的相性。 小小喜欢什么样的队友： 我可以让你先手，我能用v抢人头的屠夫，人马，大鱼人，lion之类。 你输出高我帮你打控制的宙斯，火女之类。 你可以救我我可以无脑拆的神谕者，毒狗，vs，暗牧（暗牧加护甲真的很屌） 回蓝冰女，加速兔子之类 不喜欢什么对手： 救人的，同理，不过救人的并不是最讨厌的，你在这些辅助出保命装之前把他们提个够就行了。中期盯着他们秒就好了。 魔免的小狗，撑到2000血左右的小狗。小小拿它毫无办法，被狗翻过很多盘。 减甲，真的是很烦减甲配合物理高爆发代表英雄sf vs 大鱼 暗牧 魔法爆发其实也还好，补一手bkb，但是怕的是那种4个人5个人全是技能伤害最典型的是dp nec，dp的大招物理伤害又无视魔免，抽血百分比掉血小小血量高毫无作用，只能出bkb强袭，但是bkb强袭之前没有其他装备铺垫又很蠢，dp推的又快。最好还是跳刀配合队友直接秒掉。 差不多就先写这些吧。珍惜你的队友和对手，他们都是你成长路上的石头。最后谢谢各位看官读到这里，我们下篇攻略再见。","categories":[{"name":"游戏娱乐","slug":"游戏娱乐","permalink":"http://yoursite.com/categories/游戏娱乐/"},{"name":"Dota2","slug":"游戏娱乐/Dota2","permalink":"http://yoursite.com/categories/游戏娱乐/Dota2/"}],"tags":[]},{"title":"DOTA职业圈，被黑出翔的那些梗","slug":"DOTA职业圈，被黑出翔的那些经典名人名言","date":"2016-05-24T14:14:57.232Z","updated":"2016-06-19T07:13:43.202Z","comments":true,"path":"2016/05/24/DOTA职业圈，被黑出翔的那些经典名人名言/","link":"","permalink":"http://yoursite.com/2016/05/24/DOTA职业圈，被黑出翔的那些经典名人名言/","excerpt":"","text":"NO.1： ”拉谁!说话!拉谁!“出自怒吼天尊XB。 XB在某次比赛中，使用自己擅长的蝙蝠骑士(现已冠名XB骑士)，非常嚣张的嘲讽道：“拉谁!说话!拉谁!” 于是…… 他拉了一个小兵，全世界震惊了。从此XB骑士的美名享誉全球，XB的机智打动了在场的所有人，掌声经久不息 NO.2：“给我幽鬼，输了砍手!”出自伊人HAO(砍手HAO) HAO娘刚出道没多久的时候。俗话说，初生牛犊不怕虎，面对强悍的对手，HAO娘自信的要求队友给他PICK幽鬼，“输了砍手!” 从此砍手HAO的外号不胫而走。 江湖上盛传，我有上将HAO，可斩ZHOU神! 不过HAO娘随着这几年的成长，越发的成熟，已经成为了最顶级C之一!砍手HAO的美名也逐渐被伊人HAO所代替. NO.3：“就是在，有时候比赛输了，我会觉得队友不给力。”出自徐志雷(BURNING,大B神,BB) 最近版本变更，加上DK人员调整，以及之前的所谓B神“一踢四”的风波，再加上最近BB的表现不尽如意。让我们的BB站在了风口浪尖，之前在G1冠军联赛上说过的这句话又被很多人拿出来旧事从提~BB号称是能够1V9的男人，所以输了比赛，当然怪队友不给力咯~ 其实啦，你们如果继续看这个视频，会发现BB之后有说到，“但是，DOTA是5个人的游戏。。。。。。”总而言之，就是输赢是要大家一块承担的意思 不过，想黑人~断章取义可是最容易的手段哦~ NO.4：“打个比赛像农民一样!”出自ZHOUZHOU~ 某次比赛，当时的大ZHOU神由于不满XB的无尽嘲讽，为了发泄，说出了这样一句让我大中华广大农民愤慨的话。结果呢，这句话并没有影响到XB，反而给ZHOUZHOU自己扣上了农民ZHOU的帽子…… 突出一个惨。 NO.5：“真替狗哥不值!”出自怒吼天尊XB 紧接着NO.4，当ZHOUZHOU不满XB嘲讽进行回敬之后，XB再次使出杀手锏! 当时，XB指着看台反嘲一句：“谁先骂的，昨天比赛是谁骂的狗哥(zhou前队友兄弟，ig合并时被踢出去的三生)，我真替狗哥不值!狗哥，奈斯不奈斯!?” 结果台下竟然一片“nice”回应，ZHOUZHOU嘲讽大败而归。 之后，真替XX不值这句话就成了适用范围极广的一句话。 例如，当YAO接受采访表示要打一直以来暗恋自己的花花(性别未知)的时候，广大网友纷纷表示“真替花花不值!” NO.6：“来DOTA1不把你打哭?”出自某不知名非职业选手NL_KS 当时我们的NL_KS大神还在90016DOTA1战队，他们的战队名叫GREENDY。由于入驻DOTA2比较晚，(其实就是技不如人。。。。。。)在进行一场DOTA2比赛的时候，惨遭血虐，被广大网友嘲讽。 我们的KS大神按耐不住心中的怒火，一句“来DOTA1不把你打哭?”硬是强行登基!GREENDY成功成为DOTA1宇宙第一强队! 衍生版本，来DOTA1卜把妮打哭(为什么要这么写…懂的自然懂)。 NO.7：“对不起，这个比赛我要赢。”出自09(牛肉9，大菊观9) 若干年前的SOLO大赛，决赛的对阵双方是09和LONGDD，当09的炼金在河道处击杀了龙DD的小J的时候，09霸气的打出了这样一句话“对不起，这个比赛我要赢。”当然，最终他的确赢得了比赛，获得了冠军。 尽显装B本色，广大网友不服不行，唏嘘不已。 NO.8：“我又不是最菜的!”再次出自怒吼天尊XB(话题小王子啊，不是我想黑你的) 今年的DK人员大调整，直接让旧DK除BURNING以外的所有人全部离队，作为劣势路抗压，总被网友诟病冲动爱送爱崩盘的XB，在接受一次采访的时候表示，非常不能够接受这样一种被离队的结果，明确表示“我又不是最菜的!” 不过之后XB SAMA的一次次“惊艳”表现再次印证了你就是最菜的那个!(玩笑话，XB就是这么个打法)。 每当XB被各种姿势艹翻的时候，网友们都翻出这句话来，毕竟，不是最菜的。 NO.9：“我TM又不是SB，我亲自送她上的飞机!”出自PIS(达蒙皮，P神) P神的绿帽事件，我就不再科普了，毕竟公共场合，有伤风化!说多了都是泪。 总而言之，当广大网友将诸多证据摆在PP面前的时候，PP选择在YY直播，做出澄清，对于师母到底飞没飞海南这件事，PP斩钉截铁的表示“我女朋友我还不知道么?我亲自送她上的飞机!我TM又不是SB!”尽显男儿本色，霸气外漏! 不幸的是，仅仅两小时之后，PP就再次回到YY表示，网友的爆料，情况基本属实，绿帽已戴，好聚好散。 NO.10：“这个比赛白送了!”出自怒吼天尊XB(哥你上镜太多了点吧) 当时的老队长XB已经来到了VG战队，同样打3号位，作为1号位的TUTU，之前表现不佳，不过状态逐渐回升，VG战队越战越勇，不过当TUTU得知自己如果表示不好，可能会被SYLAR换掉的时候，再加上自己家中有急事(记得好像是父亲重病)，无法集中精力比赛，状态不是很好，VG战队在NEST比赛中也失败而归。 而爱冲动的老队长在没有完全了解TUTU情况之下，在微博上说出了这些气话“这个比赛白送了!”认为TUTU是导致VG的落败原因，这个比赛算白送给别人了。 于是我大XB SAMA再次被黑，之后的比赛，一旦VG落败，网友都会在聊天频道调侃“这个比赛白送了!” NO.11：“不要放海涛切假腿。”出自海涛 海涛作为很多DOTA玩家的启蒙老师，其对DOTA做出的贡献无可厚非，再加上对DOTA2也是极力推广，不可不谓良心涛，虽然他被网友诟病“解说不专业，打的菜，话痨”。不过LZ本人现在对他还是持肯定态度的! 不过，千万，千万，不要放海涛切假腿。假腿就是动力鞋，科普一下，切假腿，是因为动力鞋有三个形态，可以切换。传说中，当你对面有海涛的时候，必须全场GANK他，不要放海涛打钱，因为，一旦海涛打出假腿的钱，合成了假腿，你们就已经输了。海涛切起假腿来，APM瞬间可以达到600以上，曾经仅靠一个假腿冰女通过超高频率的切假腿，秒杀6龙心人马，成为一段神话! 其实是海涛在自己的视频中，总是喜欢强调切假腿的妙用，这一个小小的假腿似乎爆发出了不可估量的作用，于是被网友黑出了翔! NO.12：“我们打比赛从不20投”出自ZHOUZHOU(农民ZHOU，智障ZHOU) 在一次接受采访的时候，记者刁钻的问起了DOTA与LOL的区别，智障ZHOU突然大脑短路，苦思冥想，得出结论，微微一笑“我们打比赛从不20投!” 悲剧的是，就在接下来的比赛之中，由ZHOUZHOU带领的IG，在坚持了16分钟之后，就无奈打出了GG，创造了史无前例的16投的传说!狠狠的自己扇了自己一巴掌。20投也成为了扣在ZHOUZHOU头上一顶摘不下来的帽子。。。。。。 其实呢，ZHOUZHOU后来在微博上澄清，这句话是别人怂恿他说的，并不代表他的观点，不过，WHO CARE? NO.13：“影魔已经不适合这个版本了。”出自单车(单车武士) 游戏风云直播，前职业选手单车，打水友局，对面水友霸气的手选了影魔，手选影魔的意思就等于，欠艹，求虐求侮辱，不服来SOLO!这能忍?单车思考了一下，选择了自认为拿手，并且可以针对影魔的神灵武士! 说了一句可能最让他后悔的一句话“我要证明，影魔已经不适合这个版本了。”结果不到6级，单车的神灵武士就已经连续被对面影魔艹翻几次，看着0杀5死的数据，单车表示“这个Z炮怎么可能压到我?”“我不是那种一崩到底的选手!”接着单车武士继续被杀，继续着他的传说。 旁边的BBC意味深长的说“看来影魔还是挺适合这个版本的吧”从此神灵武士冠名单车，正式更名为单车武士! NO.14：“除了B神，其他C都差不多。”出自XIAO8(下面8) 在刚刚结束的WPC-ACE联赛上，LGD战队由于之前SYLAR的赌气离队，找来了小兔几当C，经过短暂的磨合，效果竟然出奇的好，得意于LGD战队的执行力与团结。一次赛后，作为队长，XIAO8接受采访，主持人问到，LGD的C位变动，有没有导致战术等等方面的连锁变化，又或者这两个不同的C，风格，打法上有什么不同吗?XIAO8想了想，说出了“除了B神，其他C都差不多!” 这句话显然是对B神的认可，B神，代表了C位的标杆。更是对LGD战队的自信，LGD战队可以扶起任何一个C! 不过呢……由于B神最近的低迷表现，这句话又被拿出来赋予了新的含义，即，其他C都差不多，只有B神是最腿的，B神是C位的下限。。。 NO.15：“我叫战神7，和你一换一!”出自战神7(教练) 战神7的个人能力，看过他打路人的都知道，不用我赘述了，国F几根最粗大腿之一!突出一个虎! 不过正如他的ID(上头猫)，这名选手打起职业来，脑子不是很好使，虽然自己打的中单2号位，却经常做出1V3，1V4的举动(当然跪了)，而且尤其喜欢跟对面的辅助一换一，突出一个谁不服我就跟你一换一! 从此，我叫战神7，和你一换一的美名就传播开了，战神7也迫于压力没有继续打职业，不过他仍然活跃在职业圈，希望大家支持这样有干劲，有能力的选手! NO.16：“干干干!救救救!阿西吧!”出自宝哥 宝哥是以前老DK战队的5号位，专职辅助B神，老好人，现在没打职业了，在YY90016做直播，有时候会切换成“支付宝”形态，强行“支付”送人头! 大家脑补一下以下场景。 宝哥身先士卒，杀入敌阵，口中喊着“干干干!”，结果发现自己不是B神不能1V9，快要体力不支倒下了，焦急的宝哥大喊“救救救!”，结果队友一看宝哥又切换“支付宝”形态了，毅然选择了放弃宝哥，宝哥终于壮烈牺牲，宝哥长吁一口气“阿西吧!” NO.17：“我们的猪肉粒含有牛肉成分!”出自09(牛肉9，翔9，偷钱9) 我们都知道著名大忽悠09先森，通过自己在DOTA取得的一定成绩，将自己包装成了一个全民偶像，聪明的09利用自己的名声做起了淘宝生意，食品店那叫一个火爆! 其中很著名的产品就是OX牛肉粒，结果细心的网友调查发现，该牛肉粒根本不是牛肉做的而是猪肉做的!结果却明目张胆的起着牛肉粒的名字! 我大酒神迫于压力站出来辟谣，我们的猪肉粒，是含有牛肉成分的! 至于你信不信，我反正信了! NO.18：“我就问你满足不满足!”出自XB 某次DK跟TONGFU比赛，线下赛，没有隔音室，双方可以尽情嘲讽。这不正中老队长XB的下怀吗?怒吼天尊不是白叫的，于是XB在比赛中，就让对面明白了。 送!送!送!会不会玩! 杀我，杀我!咬你就是两口! 我就问你满足不满足! 满足不满足! 可怜的香蕉和HAO娘根本没见过这个阵势，直接给吓傻了…… 满足不满足，这句话也成为了嘲讽的必备!当你打路人艹翻全场的时候，一句，我就问你满足不满足，尽显霸气本色! NO.19：“打的跟NM要赢了一样!”出自LONGDD(霍比特龙，AKK龙，翔龙，DOUBLE DICK LONG，喝汤龙，长沙赵子龙，吹B龙) 著名单口相声演员LONGDD在90016的单口相声，相信很多吧友都听过。像我们龙神这种混黑社会的，一把砍刀从东边砍到西边，血浮血海的，免不了结了很多仇人。俗话说，常在河边走，哪能不湿鞋! 龙神直播的时候，经常会匹配到自己的仇家，仇人相见，分外眼红，对面经常打的十分凶残，满场干!无奈我大龙神的护龙山庄，高手如云，几大护法武功高强，对面路人也是有心杀贼无力回天!虽然打的很有血性，可惜却仍旧败下阵来! 这时候龙神喝一口汤，嚼一口槟榔，轻蔑的一句“打的跟NM要赢了一样!”，霸气十足! NO.20：“FEN SHOU KUAI LE, ZHAO JIE SHI WO DE LE。”分手快乐，赵洁是我的了!出自DENDI(乌克兰老司机) DENDI一直以搞怪出名，不过他所在的NAVI战队成绩却是十分傲人的!一次与LGD的比赛临近结束，NAVI战队获得胜利，DENDI当着几万人看直播的面，在公共频道打出了fen shou kuai le ,zhao jie shi wo de le !这样一句话。 要知道，赵洁当时还是LGD战队队长小8的女朋友，两人之前因为一些摩擦闹了矛盾，甚至要分手。结果DENDI竟然在比赛中来了这么一句话!当时小8就忍不住回了句SHA BI! 事后，DENDI道了歉，表示是别人教他这么说的，他也不知道什么意思。至于到底谁教的?有人说就是赵洁本人，有人说是赵洁的好友狐狸妈(430前女友)，至于到底是谁?笑笑就好~~ NO.21：“最多55开!”出自LONGDD 我们都知道，55开用来形容双方形势差不多，比赛难以预料，旗鼓相当。 可是DOTA比赛上出现真正55开的局面并不很多见，于是，当局面一边倒的时候，为了稳定观众情绪，活跃气氛，使比赛充满悬念，著名相声演员LONGDD只能强行55开了! 某比赛，20分钟出头，比分已经26比3，劣势方外塔掉光，只能高地打麻将。 狗哥：“龙神，你说现在这个局面，几几开，给我们分析分析?” LONGDD思索了一下：“最多55开，不能再多了!” 狗哥：“这还55开啊?真的假的?” LONGDD：“强行55开，不能再多了!” NO.22：“3154”出自卡师傅 其实是最近newbe和秘密的比赛，当时秘密打了个盾，卡师傅记盾消时间3154打在公屏了，结果EE真的在31分50秒带盾上高，然后盾消失了，EE送了，秘密死了4个，于是这个就成了一个EE中国心的梗。 ee：我的中国心 先来预热，脑补一下 连续输给中国队后，秘密战队开了紧急会议。 讨论室内，5名队员并排而坐，气氛异常紧张。突然，Puppy坚定的说道：我们中间有叛徒！其他队员沉默不语，低头思索着什么。“叛徒是EE”队员们齐刷刷抬头，原来是RTZ在说话。“EE你经常秘之送人头，你一定是CN派来的间谍。”“我是奸细？你RTZ中单被人把屎都压出来了，我看你才是CN的内奸”EE怒目而视，反驳道。“我看你经常打神秘电话，还用粤语，你一定将我们的战术透露给了CN战队”RTZ越说越激动，已经站了起来。“顶你个肺。你在EG的时候，EG输给CN队；你来秘密，秘密又输给CN队。你不是叛徒谁是叛徒？”EE也站了起来，似乎想和RTZ干架。两个人眼看就要打起来了，坐在一旁的Puppy终于开口：”行了，都别吵了。也许根本就没有叛徒，是我多虑了。散会吧。大家都好好思考一下问题所在。”RTZ和EE怒目而视，骂骂咧咧，分别从左右两侧的门离开讨论室。刚一出门，EE冷汗直冒“最近演的确实有点过，今天差点暴露了。自从B皇去了VG后，了无音信。也不知道组织还需要我做什么，只能尽力协助。 还好我今天把责任推到RTZ身上，总算没露马脚。B皇，组织难道把我忘了吗？。另一边，RTZ出门后，也是深吸一口气“他妈的，差点暴露了。最近确实送的有点多。自从ROTK去了VG后，没有任何消息。也不知道组织还需要我做什么，只能尽力协助。还好我今天把责任推到EE身上，总算满混过关。ROTK，组织难道把我忘了吗？”在另一片遥远的大陆，在上海滩边，在黄浦江。ROTK和B皇对立而坐，相视一笑。他们虽然再也有任何机会为国出战。但是整个CN的命**运，就握在他们两人手中。 NO。23：YYF经典语录我只能说，XXXXX，好吧！ 这个符抢不到我退了！ 他不死我退了啊！ 对面什么梗啊，要干我 救救救！ 这波我很强，可以打！可 谁变个鸟啊，没鸟我退了 谁变个鸟啊，很急，很关键！ 强壮的我 血崩啊！ 这波不亏 XXX还是XX玩得好啊 这他跑不了吧，他跑了我退了！ 这个符被抢了我退了！ 这波我很强 他们绝对有人蹲 YYF：你们有没有鸟啊，很急 黄翔：没有，没钱 或：FNMDX，现在才2分钟 英服累目 我草！ 这游戏叫抗压…… 这波我很强好吧…… 救，救，救 我是一个飘逸的女人，飘飘飘我疯狂的在飘 哎！哎！艾！（然后死了） 这个游戏叫我很肥好吧，不多说 这个游戏叫做再也不会死一次（然后死了） 黄翔你别急，我很生气，现在我坐起来了 这个游戏叫做我很肥他们全TMD都要死","categories":[{"name":"游戏娱乐","slug":"游戏娱乐","permalink":"http://yoursite.com/categories/游戏娱乐/"},{"name":"Dota2","slug":"游戏娱乐/Dota2","permalink":"http://yoursite.com/categories/游戏娱乐/Dota2/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-XII-透视表和交叉表","date":"2016-05-24T13:24:48.818Z","updated":"2016-06-19T07:13:16.061Z","comments":true,"path":"2016/05/24/PANDAS常用手册-XII-透视表和交叉表/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-XII-透视表和交叉表/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验介绍 本实验将学习透视表和交叉表相关的知识 二、透视表 透视表（pivot table）是各种电子表格程序和其他数据分析软件中一种常见的数据汇总工具。它根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到各个矩形区域中。在 Python 和 pandas 中，可以通过我们前面所学过的 groupby 功能以及（能够利用层次化索引的）重塑运算制作透视表。DataFrame 有一个 pivot_table 方法，此外还有一个顶级的 pandas.pivot_table函数。除能为 groupby 提供便利之外，pivot_table 还可以添加分项小计（也叫做 margins）。 回到上一节实验课的小费数据集，假设我想根据 sex 和 smoker 计算分组平均数（pivot_table 的默认聚合类型），并将 sex 和 smoker 放在行上：12345678In [31]: tips.pivot_table(index=['sex','smoker'])Out[31]: size tip tip_pct total_billsex smoker Female No 2.592593 2.773519 0.156921 18.105185 Yes 2.242424 2.931515 0.182150 17.977879Male No 2.711340 3.113402 0.160669 19.791237 Yes 2.500000 3.051167 0.152771 22.284500 这对 groupby 来说也是很简单的事情。现在，假设我们只想聚合 tip_pct 和 size，而且想根据 day 进行分组。我将 smoker 放到列上，把 day 放到行上: 1234567891011121314In [35]: tips.pivot_table(['tip_pct','size'],index=['sex','day'],columns='smoker')Out[35]: tip_pct size smoker No Yes No Yessex day Female Fri 0.165296 0.209129 2.500000 2.000000 Sat 0.147993 0.163817 2.307692 2.200000 Sun 0.165710 0.237075 3.071429 2.500000 Thur 0.155971 0.163073 2.480000 2.428571Male Fri 0.138005 0.144730 2.000000 2.125000 Sat 0.162132 0.139067 2.656250 2.629630 Sun 0.158291 0.173964 2.883721 2.600000 Thur 0.165706 0.164417 2.500000 2.300000 还可以对这个表作进一步的处理，传入margins=True添加分项小计。这将会添加标签为 All 的行和列，其值对应于单个等级中所有数据的分组统计。在下面这个例子中，All 值为平均数：不单独考虑烟民和非烟民（All 列），不单独考虑行分组两个级别中的任何单项（All 行） 12345678910111213141516In [38]: tips.pivot_table(['tip_pct','size'], ....: index = ['sex','day'],columns='smoker', ....: margins=True)Out[38]: tip_pct size smoker No Yes All No Yes Allsex day Female Fri 0.165296 0.209129 0.199388 2.500000 2.000000 2.111111 Sat 0.147993 0.163817 0.156470 2.307692 2.200000 2.250000 Sun 0.165710 0.237075 0.181569 3.071429 2.500000 2.944444 Thur 0.155971 0.163073 0.157525 2.480000 2.428571 2.468750Male Fri 0.138005 0.144730 0.143385 2.000000 2.125000 2.100000 Sat 0.162132 0.139067 0.151577 2.656250 2.629630 2.644068 Sun 0.158291 0.173964 0.162344 2.883721 2.600000 2.810345 Thur 0.165706 0.164417 0.165276 2.500000 2.300000 2.433333All 0.159328 0.163196 0.160803 2.668874 2.408602 2.569672 要使用其他的聚合函数，将其传给 aggfunc 即可。例如，使用 count 或 len 可以得到有关分组大小的交叉表： 1234567891011121314151617181920212223242526In [41]: tips.pivot_table('tip_pct',index=['sex','smoker'], ....: columns='day',aggfunc=len,margins=True)Out[41]: day Fri Sat Sun Thur Allsex smoker Female No 2 13 14 25 54 Yes 7 15 4 7 33Male No 2 32 43 20 97 Yes 8 27 15 10 60All 19 87 76 62 244In [42]: # 如果存在空的组合（NA），我们会设置一个 fill_valueIn [43]: tips.pivot_table('size',index=['time','sex','smoker'],columns='day',aggfunc='sum',fill_value=0)Out[43]: day Fri Sat Sun Thurtime sex smoker Dinner Female No 2 30 43 2 Yes 8 33 10 0 Male No 4 85 124 0 Yes 12 71 39 0Lunch Female No 3 0 0 60 Yes 6 0 0 17 Male No 0 0 0 50 Yes 5 0 0 23 pivot_table的参数说明如下： 参数名 说明 values 待聚合的列名称。默认聚合所有数值列 index 用于分组的列名或其他分组键，出现在结果透视表的行 columns 用于分组的列名或其他分组键，出现在结果透视表的列 aggfunc 聚合函数或函数列表，默认为’mean’。可以是任何对 groupby 有效的函数 fill_value 用于替换结果表中的缺失值 margins 添加行/列小计和总计，默认为 False 三、交叉表：crosstab 交叉表（cross-tabulation，简称 crosstab）是一种用于计算分组频率的特殊透视表。下面这个范例数据很典型，取自交叉表的 Wikipedia 页：12345678910111213In [52]: dataOut[52]: Sample Gender Handedness0 1 Female Right-handed1 2 Male Left-handed2 3 Female Right-handed3 4 Male Right-handed4 5 Male Left-handed5 6 Male Right-handed6 7 Female Right-handed7 8 Female Left-handed8 9 Male Right-handed9 10 Female Right-handed 假设我们想要根据性别和用手习惯对这段数据进行统计汇总。虽然可以用 pivot_table 实现该功能，但是 pandas.crosstab函数会更方便：123456789101112131415161718192021222324In [53]: pd.crosstab(data.Gender,data.Handedness,margins=True)Out[53]: Handedness Left-handed Right-handed AllGender Female 1 4 5Male 2 3 5All 3 7 10In [54]: #crosstab 的前两个参数可以是数组、Series 或数组列表In [55]: #再比如对小费数据集In [56]: pd.crosstab([tips.time,tips.day],tips.smoker, ....: margins=True)Out[56]: smoker No Yes Alltime day Dinner Fri 3 9 12 Sat 45 42 87 Sun 57 19 76 Thur 1 0 1Lunch Fri 1 6 7 Thur 44 17 61All 151 93 244","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-XI-分组级运算和转换","date":"2016-05-24T13:24:42.795Z","updated":"2016-06-19T07:13:28.885Z","comments":true,"path":"2016/05/24/PANDAS常用手册-XI-分组级运算和转换/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-XI-分组级运算和转换/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验介绍 聚合只不过是分组运算中的一种而已。它是数据转换的一个特例，也就是说，它接受能够将一维数组简化为标量值的函数。在本节中，我们将学习 transform 和 apply 方法，它们能够执行更多其他的分组运算。 二、分组级运算和转换初探 假设我们想要为一个 DataFrame 添加一个用于存放各索引分组平均值的列。一个办法是先聚合再合并：1234567891011121314151617181920212223242526272829303132333435In [67]: df = DataFrame(&#123;'key1':['a','a','b','b','a'], 'key2':['one','two','one','two','one'], 'data1':np.random.randn(5), 'data2':np.random.randn(5)&#125;)In [68]: dfOut[68]: data1 data2 key1 key20 0.165949 -0.470137 a one1 1.780182 -1.002254 a two2 -0.371285 0.543113 b one3 -1.675172 0.691865 b two4 1.591724 -2.087385 a oneIn [69]: k1_means=df.groupby('key1' ....: ).mean().add_prefix('mean_')In [70]: k1_meansOut[70]: mean_data1 mean_data2key1 a 1.179285 -1.186592b -1.023229 0.617489In [71]: pd.merge(df,k1_means,left_on='key1', right_index=True)Out[71]: data1 data2 key1 key2 mean_data1 mean_data20 0.165949 -0.470137 a one 1.179285 -1.1865921 1.780182 -1.002254 a two 1.179285 -1.1865924 1.591724 -2.087385 a one 1.179285 -1.1865922 -0.371285 0.543113 b one -1.023229 0.6174893 -1.675172 0.691865 b two -1.023229 0.617489 虽然这样也行，但是不太灵活。我们可以将该过程看做利用 np.mean 函数对两个数据列进行行转换。再以上一个实验用过的那个 people DataFrame 为例，这次我们在 GroupBy 上使用 transform 方法：1234567891011121314151617181920212223242526272829In [77]: people = DataFrame(np.random.randn(5,5), columns = ['a','b','c','d','e'], index=['Joe','Steve','Wes','Jim','Travis'])In [78]: peopleOut[78]: a b c d eJoe -0.454871 0.097879 0.122988 -0.160000 0.302538Steve -0.717674 -0.188876 -2.098488 0.345893 -1.308665Wes 0.619586 -1.534228 -1.100433 0.133167 -0.912879Jim -0.406623 -1.382807 -1.361487 -1.027443 -0.118505Travis -1.227921 2.259946 -0.152834 1.444180 1.077661In [79]: key = ['one','two','one','two','one']In [80]: people.groupby(key).mean()Out[80]: a b c d eone -0.354402 0.274533 -0.376759 0.472449 0.155773two -0.562148 -0.785842 -1.729988 -0.340775 -0.713585In [81]: people.groupby(key).transform(np.mean)Out[81]: a b c d eJoe -0.354402 0.274533 -0.376759 0.472449 0.155773Steve -0.562148 -0.785842 -1.729988 -0.340775 -0.713585Wes -0.354402 0.274533 -0.376759 0.472449 0.155773Jim -0.562148 -0.785842 -1.729988 -0.340775 -0.713585Travis -0.354402 0.274533 -0.376759 0.472449 0.155773 不难看出，transform 会将一个函数应用到各个分组，然后将结果放置到适当的位置上。如果各分组产生的是一个标量值，则该值就会被广播出去。现在，假设我们希望从各组中减去平均值。为此，我们先创建一个距平化函数（demeaning function），然后将其传给 transform123456789101112131415161718192021In [83]: def demean(arr): ....: return arr - arr.mean() ....: In [84]: demeaned = people.groupby(key).transform(demean)In [85]: demeanedOut[85]: a b c d eJoe -0.100469 -0.176653 0.499747 -0.632449 0.146765Steve -0.155526 0.596966 -0.368500 0.686668 -0.595080Wes 0.973988 -1.808760 -0.723673 -0.339282 -1.068652Jim 0.155526 -0.596966 0.368500 -0.686668 0.595080Travis -0.873519 1.985414 0.223926 0.971731 0.921887In [86]: # 我们可以检查一下 demeaned 现在的分组平均值是否为 In [87]: demeaned.groupby(key).mean()Out[87]: a b c d eone 0 0 0 0 0two 0 0 0 0 0 三、apply：一般性的“拆分-应用-合并” 跟 aggregate 一样，transform 也是一个有着严格条件的特殊函数：传入的函数只能产生两种结果，要么产生一个可以广播的标量值（如 np.mean），要么产生一个相同大小的结果数组。最一般化的 GroupBy 方法是 apply，本实验剩余部分将重点讲解它。apply 会将待处理的对象拆分成多个片段，然后对各片段调用传入的函数，最后尝试将各片段组合到一起。 回到上一节实验课的小费数据集，假设我们想要根据分组选出最高的5个 tip_pct 值。首先，编写一个选取指定列具有最大值的行的函数。123456789101112131415161718192021222324252627282930In [105]: def top(df,n=5,column='tip_pct'): .....: return df.sort_index(by=column)[-n:] .....: In [106]: top(tips,n=6)Out[106]: total_bill tip sex smoker day time size tip_pct109 14.31 4.00 Female Yes Sat Dinner 2 0.279525183 23.17 6.50 Male Yes Sun Dinner 4 0.280535232 11.61 3.39 Male No Sat Dinner 2 0.29199067 3.07 1.00 Female Yes Sat Dinner 1 0.325733178 9.60 4.00 Female Yes Sun Dinner 2 0.416667172 7.25 5.15 Male Yes Sun Dinner 2 0.710345In [107]: # 现在，如果对 smoker 分组并用该函数调用 applyIn [108]: tips.groupby('smoker').apply(top)Out[108]: total_bill tip sex smoker day time size tip_pctsmoker No 88 24.71 5.85 Male No Thur Lunch 2 0.236746 185 20.69 5.00 Male No Sun Dinner 5 0.241663 51 10.29 2.60 Female No Sun Dinner 2 0.252672 149 7.51 2.00 Male No Thur Lunch 2 0.266312 232 11.61 3.39 Male No Sat Dinner 2 0.291990Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525 183 23.17 6.50 Male Yes Sun Dinner 4 0.280535 67 3.07 1.00 Female Yes Sat Dinner 1 0.325733 178 9.60 4.00 Female Yes Sun Dinner 2 0.416667 172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 这里发生了什么？top 函数在 DataFrame 的各个片段上调用，然后结果由 pandas.concat 组装到一起，并以分组名称进行了标记。于是，最终结果就有了一个层次化索引，其内层索引值来自原 DataFrame。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 如果传给 `apply` 的函数能够接受其他参数或关键字，则可以将这些内容放在函数名后面一并传入：In [109]: tips.groupby(['smoker','day']).apply( .....: top,n=1,column='total_bill')Out[109]: total_bill tip sex smoker day time size \\smoker day No Fri 94 22.75 3.25 Female No Fri Dinner 2 Sat 212 48.33 9.00 Male No Sat Dinner 4 Sun 156 48.17 5.00 Male No Sun Dinner 6 Thur 142 41.19 5.00 Male No Thur Lunch 5 Yes Fri 95 40.17 4.73 Male Yes Fri Dinner 4 Sat 170 50.81 10.00 Male Yes Sat Dinner 3 Sun 182 45.35 3.50 Male Yes Sun Dinner 3 Thur 197 43.11 5.00 Female Yes Thur Lunch 4 tip_pct smoker day No Fri 94 0.142857 Sat 212 0.186220 Sun 156 0.103799 Thur 142 0.121389 Yes Fri 95 0.117750 Sat 170 0.196812 Sun 182 0.077178 Thur 197 0.115982 In [110]: #GroupBy 对象上调用 describeIn [111]: result = tips.groupby( .....: 'smoker')['tip_pct'].describe()In [112]: resultOut[112]: smoker No count 151.000000 mean 0.159328 std 0.039910 min 0.056797 25% 0.136906 50% 0.155625 75% 0.185014 max 0.291990Yes count 93.000000 mean 0.163196 std 0.085119 min 0.035638 25% 0.106771 50% 0.153846 75% 0.195059 max 0.710345dtype: float64In [113]: result.unstack('smoker')Out[113]: smoker No Yescount 151.000000 93.000000mean 0.159328 0.163196std 0.039910 0.085119min 0.056797 0.03563825% 0.136906 0.10677150% 0.155625 0.15384675% 0.185014 0.195059max 0.291990 0.710345 在 GroupBy 中，当你调用诸如 describe 之类的方法时，实际上只是应用了下面两条代码的快捷方式而已：12f = lambda x: x.describe()grouped.apply(f) 从上面的例子中可以看出，分组键会跟原始对象的索引共同构成结果对象中的层次化索引。将 group_keys = False 传入 groupby 即可禁止该效果：12345678910111213In [116]: tips.groupby('smoker',group_keys=False).apply(top)Out[116]: total_bill tip sex smoker day time size tip_pct88 24.71 5.85 Male No Thur Lunch 2 0.236746185 20.69 5.00 Male No Sun Dinner 5 0.24166351 10.29 2.60 Female No Sun Dinner 2 0.252672149 7.51 2.00 Male No Thur Lunch 2 0.266312232 11.61 3.39 Male No Sat Dinner 2 0.291990109 14.31 4.00 Female Yes Sat Dinner 2 0.279525183 23.17 6.50 Male Yes Sun Dinner 4 0.28053567 3.07 1.00 Female Yes Sat Dinner 1 0.325733178 9.60 4.00 Female Yes Sun Dinner 2 0.416667172 7.25 5.15 Male Yes Sun Dinner 2 0.710345 四、分位数和桶分析 我们原来讲过，pandas 有一些能根据指定面元或样本分位数将数据拆分成多块的工具（比如 cut 和 qcut）。将这些函数跟 groupby 结合起来，就能非常轻松地实现对数据集的桶（bucket）或分位数（quantile）分析了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566In [120]: frame=DataFrame(&#123;'data1':np.random.randn(1000), .....: 'data2':np.random.randn(1000)&#125;)In [121]: factor = pd.cut(frame.data1,4)In [122]: factor[:10]Out[122]: 0 (-3.335, -1.695]1 (-3.335, -1.695]2 (-1.695, -0.0626]3 (-1.695, -0.0626]4 (-0.0626, 1.57]5 (-1.695, -0.0626]6 (-1.695, -0.0626]7 (-1.695, -0.0626]8 (-1.695, -0.0626]9 (-1.695, -0.0626]Name: data1, dtype: categoryCategories (4, object): [(-3.335, -1.695] &lt; (-1.695, -0.0626] &lt; (-0.0626, 1.57] &lt; (1.57, 3.203]]In [123]: # 由 cut 返回的 Factor 对象可直接用于 groupbyIn [124]: #我们可以像下面这样对 data2 做一些统计计算In [125]: def get_stats(group): .....: return &#123;'min':group.min(), .....: 'max':group.max(), .....: 'count':group.count(), .....: 'mean':group.mean()&#125; .....: In [126]: grouped = frame.data2.groupby(factor)In [127]: grouped.apply(get_stats).unstack()Out[127]: count max mean mindata1 (-3.335, -1.695] 54 1.816570 0.191717 -1.593857(-1.695, -0.0626] 440 3.050292 -0.012199 -4.208954(-0.0626, 1.57] 467 2.465807 0.048193 -3.790881(1.57, 3.203] 39 2.003740 0.007436 -1.160659In [128]: # 这些都是长度相等的桶In [129]: #要根据样本分位数得到大小相等的桶，使用 qcutIn [130]: #传入 labels=False 即可只获取分位数的编号In [131]: grouping = pd.qcut(frame.data1, 10, labels=False) In [132]: grouped = frame.data2.groupby(grouping)In [133]: grouped.apply(get_stats).unstack()Out[133]: count max mean min0 100 1.993976 0.015366 -1.9214661 100 2.330603 -0.027885 -2.3850292 100 1.691100 -0.040104 -1.8463313 100 3.050292 0.103089 -4.2089544 100 2.346644 -0.001461 -2.4456435 100 2.407185 -0.118178 -3.7908816 100 2.465807 0.131676 -2.5039407 100 2.464500 0.040245 -2.8106168 100 2.261199 0.164115 -3.2021159 100 2.365629 0.010951 -2.188505","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-X-数据聚合","date":"2016-05-24T13:24:30.406Z","updated":"2016-06-19T07:13:21.498Z","comments":true,"path":"2016/05/24/PANDAS常用手册-X-数据聚合/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-X-数据聚合/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验介绍 对于聚合，我指的是任何能够从数组产生标量值的数据转换过程。上一节实验课的例子中，我们已经用过一些，比如 mean、count、min 以及 sum 等。你可能想知道在 GroupBy 对象上调用 mean()时究竟发生了什么。 许多常见的聚合运算都有就地计算数据集统计信息的优化实现。 函数名 说明 count 分组中非 NA 值的数量 sum 非 NA 值的和 mean 非 NA 值的平均值 median 非 NA 值的算术中位数 std、var 无偏（分母为你n-1）标准差和方差 min、max 非 NA 值的最小值和最大值 prop 非 NA 值的积 first、last 第一个和最后一个非 NA 值 然而，并不是只能使用这些方法。我们可以使用自己发明的聚合运算，还可以调用分组对象已经定义好的任何方法。例如，quantile 可以计算 Series 或 DataFrame 列的样本分位数：12345678910111213141516171819202122In [28]: df = DataFrame(&#123;'key1':['a','a','b','b','a'], 'key2':['one','two','one','two','one'], 'data1':np.random.randn(5), 'data2':np.random.randn(5)&#125;)In [29]: dfOut[29]: data1 data2 key1 key20 1.569326 0.129858 a one1 -2.535518 -1.749728 a two2 -0.225235 0.799330 b one3 -0.342218 1.061132 b two4 -0.605917 -1.925334 a oneIn [30]: grouped = df.groupby('key1')In [31]: grouped['data1'].quantile(0.9)Out[31]: key1a 1.134277b -0.236933Name: data1, dtype: float64 虽然 quantile 并没有明确地实现于 GroupBy，但它是一个 Series 方法，所以这里是能用的。实际上，GroupBy 会高效地对 Series 进行切片，然后对各片调用 piece.quantile(0.9)，最后将这些结果组装成最终结果。 如果要使用你自己的聚合函数，只需将其传入 aggregate 或 agg 方法即可：12345678910In [32]: def peak_to_peak(arr): ....: return arr.max() - arr.min() ....: In [33]: grouped.agg(peak_to_peak)Out[33]: data1 data2key1 a 4.104844 2.055192b 0.116983 0.261802 注意，有些方法（如 describe）也是可以用在这里的，即使严格来讲，它们并非聚合运算：1234567891011121314151617181920In [34]: grouped.describe()Out[34]: data1 data2key1 a count 3.000000 3.000000 mean -0.524037 -1.181735 std 2.053647 1.139261 min -2.535518 -1.925334 25% -1.570718 -1.837531 50% -0.605917 -1.749728 75% 0.481704 -0.809935 max 1.569326 0.129858b count 2.000000 2.000000 mean -0.283727 0.930231 std 0.082720 0.185122 min -0.342218 0.799330 25% -0.312973 0.864781 50% -0.283727 0.930231 75% -0.254481 0.995681 max -0.225235 1.061132 二、面向列的多函数应用 为了说明一些更高级的聚合功能，我将使用一个关于餐馆小费的数据集，同样的，同学们将下面的数据贴在文本文件里，等会儿会用到：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687total_bill,tip,sex,smoker,day,time,size16.99,1.01,Female,No,Sun,Dinner,210.34,1.66,Male,No,Sun,Dinner,321.01,3.5,Male,No,Sun,Dinner,323.68,3.31,Male,No,Sun,Dinner,224.59,3.61,Female,No,Sun,Dinner,425.29,4.71,Male,No,Sun,Dinner,48.77,2.0,Male,No,Sun,Dinner,226.88,3.12,Male,No,Sun,Dinner,415.04,1.96,Male,No,Sun,Dinner,214.78,3.23,Male,No,Sun,Dinner,210.27,1.71,Male,No,Sun,Dinner,235.26,5.0,Female,No,Sun,Dinner,415.42,1.57,Male,No,Sun,Dinner,218.43,3.0,Male,No,Sun,Dinner,414.83,3.02,Female,No,Sun,Dinner,221.58,3.92,Male,No,Sun,Dinner,210.33,1.67,Female,No,Sun,Dinner,316.29,3.71,Male,No,Sun,Dinner,316.97,3.5,Female,No,Sun,Dinner,320.65,3.35,Male,No,Sat,Dinner,317.92,4.08,Male,No,Sat,Dinner,220.29,2.75,Female,No,Sat,Dinner,215.77,2.23,Female,No,Sat,Dinner,239.42,7.58,Male,No,Sat,Dinner,419.82,3.18,Male,No,Sat,Dinner,217.81,2.34,Male,No,Sat,Dinner,413.37,2.0,Male,No,Sat,Dinner,212.69,2.0,Male,No,Sat,Dinner,221.7,4.3,Male,No,Sat,Dinner,219.65,3.0,Female,No,Sat,Dinner,29.55,1.45,Male,No,Sat,Dinner,218.35,2.5,Male,No,Sat,Dinner,415.06,3.0,Female,No,Sat,Dinner,220.69,2.45,Female,No,Sat,Dinner,417.78,3.27,Male,No,Sat,Dinner,224.06,3.6,Male,No,Sat,Dinner,316.31,2.0,Male,No,Sat,Dinner,316.93,3.07,Female,No,Sat,Dinner,318.69,2.31,Male,No,Sat,Dinner,331.27,5.0,Male,No,Sat,Dinner,316.04,2.24,Male,No,Sat,Dinner,317.46,2.54,Male,No,Sun,Dinner,213.94,3.06,Male,No,Sun,Dinner,29.68,1.32,Male,No,Sun,Dinner,230.4,5.6,Male,No,Sun,Dinner,418.29,3.0,Male,No,Sun,Dinner,222.23,5.0,Male,No,Sun,Dinner,232.4,6.0,Male,No,Sun,Dinner,428.55,2.05,Male,No,Sun,Dinner,318.04,3.0,Male,No,Sun,Dinner,212.54,2.5,Male,No,Sun,Dinner,210.29,2.6,Female,No,Sun,Dinner,234.81,5.2,Female,No,Sun,Dinner,49.94,1.56,Male,No,Sun,Dinner,225.56,4.34,Male,No,Sun,Dinner,419.49,3.51,Male,No,Sun,Dinner,238.01,3.0,Male,Yes,Sat,Dinner,426.41,1.5,Female,No,Sat,Dinner,211.24,1.76,Male,Yes,Sat,Dinner,248.27,6.73,Male,No,Sat,Dinner,420.29,3.21,Male,Yes,Sat,Dinner,213.81,2.0,Male,Yes,Sat,Dinner,211.02,1.98,Male,Yes,Sat,Dinner,218.29,3.76,Male,Yes,Sat,Dinner,417.59,2.64,Male,No,Sat,Dinner,320.08,3.15,Male,No,Sat,Dinner,316.45,2.47,Female,No,Sat,Dinner,23.07,1.0,Female,Yes,Sat,Dinner,120.23,2.01,Male,No,Sat,Dinner,215.01,2.09,Male,Yes,Sat,Dinner,212.02,1.97,Male,No,Sat,Dinner,217.07,3.0,Female,No,Sat,Dinner,326.86,3.14,Female,Yes,Sat,Dinner,225.28,5.0,Female,Yes,Sat,Dinner,214.73,2.2,Female,No,Sat,Dinner,210.51,1.25,Male,No,Sat,Dinner,217.92,3.08,Male,Yes,Sat,Dinner,227.2,4.0,Male,No,Thur,Lunch,422.76,3.0,Male,No,Thur,Lunch,217.29,2.71,Male,No,Thur,Lunch,219.44,3.0,Male,Yes,Thur,Lunch,216.66,3.4,Male,No,Thur,Lunch,210.07,1.83,Female,No,Thur,Lunch,132.68,5.0,Male,Yes,Thur,Lunch,215.98,2.03,Male,No,Thur,Lunch,234.83,5.17,Female,No,Thur,Lunch,4 我们已经看到，对 Series 或 DataFrame 列的聚合运算其实就是使用 aggregate（使用自定义函数）或调用诸如 mean、std 之类的方法。然而，你可能希望对不同的列使用不同的聚合函数，或一次应用多个函数。其实这事也好办，我将通过一些示例来进行讲解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657In [35]: tips = pd.read_csv('tips.csv')In [36]: # 添加“小费占总额百分比”的列In [37]: tips['tip_pct'] = tips['tip']/tips['total_bill']In [38]: tips[:6]Out[38]: total_bill tip sex smoker day time size tip_pct0 16.99 1.01 Female No Sun Dinner 2 0.0594471 10.34 1.66 Male No Sun Dinner 3 0.1605422 21.01 3.50 Male No Sun Dinner 3 0.1665873 23.68 3.31 Male No Sun Dinner 2 0.1397804 24.59 3.61 Female No Sun Dinner 4 0.1468085 25.29 4.71 Male No Sun Dinner 4 0.186240In [39]: #首先，我们根据 sex 和 smoker 对 tips 进行分组In [40]: grouped = tips.groupby(['sex','smoker'])In [41]: # 注意，统计函数名可以以字符串的形式传入In [42]: grouped_pct = grouped['tip_pct']In [43]: grouped_pct.agg('mean')Out[43]: sex smokerFemale No 0.156921 Yes 0.182150Male No 0.160669 Yes 0.152771Name: tip_pct, dtype: float64In [44]: #如果传入一组函数或函数名In [45]: #得到的 DataFrame 列就会以相应的函数命名In [46]: grouped_pct.agg(['mean','std',peak_to_peak])Out[46]: mean std peak_to_peaksex smoker Female No 0.156921 0.036421 0.195876 Yes 0.182150 0.071595 0.360233Male No 0.160669 0.041849 0.220186 Yes 0.152771 0.090588 0.674707In [47]: #当然我们并非一定要接受 GroupBy 自动给出的列名In [48]: #我们可以传入（name，function） 元组就可以In [49]: grouped_pct.agg([('foo','mean'),('bar',np.std)])Out[49]: foo barsex smoker Female No 0.156921 0.036421 Yes 0.182150 0.071595Male No 0.160669 0.041849 Yes 0.152771 0.090588 对于 DataFrame，我们还可以定义一组应用于全部列的函数，或不同的列应用不同的函数。假设我们想要对 tip_pct 和 total_bill 列计算三个统计信息： 1234567891011121314In [54]: functions = ['count','mean','max']In [55]: result = grouped['tip_pct', 'total_bill'].agg(functions)In [56]: resultOut[56]: tip_pct total_bill count mean max count mean maxsex smoker Female No 54 0.156921 0.252672 54 18.105185 35.83 Yes 33 0.182150 0.416667 33 17.977879 44.30Male No 97 0.160669 0.291990 97 19.791237 48.33 Yes 60 0.152771 0.710345 60 22.284500 50.81 如你所见，结果 DataFrame 拥有层次化的列，这相当于分别对各列进行聚合，然后用 concat 将结果组装到一起1234567891011121314151617181920212223In [57]: result['tip_pct']Out[57]: count mean maxsex smoker Female No 54 0.156921 0.252672 Yes 33 0.182150 0.416667Male No 97 0.160669 0.291990 Yes 60 0.152771 0.710345In [58]: # 这里可以传入带有自定义名称的元组列表In [59]: ftuples=[('Durchschnitt','mean'), ....: ('Abweichung',np.var)]In [60]: grouped['tip_pct','total_bill'].agg(ftuples)Out[60]: tip_pct total_bill Durchschnitt Abweichung Durchschnitt Abweichungsex smoker Female No 0.156921 0.001327 18.105185 53.092422 Yes 0.182150 0.005126 17.977879 84.451517Male No 0.160669 0.001751 19.791237 76.152961 Yes 0.152771 0.008206 22.284500 98.244673 现在我们要对不同的列应用不同的函数。具体的办法是向 agg 传入一个从列名映射到函数的字典：123456789101112131415161718192021In [61]: grouped.agg(&#123;'tip':np.max,'size':'sum'&#125;)Out[61]: tip sizesex smoker Female No 5.2 140 Yes 6.5 74Male No 9.0 263 Yes 10.0 150In [62]: grouped.agg(&#123;'tip_pct':['min','max', ....: 'mean','std'],'size':'sum'&#125;)Out[62]: tip_pct size min max mean std sumsex smoker Female No 0.056797 0.252672 0.156921 0.036421 140 Yes 0.056433 0.416667 0.182150 0.071595 74Male No 0.071804 0.291990 0.160669 0.041849 263 Yes 0.035638 0.710345 0.152771 0.090588 150 只有将多个函数应用到至少一列时，DataFrame 才会拥有层次化的列。 三、以“无索引”的形式返回聚合数据 到目前为止，所有示例中的聚合数据都有由唯一的分组键组成的索引（可能还是层次化的）。由于并不总是需要如此，所以你可以向 groupby 传入 as_index=False 以禁用该功能12345678In [64]: tips.groupby(['sex','smoker'], ....: as_index=False).mean()Out[64]: sex smoker total_bill tip size tip_pct0 Female No 18.105185 2.773519 2.592593 0.1569211 Female Yes 17.977879 2.931515 2.242424 0.1821502 Male No 19.791237 3.113402 2.711340 0.1606693 Male Yes 22.284500 3.051167 2.500000 0.152771","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PPANDAS常用手册-IX-groupby-技术","date":"2016-05-24T13:24:23.174Z","updated":"2016-06-19T07:13:31.320Z","comments":true,"path":"2016/05/24/PPANDAS常用手册-IX-groupby-技术/","link":"","permalink":"http://yoursite.com/2016/05/24/PPANDAS常用手册-IX-groupby-技术/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 对数据集进行分组并对各组应用一个函数（无论是聚合还是转换），这是数据分析工作中的重要环节。在数据集准备好之后，通常的任务就是计算分组统计或生成透视表。pandas 提供了一个灵活高效的 groupby 功能，它使我们能以一种自然的方式对数据集进行切片、切块、摘要等操作。 分组运算的第一阶段，pandas 对象（无论是 Series、DataFrame 还是其他的）中的数据会根据你所提供的一个或多个键被拆分（split）为多组。拆分操作是在对象的特定轴上执行的。例如，DataFrame 可以在其行（axis=0）或列（axis=1）上进行分组。然后，将一个函数应用到各个分组并产生一个新值。最后，所有这些函数的执行结果会被合并到最终的结果对象中。结果对象的形式一般取决于数据上所执行的操作。 分组聚合演示 分组键可以有多种形式，且类型不必相同： 列表或数组，其长度与待分组的轴一样 表示 DataFrame 某个列名的值 字典或 Series，给出待分组轴上的值与分组名之间的对应关系 函数、用于处理轴索引中的各个标签 注意，后三种都只是快捷方式而已，其最终目的仍然是产生一组用于拆分对象的值。12345678910111213In [5]: df = DataFrame(&#123;'key1':['a','a','b','b','a'], 'key2':['one','two','one','two','one'], 'data1':np.random.randn(5), 'data2':np.random.randn(5)&#125;)In [6]: df Out[6]: data1 data2 key1 key20 -1.884515 0.735152 a one1 0.320270 1.364803 a two2 1.190752 -0.877677 b one3 -2.714275 -0.641703 b two4 0.586653 -0.451896 a one 在上面的例子中，假设我们想要按 key1 进行分组，并计算 data1 列的平均值。实现该功能的方式有很多，而我们这里要用的是：访问 data1，并根据 key1调用 groupby：1234In [7]: grouped = df['data1'].groupby(df['key1'])In [8]: groupedOut[8]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x112d53290&gt; 变量 grouped 是一个 GroupBy 对象。它实际上还没有进行任何计算，只是含有一些有关分组键 df[&#39;key1&#39;]的中间数据而已。换句话说，该对象已经有了接下来对各分组执行运算所需的一切信息。例如，我们可以调用 GroupBy 的 mean 方法来计算分组平均值：123456In [10]: grouped.mean()Out[10]: key1a -0.325864b -0.761762Name: data1, dtype: float64 数据（Series）根据分组键进行了聚合，产生了一个新的 Series，其索引为 key1列中的唯一值。如果我们传入多个数组，就会得到不同的结果：12345678910In [11]: means = df['data1'].groupby([df['key1'],df['key2']]).mean()In [12]: meansOut[12]: key1 key2a one -0.648931 two 0.320270b one 1.190752 two -2.714275Name: data1, dtype: float64 这里，我们通过两个键对数据进行了分组，得到的 Series 具有一个层次化索引（由唯一的键对组成）：123456In [13]: means.unstack()Out[13]: key2 one twokey1 a -0.648931 0.320270b 1.190752 -2.714275 在上面这些示例中，分组键均为 Series。实际上，分组键可以是任何长度适当的数组：1234567891011121314151617181920212223242526272829In [14]: states = np.array(['Ohio','California','California','Ohio','Ohio'])In [15]: years = np.array([2005, 2005, 2006, 2005, 2006])In [16]: df['data1'].groupby([states,years]).mean()Out[16]: California 2005 0.320270 2006 1.190752Ohio 2005 -2.299395 2006 0.586653Name: data1, dtype: float64In [17]: # 此外，还可以将列名用作分组键In [18]: df.groupby('key1').mean()Out[18]: data1 data2key1 a -0.325864 0.549353b -0.761762 -0.759690In [19]: df.groupby(['key1','key2']).mean()Out[19]: data1 data2key1 key2 a one -0.648931 0.141628 two 0.320270 1.364803b one 1.190752 -0.877677 two -2.714275 -0.641703 在执行 df.groupby(&#39;key1&#39;).mean()时，结果中没有 key2 列。这是因为 df[&#39;key2&#39;]不是数值数据，所以从结果中排除了。默认情况下，所有数值列都会被聚合，虽然有时可能会被过滤为一个子集（稍后会讲到）。 GroupBy 的 size 方法可以返回一个含有分组大小的 Series：12345678In [20]: df.groupby(['key1','key2']).size()Out[20]: key1 key2a one 2 two 1b one 1 two 1dtype: int64 二、对分组进行迭代 GroupBy 对象支持迭代，可以产生一组二元元组（由分组名和数据块组成）123456789101112131415161718192021222324252627282930313233In [21]: for name,group in df.groupby('key1'): ....: print name ....: print group ....: a data1 data2 key1 key20 -1.884515 0.735152 a one1 0.320270 1.364803 a two4 0.586653 -0.451896 a oneb data1 data2 key1 key22 1.190752 -0.877677 b one3 -2.714275 -0.641703 b twoIn [22]: # 对于多重键的情况，元组的第一个元素将会是由键值组成的元组In [23]: for (k1, k2),group in df.groupby(['key1','key2']): ....: print k1, k2 ....: print group ....: a one data1 data2 key1 key20 -1.884515 0.735152 a one4 0.586653 -0.451896 a onea two data1 data2 key1 key21 0.32027 1.364803 a twob one data1 data2 key1 key22 1.190752 -0.877677 b oneb two data1 data2 key1 key23 -2.714275 -0.641703 b two 当然，我们可以对这些数据片段做任何操作。有一个你可能会觉得有用的运算：将这些数据片段做成一个字典。1234567In [25]: pieces = dict(list(df.groupby('key1')))In [26]: pieces['b']Out[26]: data1 data2 key1 key22 1.190752 -0.877677 b one3 -2.714275 -0.641703 b two groupby 默认是在axis=0上进行分组的，通过设置也可以在其他任何轴上进行分组。拿上面例子中的 df 来说，我们可以根据 dtype 对列进行分组：1234567891011121314151617181920212223In [27]: df.dtypesOut[27]: data1 float64data2 float64key1 objectkey2 objectdtype: objectIn [28]: grouped = df.groupby(df.dtypes, axis =1)In [29]: dict(list(grouped))Out[29]: &#123;dtype('float64'): data1 data2 0 -1.884515 0.735152 1 0.320270 1.364803 2 1.190752 -0.877677 3 -2.714275 -0.641703 4 0.586653 -0.451896, dtype('O'): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one&#125; 三、选取一个或一组列 对于由 DataFrame 产生的 GroupBy 对象，如果用一个（单个字符串）或一组（字符串数组）列名对其进行索引，就能实现选取部分列进行聚合的目的，也就是说： 12df.groupby('key1')['data1']df.groupby('key1')[['data2']] 是以下代码的语法糖：12df['data1'].groupby(df['key1'])df[['data2']].groupby(df['key1']) 尤其对于大量数据集，很有可能只需要对部分列进行聚合。例如，在前面那个数据集中，如果只需计算 data2 列的平均值并以 DataFrame 形式得到结果，我们可以编写：12345678In [21]: df.groupby(['key1','key2'])[['data2']].mean()Out[21]: data2key1 key2 a one -0.035973 two -1.650808b one 0.610671 two 0.407438 这种索引操作所返回的对象是一个已分组的 DataFrame（如果传入的是列表或数组）或已分组的 Series（如果传入的是标量形式的单个列名）：12345678910In [22]: s_groupby = df.groupby(['key1','key2'])['data2']In [23]: s_groupby.mean()Out[23]: key1 key2a one -0.035973 two -1.650808b one 0.610671 two 0.407438Name: data2, dtype: float64 注意观察行21和22之间的区别，前面返回的是一个 DataFrame，后面返回的是一个 Series 四、通过字典或 Series 进行分组 除数组以外，分组信息还可以其他形式存在。来看一个示例：123456789101112131415161718192021222324252627282930313233In [4]: people = DataFrame(np.random.randn(5,5), columns = ['a','b','c','d','e'], index = ['Joe','Steve','Wes','Jim','Travis'])In [5]: people.ix[2:3,['b','c']] = np.nanIn [6]: peopleOut[6]: a b c d eJoe -0.131332 1.361534 0.885761 1.250524 0.723013Steve -0.968665 -0.024584 1.213288 0.564578 -0.666432Wes 0.108587 NaN NaN 0.678532 -1.182141Jim 0.643063 0.065343 0.008009 -1.651852 -1.156628Travis 0.113156 0.076969 0.353941 -0.096054 -0.351033In [7]: #假设已知列的分组关系，并希望根据分组计算列的总计In [8]: mapping = &#123;'a':'red','b':'red','c':'blue', 'd':'blue','e':'red','f':'orange'&#125;In [9]: #现在，只需将这个字典传给 groupby 即可In [10]: by_column = people.groupby(mapping,axis=1)In [11]: by_column.sum()Out[11]: blue redJoe 2.136285 1.953214Steve 1.777866 -1.659681Wes 0.678532 -1.073554Jim -1.643843 -0.448222Travis 0.257887 -0.160908 Series也有同样的功能，它可以被看做一个固定大小的映射。对于上面那个例子，如果用 Series 作为分组键，则 pandas 会检查 Series 以确保其索引跟分组轴是对齐的：1234567891011121314151617181920In [13]: map_series = Series(mapping)In [14]: map_seriesOut[14]: a redb redc blued bluee redf orangedtype: objectIn [15]: people.groupby(map_series,axis=1).count()Out[15]: blue redJoe 2 3Steve 2 3Wes 1 2Jim 2 3Travis 2 3 五、通过函数进行分组 相较于字典或 Series，Python 函数在定义分组映射关系时可以更有创意且更为抽象。任何被当做分组键的函数都会在各个索引值上被调用一次，其返回值就会被用作分组名称。具体点说，以上一小节的示例 DataFrame 为例，其索引值为人的名字。假设你希望根据人名的长度进行分组，虽然可以求取一个字符串长度数组，但其实仅仅传入 len 函数就可以了：123456In [16]: people.groupby(len).sum()Out[16]: a b c d e3 0.620317 1.426876 0.893770 0.277204 -1.6157565 -0.968665 -0.024584 1.213288 0.564578 -0.6664326 0.113156 0.076969 0.353941 -0.096054 -0.351033 将函数跟数组、列表、字典、Series 混合使用也不是问题，因为任何东西最终都会被转换为数组： 123456789In [17]: key_list = ['one','one','one','two','two']In [18]: people.groupby([len,key_list]).min()Out[18]: a b c d e3 one -0.131332 1.361534 0.885761 0.678532 -1.182141 two 0.643063 0.065343 0.008009 -1.651852 -1.1566285 one -0.968665 -0.024584 1.213288 0.564578 -0.6664326 two 0.113156 0.076969 0.353941 -0.096054 -0.351033 层次化索引数据集最方便的地方就在于它能够根据索引级别进行聚合。要实现该目的，通过 level 关键字传入级别编号或名称即可：1234567891011121314151617181920212223In [22]: columns = pd.MultiIndex.from_arrays([['US','US', 'US','CN','CN'],[1,3,5,1,3]],names=['cty','tenor'])In [23]: hier_df = DataFrame(np.random.randn(4,5), columns = columns)In [24]: hier_dfOut[24]: cty US CN tenor 1 3 5 1 30 0.823673 0.783013 0.777291 -0.065750 0.5075801 -2.173389 -0.339692 -1.793867 -1.075630 -0.2359642 1.973584 0.526835 -1.274129 -0.355864 -0.9174853 0.984408 0.246716 0.383760 -2.521464 0.078212In [25]: hier_df.groupby(level='cty',axis=1).count()Out[25]: cty CN US0 2 31 2 32 2 33 2 3","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-VIII-绘图和可视化","date":"2016-05-24T13:24:17.058Z","updated":"2016-06-19T07:13:12.883Z","comments":true,"path":"2016/05/24/PANDAS常用手册-VIII-绘图和可视化/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-VIII-绘图和可视化/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验介绍 绘图是数据分析工作中最重要的任务之一，是探索过程的一部分，例如，帮我们找出异常值，必要的数据转换、得出有关模型的 idea 等。 二、Matplotlib API matplotlib 的图像都位于 Figure 对象中。你可以使用 plt.figure 创建一个新的 Figure：12345In [4]: import matplotlib.pyplot as pltIn [5]: fig = plt.figure()In [6]: fig.show() 这时会弹出一个空窗口。plt.figure 有一些选项，特别是 figsize，它用于确保当图片保存到磁盘时具有一定的大小和纵横比。matplotlib 中的 Figure 还支持一种 MATLAB 式的编号架构（例如 plt.figure（2））。通过 plt.gcf()即可得到当前 Figure 的引用。 不能通过空 Figure 绘图。必须用 add_subplot 创建一个或多个 subplot 才行：12345In [10]: ax1 = fig.add_subplot(2,2,1)In [11]: ax2 = fig.add_subplot(2,2,2)In [12]: ax3 = fig.add_subplot(2,2,3) 如果这时发出一条绘图命令（如plt.plot([1.5, 3.5, -2, 1.6])），matplotlib就会在最后一个用过的 subplot（如果没有则创建一个）上绘制。12In [13]: plt.plot(randn(50).cumsum(),'k--')Out[13]: [&lt;matplotlib.lines.Line2D at 0x10830c4d0&gt;] 一个图形的绘制 “k–”是一个线性选项，用于告诉 matplotlib 绘制黑色虚线图。上面那些由 fig.add_subplot 所返回的对象是 AxesSubplot 对象，直接调用它们的实例方法就可以在其他空着的格子里面画图了。1234In [14]: _ = ax1.hist(randn(100),bins = 20, color = 'k',alpha = 0.3)In [15]: ax2.scatter(np.arange(30),np.arange(30)+3*randn(30))Out[15]: &lt;matplotlib.collections.PathCollection at 0x10c29fb90&gt; 三个图形绘制 我们可以在 matplotlib 的文档中找到各种图表类型。用于根据特定布局创建 Figure 和 subplot 是一件非常常见的任务，于是便出现了一个更为方便的方法（plt.subplots），它可以创建一个新的 Figure，并返回一个含有已创建的 subplotduixiang 的 NumPy 数组12345678910In [17]: fig, axes = plt.subplots(2,3)In [18]: axesOut[18]: array([[&lt;matplotlib.axes.AxesSubplot object at 0x11087f610&gt;, &lt;matplotlib.axes.AxesSubplot object at 0x11090db50&gt;, &lt;matplotlib.axes.AxesSubplot object at 0x1109b89d0&gt;], [&lt;matplotlib.axes.AxesSubplot object at 0x110999d90&gt;, &lt;matplotlib.axes.AxesSubplot object at 0x110989c10&gt;, &lt;matplotlib.axes.AxesSubplot object at 0x110a1b510&gt;]], dtype=object) 这是非常实用的，因为可以轻松地对 axes 数组进行索引，就好像是一个二维数组一样，例如，axes[0, 1]。 我们来看看 pyplot.subplot 的选项 参数 说明 nrows subplot 的行数 ncols subplot 的列数 sharex 所有 subplot 应该使用相同的 X轴刻度（调节 xlim 将会影响所有 subplot） sharey 所有 subplot 应该使用相同的 Y轴刻度（调节 ylim 将会影响所有 subplot） subplot——kw 用于创建各 subplot 的关键字典 **fig_kw 创建 figure 时的其他关键字，如 plt.subplots(2,2,figsize = (8,6)) matplotlib 的 plot 函数接收一组 X和Y坐标，还可以接受一个颜色和线性的字符串缩写。例如，要根据 x 和 y 绘制绿色虚线：12ax.plot(x, y, 'g--')ax.plot(x, y, linestyle = '--', color = 'g') 线型图还可以加上一些标记（mark），以强调实际的数据点。由于 matplotlib 创建的是连续的线型图（点与点之间插值），因此有时可能不太容易看出真实数据点的位置。标记也可以放到格式字符串中，但标记类型和线性必须放在颜色后面。12345678910111213141516In [30]: plt.plot(randn(30).cumsum(),'ko--')Out[30]: [&lt;matplotlib.lines.Line2D at 0x110411690&gt;]In [31]: plot(randn(30).cumsum(),color='r',linestyle='dashed',marker='o')Out[31]: [&lt;matplotlib.lines.Line2D at 0x11041ebd0&gt;]In [32]: data = randn(30).cumsum()In [33]: plt.plot(data,'k--',label='Default')Out[33]: [&lt;matplotlib.lines.Line2D at 0x11042f750&gt;]In [34]: plt.plot(data,'k--',drawstyle = 'steps-post',label='steps-post')Out[34]: [&lt;matplotlib.lines.Line2D at 0x11042fe90&gt;]In [35]: plt.legend(loc='best')Out[35]: &lt;matplotlib.legend.Legend at 0x1104383d0&gt; 四条图形绘制 对于大多数的图表装饰项，其主要实现方式有二：使用过程型的 pyplot 接口（MATLAB 用户非常熟悉）以及更为面向对象的原生 matplotlib API。 pyplot 接口设计目的是交互式使用，含有诸如 xlim、xticks 和 xticklabels 之类的方法。它们分别控制图表的范围、刻度位置、刻度标签等。其使用方式有以下两种： 调用时不带参数，则返回当前的参数值。例如，plt.xlim()返回当前的 X轴绘图范围 调用时带参数，则设置参数值。因此，plt.xlim([0,10])会将 X轴的范围设置为0到10 所有这些方法都是对当前或最近创建的 AxesSubplot 起作用。它们各自对应 subplot 度持续昂的两个方法，以 xlim 为例，就是 ax.get_xlim 和 ax.set_xlim。123456In [40]: fig = plt.figure()In [41]: ax = fig.add_subplot(1,1,1)In [42]: ax.plot(randn(1000).cumsum())Out[42]: [&lt;matplotlib.lines.Line2D at 0x1105d6810&gt;] 随机漫步 要修改 X 轴的刻度，最简单的办法是使用set_xticks 和 set——xticklabels。前者告诉 matplotlib 要将刻度放在数据范围中的哪些位置，默认情况下，这些位置也就是刻度标签。后者是可以将任何其他的值用作标签1234567891011121314In [43]: ticks = ax.set_xticks([0,250,500,750,1000])In [44]: labels = ax.set_xticklabels(['one','two','three','four','five'], ....: rotation=30, fontsize='small')In [45]: #最后再用 set_xlabel 为 X 轴设置一个名称In [46]: ax.set_title('My first matplotlib plot')Out[46]: &lt;matplotlib.text.Text at 0x1104105d0&gt;In [47]: ax.set_xlabel('Stages')Out[47]: &lt;matplotlib.text.Text at 0x1105bed50&gt;In [48]: #Y 轴的修改方式与此类似，只需将 x 替换为 y 即可 图例是另一种用于标识图表元素的重要工具。添加图例的方式有二。最简单的是在添加 subplot 的时候传入 label 参数1234567891011121314151617In [49]: fig = plt.figure()In [50]: ax = fig.add_subplot(1,1,1)In [51]: ax.plot(randn(1000).cumsum(),'k',label='one')Out[51]: [&lt;matplotlib.lines.Line2D at 0x105ce7190&gt;]In [52]: ax.plot(randn(1000).cumsum(),'g--',label='two')Out[52]: [&lt;matplotlib.lines.Line2D at 0x105ceaa50&gt;]In [53]: ax.plot(randn(1000).cumsum(),'r',label='three')Out[53]: [&lt;matplotlib.lines.Line2D at 0x105ceae50&gt;]In [54]: # 调用 ax.legend()或 plt.legend()来自动创建图例In [55]: ax.legend(loc = 'best')Out[55]: &lt;matplotlib.legend.Legend at 0x105ce66d0&gt; 图例 除标准的图表对象之外，我们可能还希望绘制一些自定义的注解（比如文本，箭头或其他图形等）。注解可以通过 text、arrrow 和 annotate 等函数进行添加。123456789101112131415161718In [72]: fig = plt.figure()In [73]: ax = fig.add_subplot(1,1,1)In [74]: rect = plt.Rectangle((0.2,0.75),0.4,0.15,color='k',alpha=0.3)In [75]: circ = plt.Circle((0.7,0.2),0.15,color='b',alpha=0.3)In [76]: pgon = plt.Polygon([[0.15,0.15],[0.35,0.4],[0.2,0.6]],color='g',alpha=0.5)In [77]: ax.add_patch(rect)Out[77]: &lt;matplotlib.patches.Rectangle at 0x10c6105d0&gt;In [78]: ax.add_patch(circ)Out[78]: &lt;matplotlib.patches.Circle at 0x10c610750&gt;In [79]: ax.add_patch(pgon)Out[79]: &lt;matplotlib.patches.Polygon at 0x10c610a90&gt; 添加图形 利用 plt.savefig 可以将当前图表保存到文件。该方法相当于 Figure 对象的实例方法 savefig。例如，要将图表保存为 SVG文件：1In [80]: plt.savefig('figpath.svg') 文件类型是通过文件扩展名推断出来的，因此，如果你使用的是.pdf，就会得到一个 PDF 文件。1In [85]: plt.savefig('figpath.png',dpi=400,bbox_inches='tight') Figure.savefig 的参数选项： 参数 说明 fname 含有文件路径的字符串或 Python 的文件型对象。图像格式由文件扩展名推断得出dpi 图像分辨率，默认为100 facecolor、edgecolor 图像的背景色，默认为’w’（白色） format 显示设置文件格式（“png”、“pdf”、“svg”，“ps”，“eps”……） bbox_inches 图表需要保存的部分。如果设置为“tight”，则将尝试剪出图表周围的空白部分 三、pandas 中的绘图函数 不难看出，matplotlib 实际上是一种比较低级的工具。要组装一张图表，你得用它的各种基础组件才行：数据展示（即图表类型：线性图、柱状图、盒形图、散布图、等值线图等）、图例、标题、刻度标签以及其他注解型信息。这是因为要根据数据制作一张完整图表通常都需要用到多个对象。在 pandas 中，我们有行标签、列标签以及分组信息。这也就是说，要制作一张完整的图表，原本需要一大堆的 matplotlib 代码，现在只需要一两条简洁的语句就可以了。pandas 有许多能够利用 DataFrame 对象数据组织特点来创建标准图表的高级绘图方法（这些函数的数量还在不断增加） 。 1.线性图 Series 和 DataFrame 都有一个用于生成各类图表的 plot 方法。默认情况下，它们所生成的是线性图：1234567In [3]: from pandas import SeriesIn [4]: import numpy as npIn [5]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0,100,10))In [6]: s.plot() 线性图 该 Series 对象的索引会被传入 matplotlib，并用以绘制 X 轴。可以通过 use_index = False 禁用该功能。X 轴的刻度和界限可以通过 xticks 和 xlim 选项进行调节，Y轴同理。 pandas 的大部分绘图方法都有一个可选的 ax 参数，它可以是一个 matplotlib 的 subplot 对象。这使我们能够在网格布局中更为灵活地处理 subplot 的位置。 DataFrame 的 plot 方法会在一个 subplot 中为各列绘制一条线，并自动创建图例：1234567In [10]: df = DataFrame(np.random.randn(10,4).cumsum(0), columns = ['A','B','C','D'], index = np.arange(0,100,10))In [11]: df.plot()Out[11]: &lt;matplotlib.axes.AxesSubplot at 0x104b2d350&gt;DataFrame 图 Series.plot 方法的参数 参数 说明 label 用于图例的标签 ax 要在其上进行绘制的 matplotlib subplot 对象。如果没有设置，则使用当前 matplotlib subplot style 将要传给 matplotlib 的风格字符串 alpha 图表的填充不透明度（0到1之间） kind 可以是“line”、“bar”、“barh”、“kde” logy 在 Y 轴上使用对数标尺 use_index 将对象的索引用作刻度标签 rot 旋转刻度标签（0到360） xticks 用作 X 轴刻度的值 yticks 用作 Y 轴刻度的值 xlim X轴的界限（例如[0,10]） ylim Y轴的界限 grid 显示轴网格线（默认打开） DataFrame 的 plot 的参数： 参数 说明 subplots 将各个 DataFrame 列绘制到单独的 subplot 中 sharex 如果 subplots = True，则共用同一个 X轴，包括刻度和界限 sharey 如果 subplots = True，则共用同一个 Y轴 figsize 表示图像大小的元组 title 表示图像标题的字符串 legend 添加一个 subplot 图例（默认为 True） sort_columns 以字母表顺序绘制各列，默认使用当前列顺序 2.柱状图 在生成线性图的代码中加上 kind = ‘bar’（垂直柱状图）或 kind = ‘barh’（水平柱状图）即可生成柱状图。123456789In [15]: fg, axes = plt.subplots(2,1)In [16]: data = Series(np.random.rand(16),index = list('abcdefghijklmnop'))In [17]: data.plot(kind='bar',ax=axes[0],color='b',alpha=0.7)Out[17]: &lt;matplotlib.axes.AxesSubplot at 0x104c4ccd0&gt;In [18]: data.plot(kind='barh',ax=axes[1],color='k',alpha=0.7)Out[18]: &lt;matplotlib.axes.AxesSubplot at 0x10888d250&gt; 柱状图12345678910111213141516171819In [19]: #对于 DataFrame，柱状图会将每一行的值分为一组In [20]: df = DataFrame(np.random.rand(6,4), ....: index=['one','two','three','four','five','six'], ....: columns=pd.Index(['A','B','C','D'],name='Genus'))In [21]: dfOut[21]: Genus A B C Done 0.299380 0.926561 0.755805 0.231251two 0.892068 0.817717 0.958638 0.719738three 0.309930 0.428440 0.842603 0.154400four 0.325818 0.372899 0.189007 0.239295five 0.878813 0.968320 0.321430 0.014846six 0.277675 0.605484 0.339945 0.989320In [22]: df.plot(kind='bar')Out[22]: &lt;matplotlib.axes.AxesSubplot at 0x103d58650&gt;DataFrame 柱状图 注意，DataFrame 各列的名称“Genus”被用作了图例的标题，设置 stacked = True 即可为 DataFrame 生成堆积柱状图，这样每行的值就会被堆积在一起：12In [23]: df.plot(kind='bar', stacked=True,alpha=0.5)Out[23]: &lt;matplotlib.axes.AxesSubplot at 0x1063d7310&gt; 堆积的柱状图","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-VII-字符串操作","date":"2016-05-24T13:24:09.426Z","updated":"2016-06-19T07:13:24.330Z","comments":true,"path":"2016/05/24/PANDAS常用手册-VII-字符串操作/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-VII-字符串操作/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 Python 能够成为流行的数据处理语言，部分原因是其简单易用的字符串和文本处理功能。大部分文本运算都直接做成了字符串对象的内置方法。对于更为复杂的模式匹配和文本操作，则可能需要用到正则表达式。pandas 对此进行了加强，它使你能够对整组数据应用字符串表达式和正则表达式，而且能处理烦人的缺失数据。 二、字符串对象方法 对于大部分字符串处理应用而言，内置的字符串方法已经能够满足要求了。例如，以逗号分隔的字符串可以用 split 拆分成数段：12345678910111213141516171819202122232425262728293031323334353637383940414243444546In [131]: val = 'a,b guido'In [132]: val = 'a,b, guido'In [133]: val.split(',')Out[133]: ['a', 'b', ' guido']In [134]: #split 常常结合 strip（用于修剪空白符（包括换行符））In [135]: pieces = [x.strip() for x in val.split(',')]In [136]: piecesOut[136]: ['a', 'b', 'guido']In [137]: # 利用加法，可以将这些子字符串以双冒号分隔符的形式连接起来In [138]: first, second, third = piecesIn [139]: first + \"::\" + second + \"::\" + thirdOut[139]: 'a::b::guido'In [140]: # 但这种方式并不是很实用，有一种更快捷的方法In [141]: '::'.join(pieces)Out[141]: 'a::b::guido'In [142]: # 另一类方法关注的是子串定位In [143]: ' guido' in valOut[143]: TrueIn [144]: val.index(',')Out[144]: 1In [145]: val.find(':')Out[145]: -1In [146]: # count 函数可以返回指定子串出现的次数In [147]: val.count(',')Out[147]: 2In [148]: #replace 用于将指定模式替换成另一个模式In [149]: val.replace(',','::')Out[149]: 'a::b:: guido' Python 内置的字符串方法： 方法 说明 count 返回子串在字符串中的出现次数（非重叠） endswith、startswith 如果字符串以某个后缀结尾（以某个前缀开头），则返回 True join 将字符串用作连接其他字符串序列的分隔符 index 如果在字符串中找到子串，则返回第一个发现的子串的第一字符所在的位置。如果没找到，则引发 ValueError find 如果在字符串中找到子串，则返回第一个发现的子串的第一字符所在的位置。如果没找到，则返回-1 rfind 如果在字符串中找到子串，则返回最后一个发现的子串的第一字符所在的位置。如果没找到，则返回-1 replace 用另一个字符串替换指定子串 strip、rstrip、lstrip 去除空白符（包括换行符）。相当于对各个元素执行 x.strip()（以及 rstrip、lstrip） split 通过指定的分隔符将字符串拆分为一组子串 lower、upper 分别将字母字符转换为小写或大写 ljust、rjust 用空格（或其他字符）填充字符串的空白侧以返回符合最低宽度的字符串 三、正则表达式 正则表达式（通常称作 regex）提供了一种灵活的在文本中搜索或匹配字符串模式的方式。正则表达式是根据正则表达式语言编写的字符串。Python 内置的 re 模块负责对字符串应用正则表达式。（正则表达式的具体学习，请参照实验楼的相关课程，这里仅对 Python 内置模块使用的介绍） re 模块的函数可以分为三大类：模式匹配、替换以及拆分。当然，它们之间是相辅相成的。一个 regex 描述了需要在文本中定位的一个模式，它可以用于许多目的。假如我们想要拆分一个字符串，分隔符为数量不定的一组空白符（制表符、空格、换行符等）。描述一个或多个空白符的 regex 是\\s+1234567891011121314151617In [150]: import reIn [151]: text = 'foo bar\\t baz \\tqux'In [152]: re.split('\\s+', text)Out[152]: ['foo', 'bar', 'baz', 'qux'] 调用re.split('\\s+',text)时，正则表达式会先被编译，然后再在 text 上调用其 split 方法。你可以用 re.compile 自己编译 regex 以得到一个可重用的 regex 对象：In [153]: regex = re.compile('\\s+')In [154]: regex.split(text)Out[154]: ['foo', 'bar', 'baz', 'qux']In [155]: # 如果只希望得到匹配 regex 的所有模式，则可以使用 findall 方法In [156]: regex.findall(text)Out[156]: [' ', '\\t ', ' \\t'] 如果打算对许多字符串应用同一条正则表达式，强烈建议通过re.compile创建 regex 对象，这样将可以节省大量的 CPU时间123456789101112131415161718192021222324252627282930313233343536 `match` 和 `search` 跟 `findall` 功能类似。`findall` 返回的是字符串中所有的匹配项，而 `search` 则只返回第一个匹配项。`match` 更加严格，它只匹配字符串的首部。In [157]: text = \"\"\"Dave dave@google.com .....: Steve steve@gmail.com .....: Rob rob@gmail.com .....: Ryan ryan@yahoo.com .....: \"\"\"In [158]: pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]&#123;2,4&#125;'In [159]: #re.IGNORECASE 的作用是使正则表达式对大小写不敏感In [160]: regex = re.compile(pattern,flags = re.IGNORECASE)In [161]: #对 text 使用 findall 将得到一组电子邮件地址In [162]: regex.findall(text)Out[162]: ['dave@google.com', 'steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com']In [163]: #search 返回的是文本中第一个电子邮件地址In [164]: m = regex.search(text)In [165]: mOut[165]: &lt;_sre.SRE_Match at 0x109958d98&gt;In [166]: # 对于上面的 m，匹配项对象只能告诉我们模式在原字符串中的起始和结束位置In [167]: text[m.start():m.end()]Out[167]: 'dave@google.com'In [168]: #regex.match则返回 None，因为它只匹配出现在字符串开头的模式In [169]: print regex.match(text)None 正则表达式的方法如下： 方法 说明 findall、finditer 返回字符串中所有的非重叠匹配模式。findall 返回的是由所有模式组成的列表，而 finditer 则通过一个迭代器逐个返回 match 从字符串起始位置匹配模式，还可以对模式各部分进行分组。如果匹配到模式，则返回一个匹配项对象，否则返回 None search 扫描整个字符串一匹配模式。如果找到则返回一个匹配项对象。跟 match 不同，其匹配项可以位于字符串的任意位置，而不仅仅是起始处 split 根据找到的模式将字符拆分为数段 sub、subn 将字符串中所有的（sub）或前 n个（subn）模式替换为指定表达式。在替换字符串中可以通过\\1、\\2等符号表示各分组项 四、pandas 中矢量化的字符串函数 清理待分析的散乱数据时，常常需要做一些字符串规整化工作。更为复杂的情况是，含有字符串的列有时还含有缺失数据1234567891011121314151617181920In [184]: data = &#123;'Dave': 'dave@google.com','Steve':'steve@gmail.com', .....: 'Rob': 'rob@gmail.com','Wes':np.nan&#125;In [185]: data = Series(data)In [186]: dataOut[186]: Dave dave@google.comRob rob@gmail.comSteve steve@gmail.comWes NaNdtype: objectIn [187]: data.isnull()Out[187]: Dave FalseRob FalseSteve FalseWes Truedtype: bool 通过 data.map，所有字符串和正则表达式方法都能被应用于（传入 lambda表达式或其他函数）各个值，但是如果存在 NA 就会报错。为了解决这个问题，Series 有一些能够跳过 NA 值的字符串操作方法。通过 Series 的 str 属性即可访问这些方法。例如，我们可以通过 str.contains检查各个电子邮件地址是否含有“gmail”1234567891011121314151617181920In [188]: data.str.contains('gmail')Out[188]: Dave FalseRob TrueSteve TrueWes NaNdtype: objectIn [189]: # 这里也可以使用正则表达式In [190]: patternOut[190]: '([A-Z0-9._%+-]+)@([A-Z0-9.z-]+)\\\\.([A-Z]&#123;2,4&#125;)'In [191]: data.str.findall(pattern, flags = re.IGNORECASE)Out[191]: Dave [(dave, google, com)]Rob [(rob, gmail, com)]Steve [(steve, gmail, com)]Wes NaNdtype: object 有两个方法可以实现矢量化的元素获取操作：要么使用 str.get，要么在 str 属性上使用索引123456789101112131415161718192021222324252627282930313233In [192]: matches = data.str.match(pattern, flags = re.IGNORECASE)In [193]: matchesOut[193]: Dave (dave, google, com)Rob (rob, gmail, com)Steve (steve, gmail, com)Wes NaNdtype: objectIn [194]: matches.str.get(1)Out[194]: Dave googleRob gmailSteve gmailWes NaNdtype: objectIn [195]: matches.str[0]Out[195]: Dave daveRob robSteve steveWes NaNdtype: objectIn [196]: data.str[:5]Out[196]: Dave dave@Rob rob@gSteve steveWes NaNdtype: object 下面给出了矢量化的字符串方法： 方法 说明 cat 实现元素级的字符串连接操作，可指定分隔符 contains 返回表示各字符是否含有指定模式的布尔型数组 count 模式的出现次数 endswith、startswitch 相当于对各个元素执行 x.endswith(pattern)或 x.startswith(pattern) findall 计算各字符串的模式列表 get 获取各元素的第 i 个字符 join 根据指定的分隔符将 Series 中各元素的字符串连接起来 len 计算各字符串的长度 lower、upper 转换大小写。相当于各个元素执行 x.lower()或 x.upper() match 根据指定的正则表达式对各个元素执行 re.match pad 在字符串的左边、右边或左右两边添加空白符 center 相当于 pad(side = &#39;both&#39;) repeat 重复值 replace 用指定字符串替换找到的模式 slice 对 Series 中的各个字符串进行子串截取 split 根据分隔符或正则表达式对字符串进行拆分 strip、rstrip、lstrip 去除空白符，包括换行符。相当于对各个元素执行 x.strip()、x.rstrip()、x.lstrip()","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-VI-数据转换","date":"2016-05-24T13:23:57.382Z","updated":"2016-06-19T07:13:33.751Z","comments":true,"path":"2016/05/24/PANDAS常用手册-VI-数据转换/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-VI-数据转换/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验介绍 到目前为止，之前介绍的都是数据的重排。而另一类重要操作则是过滤、清理以及其他的转换工作。今天我们就来更深入的学习数据的转换知识吧！ 二、移除重复数据 DataFrame 中常常会出现重复行，举个例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172In [5]: data = DataFrame(&#123;'k1':['one']*3+['two']*4, 'k2':[1,1,2,3,3,4,4]&#125;)In [6]: dataOut[6]: k1 k20 one 11 one 12 one 23 two 34 two 35 two 46 two 4In [7]: #DataFrame 的 duplicated 方法返回一个布尔型 SeriessIn [8]: # 表示各行是否是重复行In [9]: data.duplicated()Out[9]: 0 False1 True2 False3 False4 True5 False6 Truedtype: boolIn [10]: #drop_duplicates 方法，用于返回一个移除了重复行的 DataFrameIn [11]: data.drop_duplicates()Out[11]: k1 k20 one 12 one 23 two 35 two 4In [12]: # 这个方法默认会判断全部列In [13]: #当然我们也可以指定部分列进行重复判断In [14]: data['v1'] = range(7)In [15]: dataOut[15]: k1 k2 v10 one 1 01 one 1 12 one 2 23 two 3 34 two 3 45 two 4 56 two 4 6In [16]: data.drop_duplicates(['k1'])Out[16]: k1 k2 v10 one 1 03 two 3 3In [17]: # 上面是只通过k1列过滤重复项 duplicated 和 drop_duplicates 默认保留的是第一个出现的值组合。传入 take_last = True 则保留最后一个：In [18]: data.drop_duplicates(['k1','k2'],take_last = True)Out[18]: k1 k2 v11 one 1 12 one 2 24 two 3 46 two 4 6 三、利用函数或映射进行数据转换 在对数据集进行转换时，我们希望根据数组、Series 或 DataFrame 列中的值来实现工作。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162In [4]: data = DataFrame(&#123;'food':['bacon','pulled pork','bacon', ...: 'pastrami','corned beef','bacon', ...: 'pastrami','honey ham','nova lox'], ...: 'ounces':[4, 3, 12, 6, 7.5, 8, 3, 5, 6]&#125;)In [5]: dataOut[5]: food ounces0 bacon 4.01 pulled pork 3.02 bacon 12.03 pastrami 6.04 corned beef 7.55 bacon 8.06 pastrami 3.07 honey ham 5.08 nova lox 6.0In [6]: # 我们添加一个食物来源In [7]: meat_to_animal = &#123; ...: 'bacon': 'pig', ...: 'pulled pork': 'pig', ...: 'pastrami': 'cow', ...: 'corned beef': 'cow', ...: 'honey ham': 'pig', ...: 'nova lox': 'salmon' ...: &#125;In [8]: #Series 的 map 方法可以接受一个函数或含有映射关系的字典型对象In [9]: data['animal'] = data['food'].map(meat_to_animal)In [10]: dataOut[10]: food ounces animal0 bacon 4.0 pig1 pulled pork 3.0 pig2 bacon 12.0 pig3 pastrami 6.0 cow4 corned beef 7.5 cow5 bacon 8.0 pig6 pastrami 3.0 cow7 honey ham 5.0 pig8 nova lox 6.0 salmonIn [11]: # 我们也可以传入一个能够完成全部工作的函数：In [12]: data['food'].map(lambda x: meat_to_animal[x])Out[12]: 0 pig1 pig2 pig3 cow4 cow5 pig6 cow7 pig8 salmonName: food, dtype: object 使用 map 是一种实现元素级转换以及其他数据清理工作的便捷方式。 四、替换值 利用 fillna 方法填充缺失数据可以看做值替换的一种特殊情况。虽然前面提到的 map 可用于修改对象的数据子集，而 replace 则提供了一种实现该功能的更简单、更灵活的方式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455In [20]: data = Series([1., -999., 2., -999., -1000., 3.])In [21]: dataOut[21]: 0 11 -9992 23 -9994 -10005 3dtype: float64In [22]: #接下来我想将-999 换成 NANIn [23]: data.replace(-999, np.nan)Out[23]: 0 11 NaN2 23 NaN4 -10005 3dtype: float64In [24]: # 如果我们想一次性换多个值In [25]: data.replace([-999,-1000],np.nan)Out[25]: 0 11 NaN2 23 NaN4 NaN5 3dtype: float64In [26]: data.replace([-999,-1000],[np.nan, 0])Out[26]: 0 11 NaN2 23 NaN4 05 3dtype: float64In [27]: data.replace(&#123;-999: np.nan, -1000: 0&#125;)Out[27]: 0 11 NaN2 23 NaN4 05 3dtype: float64 五、重命名轴索引 跟 Series 中的值一样，轴标签也可以通过函数或映射进行转换，从而得到一个新对象。轴还可以被就地修改，而无需新建一个数据结构。1234567891011121314151617181920212223242526272829303132333435363738In [28]: data = DataFrame(np.arange(12).reshape((3,4)), ....: index = ['Ohio','Colorado','New York'], ....: columns = ['one','two','three','four'])In [29]: # 跟 Series 一样，轴标签也有一个 map 方法In [30]: data.index.map(str.upper)Out[30]: array(['OHIO', 'COLORADO', 'NEW YORK'], dtype=object)In [31]: # 同样我们也可以赋值给 indexIn [32]: data.index = data.index.map(str.upper)In [33]: dataOut[33]: one two three fourOHIO 0 1 2 3COLORADO 4 5 6 7NEW YORK 8 9 10 11In [34]: # 如果想要创建数据集的转换版（不是修改原始数据）In [35]: data.rename(index = str.title,columns = str.upper)Out[35]: ONE TWO THREE FOUROhio 0 1 2 3Colorado 4 5 6 7New York 8 9 10 11In [36]: #rename 可以结合字典对象实现部分轴标签的更新In [37]: data.rename(index=&#123;'OHIO':'INDIANA'&#125;, ....: columns = &#123;'three':'peekaboo'&#125;)Out[37]: one two peekaboo fourINDIANA 0 1 2 3COLORADO 4 5 6 7NEW YORK 8 9 10 11 六、离散化和面元划分 为了便于分析，连续数据常常被离散化或拆分为“面元”（bin）。假设有一组人员数据，我们希望将它们划分为不同的年龄组： 1234567891011121314151617181920212223242526272829303132333435In [39]: ages = [20, 22, 25, 27, 21, 23, 24, 18, 37, 31,61, 42, 48, 32]In [40]: # 接下来我们将这些数据分成18~25、26~35、35~60和60岁以上In [41]: bins = [18, 25, 35, 60, 100]In [42]: cats = pd.cut(ages,bins)In [43]: catsOut[43]: [(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]Length: 14Categories (4, object): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]]In [44]: #pandas 用 cut 函数返回的是一个特殊的 Categorical 对象In [45]: #我们可以将其看做一组表示面元名称的字符串In [46]: #它含有一个表示不同分类名称的 levels 数组In [47]: cats.levelsOut[47]: Index([u'(18, 25]', u'(25, 35]', u'(35, 60]', u'(60, 100]'], dtype='object')In [48]: # 还有一个为年龄数据进行标号的 labels 属性In [49]: cats.labelsOut[49]: array([ 0, 0, 0, 1, 0, 0, 0, -1, 2, 1, 3, 2, 2, 1], dtype=int8)In [50]: pd.value_counts(cats)Out[50]: (18, 25] 6(35, 60] 3(25, 35] 3(60, 100] 1dtype: int64 如果向 cut 传入的是面元的数量而不是确切的面元边界，则它会根据数据的最小值和最大值计算等长面元。 1234567In [56]: data = np.random.rand(20)In [57]: pd.cut(data, 4, precision = 2)Out[57]: [(0.72, 0.96], (0.25, 0.49], (0.49, 0.72], (0.72, 0.96], (0.25, 0.49], ..., (0.014, 0.25], (0.72, 0.96], (0.49, 0.72], (0.25, 0.49], (0.72, 0.96]]Length: 20Categories (4, object): [(0.014, 0.25] &lt; (0.25, 0.49] &lt; (0.49, 0.72] &lt; (0.72, 0.96]] qcut 是一个非常类似于cut的函数，它可以根据样本分位数对数据进行面元划分。根据数据的分布情况，cut可能无法使各个面元含有相同数量的数据点。而qcut由于使用的是样本分位数，因此可以得到大小基本相等的面元：1234567891011121314151617In [58]: data = np.random.randn(1000)In [59]: cats = pd.qcut(data,4)In [60]: catsOut[60]: [(-0.699, 0.0415], [-3.0467, -0.699], (0.0415, 0.731], (0.731, 3.0755], [-3.0467, -0.699], ..., (0.0415, 0.731], [-3.0467, -0.699], (-0.699, 0.0415], [-3.0467, -0.699], [-3.0467, -0.699]]Length: 1000Categories (4, object): [[-3.0467, -0.699] &lt; (-0.699, 0.0415] &lt; (0.0415, 0.731] &lt; (0.731, 3.0755]]In [61]: pd.value_counts(cats)Out[61]: (0.731, 3.0755] 250(0.0415, 0.731] 250(-0.699, 0.0415] 250[-3.0467, -0.699] 250dtype: int64 七、检测和过滤异常值 异常值（outlier）的过滤或变换运算在很大程度上其实就是数组运算。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859In [63]: np.random.seed(12345)In [64]: data = DataFrame(np.random.randn(1000,4))In [65]: data.describe()Out[65]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.067684 0.067924 0.025598 -0.002298std 0.998035 0.992106 1.006835 0.996794min -3.428254 -3.548824 -3.184377 -3.74535625% -0.774890 -0.591841 -0.641675 -0.64414450% -0.116401 0.101143 0.002073 -0.01361175% 0.616366 0.780282 0.680391 0.654328max 3.366626 2.653656 3.260383 3.927528In [66]: # 假设你想要找出某列中绝对值大小超过3的值In [67]: col = data[3]In [68]: col[np.abs(col) &gt; 3]Out[68]: 97 3.927528305 -3.399312400 -3.745356Name: 3, dtype: float64In [69]: # 要选出全部的绝对值大于三的行In [70]: data[(np.abs(data)&gt;3).any(1)]Out[70]: 0 1 2 35 -0.539741 0.476985 3.248944 -1.02122897 -0.774363 0.552936 0.106061 3.927528102 -0.655054 -0.565230 3.176873 0.959533305 -2.315555 0.457246 -0.025907 -3.399312324 0.050188 1.951312 3.260383 0.963301400 0.146326 0.508391 -0.196713 -3.745356499 -0.293333 -0.242459 -3.056990 1.918403523 -3.428254 -0.296336 -0.439938 -0.867165586 0.275144 1.179227 -3.184377 1.369891808 -0.362528 -3.548824 1.553205 -2.186301900 3.366626 -2.372214 0.851010 1.332846In [71]: #根据这些条件，既可以轻松对值进行设置In [72]: data[np.abs(data)&gt;3] = np.sign(data)*3In [73]: data.describe()Out[73]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.067623 0.068473 0.025153 -0.002081std 0.995485 0.990253 1.003977 0.989736min -3.000000 -3.000000 -3.000000 -3.00000025% -0.774890 -0.591841 -0.641675 -0.64414450% -0.116401 0.101143 0.002073 -0.01361175% 0.616366 0.780282 0.680391 0.654328max 3.000000 2.653656 3.000000 3.000000 八、排列和随机采样 利用 numpy.random.permutation 函数可以轻松实现对 Series 和 DataFrame 的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用 permutation，产生一个表示新顺序的整数数组1234567891011121314151617181920212223242526In [74]: df = DataFrame(np.arange(20).reshape(5,4))In [75]: sampler = np.random.permutation(5)In [76]: samplerOut[76]: array([1, 0, 2, 3, 4])In [77]: # 然后就可以在基于 ix 的索引操作或 take 函数中使用该数组In [78]: dfOut[78]: 0 1 2 30 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 154 16 17 18 19In [79]: df.take(sampler)Out[79]: 0 1 2 31 4 5 6 70 0 1 2 32 8 9 10 113 12 13 14 154 16 17 18 19 如果不想用替换的方式选取随机子集，则可以使用 permutation：从 permutation 返回的数组中切下前 k 个元素，其中 k 为希望的子集大小。123456In [81]: df.take(np.random.permutation(len(df))[:3])Out[81]: 0 1 2 31 4 5 6 73 12 13 14 154 16 17 18 19 九、计算指标/哑变量 另一种常用于统计建模或机器学习的转换方式：将分类变量（categorical variable）转换为“哑变量矩阵”（dummy matrix）或“指标矩阵”（indicator matrix）。如果 DataFrame 的某一列中含有 k 个不同的值，则可以派生出一个 k 列矩阵或 DataFrame（其值全为1和0）。pandas 有一个 get_dummies 函数可以实现该功能：1234567891011121314151617181920212223242526272829303132333435363738In [89]: df = DataFrame(&#123;'key':['b','b','a','c','a','b'], ....: 'data1':range(6)&#125;)In [90]: pd.get_dummies(df['key'])Out[90]: a b c0 0 1 01 0 1 02 1 0 03 0 0 14 1 0 05 0 1 0In [91]: dfOut[91]: data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 bIn [92]: # 我们可能想给指标 DataFrame 的列加上一个前缀，以便合并In [93]: dummies = pd.get_dummies(df['key'],prefix = 'key')In [94]: df_with_dummy = df[['data1']].join(dummies)In [95]: df_with_dummyOut[95]: data1 key_a key_b key_c0 0 0 1 01 1 0 1 02 2 1 0 03 3 0 0 14 4 1 0 05 5 0 1 0 如果 DataFrame 中的某行同属于多个分类，则事情就会有点复杂。同样的同学们，木木这里给大家一个数据文件，同学们把他们粘贴到文本文件中，等会会用到1234567891011121314151617181920212223242526272829301::Toy Story (1995)::Animation|Children's|Comedy2::Jumanji (1995)::Adventure|Children's|Fantasy3::Grumpier Old Men (1995)::Comedy|Romance4::Waiting to Exhale (1995)::Comedy|Drama5::Father of the Bride Part II (1995)::Comedy6::Heat (1995)::Action|Crime|Thriller7::Sabrina (1995)::Comedy|Romance8::Tom and Huck (1995)::Adventure|Children's9::Sudden Death (1995)::Action10::GoldenEye (1995)::Action|Adventure|Thriller11::American President, The (1995)::Comedy|Drama|Romance12::Dracula: Dead and Loving It (1995)::Comedy|Horror13::Balto (1995)::Animation|Children's14::Nixon (1995)::Drama15::Cutthroat Island (1995)::Action|Adventure|Romance16::Casino (1995)::Drama|Thriller17::Sense and Sensibility (1995)::Drama|Romance18::Four Rooms (1995)::Thriller19::Ace Ventura: When Nature Calls (1995)::Comedy20::Money Train (1995)::Action21::Get Shorty (1995)::Action|Comedy|Drama22::Copycat (1995)::Crime|Drama|Thriller23::Assassins (1995)::Thriller24::Powder (1995)::Drama|Sci-Fi25::Leaving Las Vegas (1995)::Drama|Romance26::Othello (1995)::Drama27::Now and Then (1995)::Drama28::Persuasion (1995)::Romance29::City of Lost Children, The (1995)::Adventure|Sci-Fi30::Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)::Drama 示例如下：12345678910111213141516171819In [103]: mnames = ['movie_id','title','genres']In [104]: movies = pd.read_table('movies.dat', sep='::',header=None, .....: names = mnames)In [105]: movies[:10]Out[105]: movie_id title genres0 1 Toy Story (1995) Animation|Children's|Comedy1 2 Jumanji (1995) Adventure|Children's|Fantasy2 3 Grumpier Old Men (1995) Comedy|Romance3 4 Waiting to Exhale (1995) Comedy|Drama4 5 Father of the Bride Part II (1995) Comedy5 6 Heat (1995) Action|Crime|Thriller6 7 Sabrina (1995) Comedy|Romance7 8 Tom and Huck (1995) Adventure|Children's8 9 Sudden Death (1995) Action9 10 GoldenEye (1995) Action|Adventure|Thriller 要为每一个 genre 添加指标变量就需要做一些数据规整操作。首先，我们从数据集中抽取出不同的 genre 值（注意巧用 set.union）：123456789101112131415161718192021222324252627282930313233343536373839In [106]: genre_iter = (set(x.split('|')) for x in movies.genres)In [107]: genres = sorted(set.union(*genre_iter))In [108]: # 现在我们从一个全零 DataFrame 开始构建指标 DataFrameIn [109]: dummies = DataFrame(np.zeros((len(movies),len(genres))), .....: columns = genres)In [110]: # 接下来，迭代每一部电影并将 dummies 各行的项设置为1In [111]: for i,gen in enumerate(movies.genres): .....: dummies.ix[i, gen.split('|')]=1 .....: In [112]: # 在将其与 movie是合并起来In [113]: movies_windic = movies.join(dummies.add_prefix('Genre_'))In [114]: movies_windic.ix[0]Out[114]: movie_id 1title Toy Story (1995)genres Animation|Children's|ComedyGenre_Action 0Genre_Adventure 0Genre_Animation 1Genre_Children's 1Genre_Comedy 1Genre_Crime 0Genre_Drama 0Genre_Fantasy 0Genre_Horror 0Genre_Romance 0Genre_Sci-Fi 0Genre_Thriller 0Name: 0, dtype: object 一个对统计应用有用的秘诀是：结合 get_dummies 和诸如 cut 之类的离散化函数12345678910111213141516171819202122In [127]: values = np.random.rand(10)In [128]: valuesOut[128]: array([ 0.65158907, 0.61562033, 0.53089883, 0.13941997, 0.48344916, 0.11734259, 0.07557764, 0.21863608, 0.90953758, 0.38441329])In [129]: bins = [0,0.2,0.4,0.6,0.8,1]In [130]: pd.get_dummies(pd.cut(values,bins))Out[130]: (0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1]0 0 0 0 1 01 0 0 0 1 02 0 0 1 0 03 1 0 0 0 04 0 0 1 0 05 1 0 0 0 06 1 0 0 0 07 0 1 0 0 08 0 0 0 0 19 0 1 0 0 0","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-V-重塑和轴向旋转","date":"2016-05-24T13:23:42.320Z","updated":"2016-06-19T07:13:39.998Z","comments":true,"path":"2016/05/24/PANDAS常用手册-V-重塑和轴向旋转/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-V-重塑和轴向旋转/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 有许多重新排列表格型数据的基础运算。这些函数也称做重塑（reshape）或轴向旋转（pivot）运算。 二、重塑层次化索引 层次化索引为 DataFrame 数据的重排任务提供了一种具有良好一致性的方式。主要功能有二： stack ：将数据的列“旋转”为行 ustack：将数据的行”旋转“为列 接下来看一个简单的 DataFrame，其中的行列索引均为字符串：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140In [183]: data = DataFrame(np.arange(6).reshape((2,3)), .....: index = pd.Index(['Ohio','Colorado'],name = 'state'), .....: columns = pd.Index(['one','two','three'],name = 'number'))In [184]: dataOut[184]: number one two threestate Ohio 0 1 2Colorado 3 4 5In [185]: # 使用该数据的 stack 方法即可将列转换为行，得到一个 SeriesIn [186]: result = data.stack()In [187]: resultOut[187]: state numberOhio one 0 two 1 three 2Colorado one 3 two 4 three 5dtype: int64In [188]: # 对于一个层次化索引的 Series，你可以用 unstack 将其重排位一个 DataFrameIn [189]: result.unstack()Out[189]: number one two threestate Ohio 0 1 2Colorado 3 4 5In [190]: # 传入分层级别的编号或名称即可对其他级别进行unstack 操作In [191]: result.unstack(0)Out[191]: state Ohio Coloradonumber one 0 3two 1 4three 2 5In [192]: result.unstack('state')Out[192]: state Ohio Coloradonumber one 0 3two 1 4three 2 5In [193]: # 如果不是所有的级别值都能在各分组中招到的话，则 unstack 操作可能会引入缺失数据In [195]: s1 = Series([0,1,2,3],index = ['a','b','c','d'])In [196]: s2 = Series([4,5,6],index = ['c','d','e'])In [197]: data2 = pd.concat([s1,s2],keys = ['one','two'])In [198]: data2.unstack()Out[198]: a b c d eone 0 1 2 3 NaNtwo NaN NaN 4 5 6In [199]: data2Out[199]: one a 0 b 1 c 2 d 3two c 4 d 5 e 6dtype: int64In [200]: #stack 默认会滤除缺失数据，因此该运算是可逆的In [201]: data2.unstack().stack()Out[201]: one a 0 b 1 c 2 d 3two c 4 d 5 e 6dtype: float64In [202]: data2.unstack().stack(dropna=False)Out[202]: one a 0 b 1 c 2 d 3 e NaNtwo a NaN b NaN c 4 d 5 e 6dtype: float64In [203]: # 在对 DataFrame 进行 unstack 操作时，作为旋转轴的级别将会成为结果中的最低级别：In [204]: df = DataFrame(&#123;'left':result,'right':result +5&#125;, .....: columns = pd.Index(['left','right'],name = 'side'))In [205]: dfOut[205]: side left rightstate number Ohio one 0 5 two 1 6 three 2 7Colorado one 3 8 two 4 9 three 5 10In [206]: df.unstack('state')Out[206]: side left right state Ohio Colorado Ohio Coloradonumber one 0 3 5 8two 1 4 6 9three 2 5 7 10In [207]: df.unstack('state').stack('side')Out[207]: state Ohio Coloradonumber side one left 0 3 right 5 8two left 1 4 right 6 9three left 2 5 right 7 10 三、将“长格式”旋转为“宽格式”时间序列数据通常是以所谓的“长格式”或“堆叠格式”存储在数据库和 CSV 中。 在这里，给大家一组数据，同学们把它粘贴到一个文本文件中，一会儿用到：12345678910111213141516171819202122232425data,item,value1959-03-31 00:00:00,realgdp,2710.3491959-03-31 00:00:00,infl,0.0001959-03-31 00:00:00,unemp,5.8001959-06-30 00:00:00,realgdp,2778.8011959-06-30 00:00:00,infl,2.3401959-06-30 00:00:00,unemp,5.1001959-09-30 00:00:00,realgdp,2775.4881959-09-30 00:00:00,infl,2.7401959-09-30 00:00:00,unemp,5.3001959-12-30 00:00:00,realgdp,2785.2041959-12-30 00:00:00,infl,2.8401959-12-30 00:00:00,unemp,5.4301960-3-31 00:00:00,realgdp,2715.4841960-3-31 00:00:00,infl,2.6401960-3-31 00:00:00,unemp,5.3301960-6-30 00:00:00,realgdp,2775.3201960-6-30 00:00:00,infl,2.8301960-6-30 00:00:00,unemp,5.6701960-09-30 00:00:00,realgdp,2788.1021960-09-30 00:00:00,infl,2.5301960-09-30 00:00:00,unemp,5.4111960-12-30 00:00:00,realgdp,2782.1921960-12-30 00:00:00,infl,2.6901960-12-30 00:00:00,unemp,5.621 我们先把数据读取出来： 123456789101112131415In [7]: data = pd.read_csv('data.csv')In [8]: data[:10]Out[8]: data item value0 1959-03-31 00:00:00 realgdp 2710.3491 1959-03-31 00:00:00 infl 0.0002 1959-03-31 00:00:00 unemp 5.8003 1959-06-30 00:00:00 realgdp 2778.8014 1959-06-30 00:00:00 infl 2.3405 1959-06-30 00:00:00 unemp 5.1006 1959-09-30 00:00:00 realgdp 2775.4887 1959-09-30 00:00:00 infl 2.7408 1959-09-30 00:00:00 unemp 5.3009 1959-12-30 00:00:00 realgdo 2785.204 长格式的数据操作起来可能不那么轻松。我们可能更喜欢 DataFrame，不同的 item 值分别形成一列，date 列中的时间值作为索引。DataFrame 的 pivot 方法完全可以实现这个转换：1234567891011In [15]: pivoted = data.pivot('data','item','value')In [16]: pivoted.head()Out[16]: item infl realgdp unempdata 1959-03-31 00:00:00 0.00 2710.349 5.8001959-06-30 00:00:00 2.34 2778.801 5.1001959-09-30 00:00:00 2.74 2775.488 5.3001959-12-30 00:00:00 2.84 2785.204 5.4301960-09-30 00:00:00 2.53 2788.102 5.411 前两个参数值分别用作行和列索引的列名，最后一个参数值则是用于填充 DataFrame 的数据列的列名。假设有两个需要参与重塑的数据列：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859In [17]: data['value2'] = np.random.randn(len(data))In [18]: data[:10]Out[18]: data item value value20 1959-03-31 00:00:00 realgdp 2710.349 -0.9104271 1959-03-31 00:00:00 infl 0.000 -1.1655012 1959-03-31 00:00:00 unemp 5.800 0.9893073 1959-06-30 00:00:00 realgdp 2778.801 -0.7981334 1959-06-30 00:00:00 infl 2.340 0.5124935 1959-06-30 00:00:00 unemp 5.100 0.4029406 1959-09-30 00:00:00 realgdp 2775.488 -0.5841857 1959-09-30 00:00:00 infl 2.740 -1.8921808 1959-09-30 00:00:00 unemp 5.300 1.6106529 1959-12-30 00:00:00 realgdp 2785.204 -1.431110In [19]: # 如果忽略最后一个参数，得到的 DataFrame 就会带有层次化In [20]: pivoted = data.pivot('data','item')In [21]: pivoted[:5]Out[21]: value value2 item infl realgdp unemp infl realgdp unempdata 1959-03-31 00:00:00 0.00 2710.349 5.800 -1.165501 -0.910427 0.9893071959-06-30 00:00:00 2.34 2778.801 5.100 0.512493 -0.798133 0.4029401959-09-30 00:00:00 2.74 2775.488 5.300 -1.892180 -0.584185 1.6106521959-12-30 00:00:00 2.84 2785.204 5.430 -2.142333 -1.431110 0.5554741960-09-30 00:00:00 2.53 2788.102 5.411 -2.077777 -1.096230 -0.175941In [22]: pivoted['value'][:5]Out[22]: item infl realgdp unempdata 1959-03-31 00:00:00 0.00 2710.349 5.8001959-06-30 00:00:00 2.34 2778.801 5.1001959-09-30 00:00:00 2.74 2775.488 5.3001959-12-30 00:00:00 2.84 2785.204 5.4301960-09-30 00:00:00 2.53 2788.102 5.411In [23]: # 注意，pivot 其实只是一个快捷方式In [24]: #用 set_indexIn [25]: # 再用 unstack 重塑In [26]: unstacked = data.set_index(['data','item']).unstack('item')In [27]: unstacked[:5]Out[27]: value value2 item infl realgdp unemp infl realgdp unempdata 1959-03-31 00:00:00 0.00 2710.349 5.800 -1.165501 -0.910427 0.9893071959-06-30 00:00:00 2.34 2778.801 5.100 0.512493 -0.798133 0.4029401959-09-30 00:00:00 2.74 2775.488 5.300 -1.892180 -0.584185 1.6106521959-12-30 00:00:00 2.84 2785.204 5.430 -2.142333 -1.431110 0.5554741960-09-30 00:00:00 2.53 2788.102 5.411 -2.077777 -1.096230 -0.175941","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-IV-合并数据集","date":"2016-05-24T13:23:33.207Z","updated":"2016-06-19T07:06:50.904Z","comments":true,"path":"2016/05/24/PANDAS常用手册-IV-合并数据集/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-IV-合并数据集/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程《PYTHON数据分析》 一、实验简介 数据分析和建模方面的大量编程工作都是用在数据准备上：加载、清理、转换以及重塑。有时候，存放在文件或数据库中的数据并不能满足你的数据处理应用的要求。许多人都选择使用普通编程语言（如 Python、Perl、R或 Java）或 UNIX 文本处理工具（如 sed 或 awk）对数据格式进行专门处理。幸运的是，pandas 和 Python 标准库提供了一组高级的、灵活的、高效的核心函数和算法，它们使你能轻松地将数据规整化为正确的形式。 二、pandas 内置方法 pandas 对象中的数据可以通过一些内置的方式进行合并： pandas.merge 可根据一个或多个键将不同 DataFrame 中的行连接起来。SQL 或其他关系型数据库的用户对此应该会比较熟悉，因为它实现的就是数据库的连接操作。 pandas.concat 可以沿着一条轴将多个对象堆叠到一起 实例方法 combine_first 可以将重复数据编接在一起，用一个对象中的值填充另一个对象中的缺失值 三、数据库风格的 DataFrame 合并 数据集的合并（merge）或连接（join）运算是通过一个或多个键将行链接起来。这些运算是关系型数据库的核心。pandas 的 merge 函数是对数据应用这些算法的主要切入点。 我们以一个简单的例子开始：1234567891011121314151617181920212223In [68]: df1 = DataFrame(&#123;'key':['b','b','a','c','a','a','b'], 'data1':range(7)&#125;)In [69]: df2 = DataFrame(&#123;'key':['a','b','d'], ....: 'data2':range(3)&#125;)In [70]: df1Out[70]: data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 a6 6 bIn [71]: df2Out[71]: data2 key0 0 a1 1 b2 2 d 这是一种多对一的合并。df1中的数据有多个被标记为 a 和 b 的行，而 df2 中 key 列的每个值则仅对应一行。对这些对象调用 merge 即可得到：123456789In [73]: pd.merge(df1,df2)Out[73]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 注意，我并没有指明要哪个列进行连接。如果没有指定，merge 就会重叠列的列名当做键。不过最好显示指定一下。123456789In [74]: pd.merge(df1,df2,on='key')Out[74]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 0 如果两个对象的列名不同，也可以分别进行指定：123456789101112131415In [75]: df3 = DataFrame(&#123;'key1':['a','a','b','b','a','a','c'], ....: 'data1': range(7)&#125;)In [76]: df4 = DataFrame(&#123;'key2':['a','b','d'], ....: 'data2': range(3)&#125;)In [77]: pd.merge(df3, df4, left_on='key1',right_on='key2')Out[77]: data1 key1 data2 key20 0 a 0 a1 1 a 0 a2 4 a 0 a3 5 a 0 a4 2 b 1 b5 3 b 1 b 可能你已经注意到了，结果里面 c 和 d 以及与之相关的数据消失了。默认情况下，merge 做的是inner连接，结果中的键是交集，其他方式还有left、right以及outer。外连接求取得是键的并集，组合了左连接和右连接的效果：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394In [78]: pd.merge(df1,df2, how = 'right')Out[78]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 06 NaN d 2In [79]: pd.merge(df1,df2, how = 'left')Out[79]: data1 key data20 0 b 11 1 b 12 2 a 03 3 c NaN4 4 a 05 5 a 06 6 b 1In [80]: pd.merge(df1,df2, how = 'outer')Out[80]: data1 key data20 0 b 11 1 b 12 6 b 13 2 a 04 4 a 05 5 a 06 3 c NaN7 NaN d 2In [81]: # 多对多的合并操作非常简单，无需额外的工作In [82]: df2 = DataFrame(&#123;'key':['a','b','a','b','d'], ....: 'data2':range(5)&#125;)In [83]: df1Out[83]: data1 key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 a6 6 bIn [84]: df2Out[84]: data2 key0 0 a1 1 b2 2 a3 3 b4 4 dIn [85]: pd.merge(df1,df2,on='key',how='left')`Out[85]: data1 key data20 0 b 11 0 b 32 1 b 13 1 b 34 2 a 05 2 a 26 3 c NaN7 4 a 08 4 a 29 5 a 010 5 a 211 6 b 112 6 b 3In [86]: # 多对多连接产生的是行的笛卡尔积In [87]: #要根据多个键进行合并，传入一个由列名组成的列表即可：In [88]: left = DataFrame(&#123;'key1':['a','a','b'], ....: 'key2':['c','d','c'], ....: 'data1':[1,2,3]&#125;)In [89]: right = DataFrame(&#123;'key1':['a','a','b','b'], ....: 'key2':['c','c','c','d'], ....: 'data2':[4,5,6,7]&#125;)In [90]: pd.merge(left,right,on=['key1','key2'],how='outer')Out[90]: data1 key1 key2 data20 1 a c 41 1 a c 52 2 a d NaN3 3 b c 64 NaN b d 7 对于合并运算需要考虑的最后一个问题是对重复列名的处理。虽然你可以手工处理列名重叠的问题（稍后将会介绍如何重命名轴标签），但 merge 有一个更实用的 suffixes 选项，用于指定附加到左右两个 DataFrame对象的重叠列名上的字符串： 好了，我们来看一看 merge 函数的参数吧： 参数 说明 left 参与合并的左侧 DataFrame right 参与合并的右侧 DataFrame how “inner”、”outer”、”left”、”right”其中之一。默认为”inner” on 用于连接的列名。必须存在于左右两个 DataFrame 对象中。如果未指定，且其他连接键也未指定，则以 left 和 right 列名的交集作为连接键 left_on 左侧 DataFrame 中用做连接键的列 right_on 右侧 DataFreme 中用作连接键的列 left_index 将左侧的行索引用作其连接键 right_index 类似于 left_index sort 根据连接键对合并后的数据进行排序，默认为 True。有时在处理大数据集时，禁用该项可获得更好的性能 suffixes 字符串值元组，用于追加到重叠列名的末尾，默认为（’_x’,’_y’）。例如，如果左右两个 DataFrame 对象都有“data”，则结果就会出现”data_x”和”data_y” copy 设置为 False，可以在某些特殊情况下避免将数据复制到结果数据结构。默认总是复制 四、索引上的合并 有时候，DataFrame 中的连接键位于其索引中。在这种情况下，你可以传入 left_index = True 或 right_index = True(或两个都传)以说明索引应该被用作连接键123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384In [96]: left1 = DataFrame(&#123;'key':['a','b','a','a','b','c'], ....: 'value':range(6)&#125;)In [97]: right1 = DataFrame(&#123;'group_val':[3.5,7]&#125;,index=['a','b'])In [98]: left1Out[98]: key value0 a 01 b 12 a 23 a 34 b 45 c 5In [99]: right1Out[99]: group_vala 3.5b 7.0In [100]: pd.merge(left1,right1,left_on='key',right_index=True)Out[100]: key value group_val0 a 0 3.52 a 2 3.53 a 3 3.51 b 1 7.04 b 4 7.0In [101]: # 由于默认的 merge 方法是求取连接的交集In [102]: #因此你可以通过外连接的方式得到它们的并集In [103]: pd.merge(left1,right1,left_on='key',right_index=True,how='outer')Out[103]: key value group_val0 a 0 3.52 a 2 3.53 a 3 3.51 b 1 7.04 b 4 7.05 c 5 NaNIn [104]: #对于层次化索引的数据，就有点复杂了In [105]: lefth = DataFrame(&#123;'key1':['Ohio','Ohio','Ohio','Nevada','Nevada'], .....: 'key2':[2000,2001,2002,2001,2002], .....: 'data':np.arange(5)&#125;)In [106]: righth = DataFrame(np.arange(12).reshape((6,2)), .....: index = [['Nevada','Nevada','Ohio','Ohio','Ohio','Ohio'], .....: [2001,2000,2000,2000,2001,2002]], .....: columns = ['event1','event2']) In [107]: lefthOut[107]: data key1 key20 0 Ohio 20001 1 Ohio 20012 2 Ohio 20023 3 Nevada 20014 4 Nevada 2002In [108]: righthOut[108]: event1 event2Nevada 2001 0 1 2000 2 3Ohio 2000 4 5 2000 6 7 2001 8 9 2002 10 11In [109]: # 这种情况，你必须以列表的形式指明用作合并键的多个列In [110]: pd.merge(lefth,righth,left_on=['key1','key2'],right_index=True)Out[110]: data key1 key2 event1 event20 0 Ohio 2000 4 50 0 Ohio 2000 6 71 1 Ohio 2001 8 92 2 Ohio 2002 10 113 3 Nevada 2001 0 1 DataFrame 还有一个 join 实例方法，它能更为方便地实现按索引合并。它还可用于合并多个带有相同或相似索引的 DataFrame 对象，而不管它们之间有没有重叠的列： 12345678910111213141516171819202122232425262728293031323334353637383940In [119]: left2.join(right2,how='outer')Out[119]: Ohio Nevada Missouri Alabamaa 1 2 NaN NaNb NaN NaN 7 8c 3 4 9 10d NaN NaN 11 12e 5 6 13 14In [120]: left1.join(right1,on = 'key')Out[120]: key value group_val0 a 0 3.51 b 1 7.02 a 2 3.53 a 3 3.54 b 4 7.05 c 5 NaNIn [121]: # 对于简单的索引合并，我们还可以向 join 传入一组 DataFrameIn [122]: another = DataFrame([[7,8],[9,10],[11,12],[16,17]], .....: index = ['a','c','e','f'],columns = ['New York','Oregon'])In [123]: left2.join([right2,another])Out[123]: Ohio Nevada Missouri Alabama New York Oregona 1 2 NaN NaN 7 8c 3 4 9 10 9 10e 5 6 13 14 11 12In [124]: left2.join([right2,another],how='outer')Out[124]: Ohio Nevada Missouri Alabama New York Oregona 1 2 NaN NaN 7 8b NaN NaN 7 8 NaN NaNc 3 4 9 10 9 10d NaN NaN 11 12 NaN NaNe 5 6 13 14 11 12f NaN NaN NaN NaN 16 17 五、轴向连接 另一种数据合并运算也被称作连接（concatenation）、绑定（binding）或堆叠（stacking）。NumPy 有一个用于合并原始 NumPy 数组的 concatenate 函数： 12345678910111213In [126]: arr = np.arange(12).reshape((3,4))In [127]: arrOut[127]: array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]])In [128]: np.concatenate([arr,arr],axis=1)Out[128]: array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]]) 对于 pandas 对象（如 Series 和 DataFrame），带有标签的轴使你能够进一步推广数组的连接运算。具体点说，我们还需要考虑以下这些东西： 如果各对象其他轴上的索引不同，那些轴应该是做并集还是交集？ 结果对象中的分组需要各不相同吗？ 用于连接的轴重要吗？ pandas 的 concat函数提供了一种能够解决这些问题的可靠方式。我将给出一些例子来讲解其使用方式。假设有三个没有重叠索引的 Series：123456789101112131415161718192021222324252627282930313233In [133]: s1 = Series([0,1],index=['a','b'])In [134]: s2 = Series([2,3,4],index = ['c','d','e'])In [135]: s3 = Series([5,6],index = ['f','g'])In [136]: # 对这些对象调用 concat 可以将值和索引粘合在一起In [137]: pd.concat([s1,s2,s3])Out[137]: a 0b 1c 2d 3e 4f 5g 6dtype: int64In [138]: # 默认情况下，concat 是在 axis=0上工作的，最终产生一个新的 SeriesIn [139]: #如果出入 axis=1，则会变成一个 DataFrameIn [140]: pd.concat([s1,s2,s3],axis=1)Out[140]: 0 1 2a 0 NaN NaNb 1 NaN NaNc NaN 2 NaNd NaN 3 NaNe NaN 4 NaNf NaN NaN 5g NaN NaN 6 concat函数的参数： 参数 说明 objs 参与连接的 pandas 对象的列表或字典。唯一必需的参数 axis 指明连接的轴向，默认为0 join “inner”、”outer”其中之一，默认为”outer”。指明其他轴向上的索引是按交集（inner）还是并集（outer）进行合并 join_axes 指明用于其他 n-1 条轴的索引，不执行并集/交集运算 keys 与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、数组列表（如果将 levels 设置成多级数组的话） levels 指定用作层次化索引各级别上的索引，如果设置了 keys 的话 names 用于创建分层级别的名称，如果设置了 keys 或 levels 的话 verify_integrity 检查结果对象新轴上的重复情况，如果发现则引发异常。默认（False）允许重复 ignore_index 不保留连接轴上的索引，产生一组新索引 range（total_length） 六、合并重叠数据 还有一种数据组合问题不能用简单的合并（merge）或连接（concatenation）运算来处理。比如，我们可能有索引全部或部分重叠的两个数据集. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960In [170]: a = Series([np.nan,2.5,np.nan,3.5,4.5,np.nan], index = ['f','e','d','c','b','a'])In [171]: b = Series(np.arange(len(a),dtype = np.float64), index = ['f','e','d','c','b','a'])In [172]: b[-1]=np.nanIn [173]: aOut[173]: f NaNe 2.5d NaNc 3.5b 4.5a NaNdtype: float64In [174]: bOut[174]: f 0e 1d 2c 3b 4a NaNdtype: float64In [175]: np.where(pd.isnull(a),b,a)Out[175]: array([ 0. , 2.5, 2. , 3.5, 4.5, nan])In [176]: #Series 有一个 combine_first 方法，实现的也是一样的功能，而且会进行数据对齐In [177]: b[:-2].combine_first(a[2:])Out[177]: a NaNb 4.5c 3.0d 2.0e 1.0f 0.0dtype: float64In [178]: # 对于 DataFrame 也是如此In [179]: df1 = DataFrame(&#123;'a':[1,np.nan,5,np.nan], .....: 'b':[np.nan,2,np.nan,6], .....: 'c':range(2,18,4)&#125;)In [180]: df2 = DataFrame(&#123;'a':[5,4,np.nan,3,7], .....: 'b':[np.nan,3,4,6,8]&#125;)In [181]: df1.combine_first(df2)Out[181]: a b c0 1 NaN 21 4 2 62 5 4 103 3 6 144 7 8 NaN","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-III-使用数据库","date":"2016-05-24T13:23:25.464Z","updated":"2016-06-19T07:13:26.660Z","comments":true,"path":"2016/05/24/PANDAS常用手册-III-使用数据库/","link":"","permalink":"http://yoursite.com/2016/05/24/PANDAS常用手册-III-使用数据库/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 在许多应用中，数据很少取自文本文件，因为用这种方式存储大量数据很低效。基于 SQL 的关系型数据库（如 SQL Server、PostgreSQL 和 MySQL 等）使用非常广泛，此外还有一些非 SQL（即所谓的 NoSQL）型数据库也变得非常流行。 二、数据库简介 将数据从 SQL 加载到 DataFrame 得过程很简单，此外 pandas 还有一些能够简化该过程的函数。例如，我们将使用一款嵌入式的 SQLite 数据库（通过 Python 内置的 sqlite3 驱动器）123456789101112131415161718192021222324In [42]: import sqlite3In [43]: query = \"\"\" ....: CREATE TABLE test ....: (a VARCHAR(20),b VARCHAR(20), ....: c REAL, d INTEGER ....: );\"\"\"In [44]: con = sqlite3.connect(':memory:')In [45]: con.execute(query)Out[45]: &lt;sqlite3.Cursor at 0x104321030&gt;In [46]: con.commit()In [47]: data = [('Atlanta','Georgia',1.25,6), ....: ('Tallahassee','Florida',2.6,3), ....: ('Sacramento','California',1.7,5)]In [48]: stmt = \"INSERT INTO test VALUES(?,?,?,?)\"In [49]: con.executemany(stmt,data)Out[49]: &lt;sqlite3.Cursor at 0x104316260&gt;In [50]: con.commit() 从表中选取数据时，大部分 Python SQL 驱动器（PyDBC、psycopg2、MySQLdb、pymssql等）都会返回一个元组列表123456789101112131415161718192021222324In [51]: cursor = con.execute('select * from test')In [52]: rows = cursor.fetchall()In [53]: rowsOut[53]: [(u'Atlanta', u'Georgia', 1.25, 6), (u'Tallahassee', u'Florida', 2.6, 3), (u'Sacramento', u'California', 1.7, 5)] 我们可以将这个元组列表传给 DataFrame 的构造器，但还需要列名（位于游标的 `description` 属性中）In [55]: cursor.descriptionOut[55]: (('a', None, None, None, None, None, None), ('b', None, None, None, None, None, None), ('c', None, None, None, None, None, None), ('d', None, None, None, None, None, None))`In [56]: DataFrame(rows,columns = zip(*cursor.description)[0])Out[56]: a b c d0 Atlanta Georgia 1.25 61 Tallahassee Florida 2.60 32 Sacramento California 1.70 5 这种数据规整操作相当多，你肯定不想每查一次数据库就重写一次。pandas 有一个可以简化该过程的 read_sql 函数（位于 pandas.io.sql 模块）。只需传入 select 语句和连接对象即可12345678In [61]: import pandas.io.sql as sqlIn [62]: sql.read_sql('select * from test',con)Out[62]: a b c d0 Atlanta Georgia 1.25 61 Tallahassee Florida 2.60 32 Sacramento California 1.70 5 三、存取 MongoDB 中的数据 noSQL 数据库有许多不同的形式。有些是简单的字典式键值对存储（如 BerkeleyDB 和 Tokyo Cabinet），另一些则是基于文档的（其中的基本单元是字典型的对象）。本例选用得是 MongoDB。我们先在自己的电脑上启动一个 MongoDB 实例，然后用 pymongo（MongoDB 的官方驱动器）通过默认端口进行连接12In [34]: import pymongoIn [35]: con = pymongo.MongoClient('localhost',port = 27017) 存储在 MongoDB 中的文档被组织在数据库的集合（collection）中。MongoDB 服务器的每个运行实例可以有多个数据库，而每个数据库又可以有多个集合。假设你想保存之前通过 GeoNames API 获取的数据。首先我们可以访问 citys 集合（暂时还是空的）1In [39]: citys = con.db.citys 然后，我将那组 city 加载进来并通过 citys.save（用于将 Python 字典写入 MongoDB）逐个存入集合中：12345678In [40]: import requests,jsonIn [41]: url = 'http://api.geonames.org/citiesJSON?north=44.1&amp;south=-9.9&amp;east=-22.4&amp;west=55.2&amp;lang=de&amp;username=demo'In [42]: data = json.loads(requests.get(url).text)In [43]: for city in data['geonames']: ....: citys.save(city) 现在，如果我想从该集合中取出我自已想要的 city（如果有的话），可以用下面的代码对集合进行查询： 1In [55]: cursor = citys.find(&#123;'name': 'Hong Kong'&#125;) 返回的游标是一个迭代器，它可以为每个文档产生一个字典。跟之前一样，我们可以将其转换为一个 DataFrame。此外还可以只获取各 city 的部分字段：12345678In [56]: city_fields = ['countrycode', 'name', 'population']In [57]: result = DataFrame(list(cursor),columns=city_fields)In [58]: resultOut[58]: countrycode name population0 HK Hong Kong 7012738","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-I-读写文本数据","date":"2016-05-23T14:52:58.763Z","updated":"2016-06-19T07:07:16.748Z","comments":true,"path":"2016/05/23/PANDAS常用手册-I-读写文本数据/","link":"","permalink":"http://yoursite.com/2016/05/23/PANDAS常用手册-I-读写文本数据/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 本实验将学习逐块读取文本文件、将数据写出到文本格式以及手工处理分隔符格式等知识。 输入输出通常可以划分为几个大类：读取文本文件和其他更高效的磁盘存储格式，加载数据库中的数据，利用 Web API 操作网络资源 二、逐块读取文本文件由于本节实验需要使用到python的lxml库，所以先在环境中安装： pip install python-lxml Python 在文本和文件的处理以其简单的文本交互语法、直观的数据结构，以及诸如元组打包解包之类的便利功能，深受人们喜爱。 pandas 提供了一些用于将表格型数据读取为 DataFrame 对象的函数。 函数 说明 read_csv 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号 read_table 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符\\t read_fwf 读取定宽列格式数据（也就是说，没有分隔符） read_clipboard 读取剪贴板中的数据，可以看做 read_table 的剪贴板版。在将网页转换为表格时很有用 这些函数的选项可以划分为以下几大类： 索引：将一个或多个列当做返回的 DataFrame 处理，以及是否从文件、用户获取列名 类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等 日期解析：包括组合功，比如将分散在多个列中的日期时间信息组合成结果中的单个列 迭代：支持对大文件进行逐块迭代 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西（比如由成千上万个逗号隔开的数值数据） 类型推断（typeinference）是这些函数中最重要的功能之一，我们不需要指定列的类型到底是数值、整数、布尔值，还是字符串。日期和其他自定义类型的处理需要多花点功夫才行。 首先我们来看一个以逗号分隔（我这里用的是 CSV 文件，同学们用 txt 文件是一样的）的文本文件：1234567891011121314In [11]: !cat test.csva,b,c,d,message1,2,3,4,hello5,6,7,8,world9,10,11,12,infoIn [12]: data = pd.read_csv('test.csv')In [13]: dataOut[13]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 info 我们也可以用read_table，只不过需要指定分隔符而已：123456In [17]: data = pd.read_table('.test.csv',sep=',')Out[17]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 info 在上面的例子中，我们在文本文件中写了标题行（即 message 那一行），但是不是所有文件都会有标题行，读入这种文件有两个办法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364In [19]: !cat test.csv1,2,3,4,hello5,6,7,8,world9,10,11,12,infoIn [20]: # 我们可以让 PANDAS 为其分配默认的列明，也可以自己定义列明In [21]: pd.read_csv('test.csv',header=None)Out[21]: 0 1 2 3 40 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 infoIn [22]:pd.read_csv('test.csv',names=['a','b','d','message'])Out[22]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 infoIn [23]: #假设我们希望 message 列作为 DataFrame 的索引In [24]: #我们可以通过 index_col 参数来指定In [25]: name = ['a','b','c','d','message']In [26]: pd.read_csv('test.csv',names = name, index_col = 'message')Out[26]: message a b c d hello 1 2 3 4 world 5 6 7 8 info 9 10 11 12 In [27]: !cat test2.csvkey1,key2,value1,value2one,a,1,2one,b,3,4one,c,5,6one,d,7,8two,a,9,10two,b,11,12In [28]: parsed = pd.read_csv('test2.csv',index_col =['key1','key2'])In [28]:parsedOut[28]: value1 value2key1 key2 one a 1 2 b 3 4 c 5 6 d 7 8two a 9 10 b 11 12In [30]:pd.read_csv('test2.csv')Out[30]: key1 key2 value1 value20 one a 1 21 one b 3 42 one c 5 63 one d 7 84 two a 9 105 two b 11 12In [31]: #我们可以传入列编号或者列名实现一次层次化索引 层次化索引 有些表格可能不是用固定的分隔符去分隔字段的（比如空白符或其他字符串）。对于这种情况，我们可以编写一个正则表达式来作为 read_table 的分隔符。 正则表达式 上面的文本中 read_table发现列名的数量比列的数量少1，于是推断第一列应该是 DataFrame 的索引。 我们现在就来看一看 read_csv/read_table 函数的参数:|参数| 说明|| ————- |:————-||path |表示文件系统位置、URL、文件型对象的字符串||sep/delimiter |用于对行中各字段进行拆分的字符序列或正则表达式||header |用作列名的行号。默认为0（第一行），如果没有 header 行就应该设置为 None||index_col |用作行索引的列编号或列名。可以是单个名称/数字或由多个名称/数字组成的列表（层次化索引）||names| 用于结果的列名列表，结合 header= None||skiprows |需要忽略的行数（从文件开始处算起），或需要跳过的行号列表（从0开始）||na_values |一组用于替换 NA 的值||comment |用于将注释信息从行尾拆分出去的字符（一个或多个）|parse_datas |尝试将数据解析为日期，默认为 False，如果为 True，则尝试解析所有列。此外，还可以指定需要解析的一组列号或列名。如果列表的元素为列表或元组，就会将多个列组合到一起再进行日期解析工作|keep_data_col |如果连接多列解析日期，则保持参与连接的列。默认为 False|converters |由列号/列名跟函数之间的映射关系组成的字典。例如，{‘foo’ : f}会对 foo 列的所有值应用函数 f|dayfirst |当解析有歧义的日期时，将其看做国际格式（例如，7/6/2012 → June 7,2012）。默认为 False|data_parser |用于解析日期的函数|nrows |需要读取的行数（从文件开始处算起）|iterator |返回一个 TextParser 以便逐块读取文件|chunksize |文件块的大小（用于迭代）|skip_footer |需要忽略的行数（从文件末尾处算起）|verbose |打印各种解析器输出信息，比如“非数值列中缺失值的数量”等||encoding |用于 unicode 的文本编码格式||squeeze |如果数据经解析后仅含一列，则返回 Series||thousand |千分位分隔符，如,或者.| 在处理很大文件或找出大文件中的参数集以便后续处理时，我们可能只想读取文件的一小部分或逐块对文件进行迭代。 写入数据 规定 nrows 逐块读 三、手工处理分隔符格式 大部分存储在磁盘上的表格型数据都能用 pandas.read_table进行加载。然而，有时还是需要做一些手工处理。由于接收到含有畸形行的文件而使用 read_table 出毛病的情况并不少见。接下来我们来看一个例子： 手工处理 CSV文件的形式有很多。只需定义 csv.Dialect 的一个子类即可定义出新格式（如专门的分隔符、字符串引用约定、行结束符等）1234567891011121314151617In [61]: class my_dialect(csv.Dialect): ....: lineterminator = '\\n' ....: delimiter = ';' ....: quotechar = '\"' ....: In [62]: reader = csv.reader(f, dialect=my_dialect, quoting = csv.QUOTE_ALL)In [63]: readerOut[63]: &lt;_csv.reader at 0x1074c29f0&gt;In [64]: with open('mydata.csv','w') as f: ....: writer = csv.writer(f,dialect = my_dialect,quoting = csv.QUOTE_ALL) ....: writer.writerow(('one','two','three')) ....: writer.writerow(('1','2','3')) ....: writer.writerow(('4','5','6')) ....: writer.writerow(('7','8','9')) ....: 四、JSON 数据 JSON（JavaScript Object Notation 的简称）已经成为通过 HTTP 请求在 Web 浏览器和其他应用程序之间发送数据的标准格式之一。它是一种比表格型文本格式（如 CSV）灵活得多的数据格式。如： 12345678obj = \"\"\" &#123; \"name\":\"Limei\", \"places_lived\":[\"China\",\"UK\",\"Germany\"], \"pet\":null, \"siblings\":[&#123;\"name\":\"Liming\",\"age\":23,\"pet\":\"Xiaobai\"&#125;, &#123;\"name\":\"Lifang\",\"age\":33,\"pet\":\"Xiaohei\"&#125;] &#125; \"\"\" 除其空值 null 和一些其他的细微差别（如列表末尾不允许存在多余的逗号）之外，JSON 非常接近于有效的 Python 代码。基本类型有对象（字典）、数组（列表）、字符串、数值、布尔值以及 null。对象中所有的键都必须是字符串。许多 Python 库都可以读写 JSON 数据。我们将使用 json，因为它是构建与 Python 标准库中的。通过 json.loads 即可将 JSON 字符串转换成 Python 形式 1234567891011In [70]: import jsonIn [71]: result = json.loads(obj)In [72]: resultOut[72]: &#123;u'name': u'Limei', u'pet': None, u'places_lived': [u'China', u'UK', u'Germany'], u'siblings': [&#123;u'age': 23, u'name': u'Liming', u'pet': u'Xiaobai'&#125;, &#123;u'age': 33, u'name': u'Lifang', u'pet': u'Xiaohei'&#125;]&#125; 相反，json.dumps则将 Python 对象转换成 JSON 格式： 1234In [78]: asjson = json.dumps(result)In [79]: asjsonOut[79]: '&#123;\"pet\": null, \"siblings\": [&#123;\"pet\": \"Xiaobai\", \"age\": 23, \"name\": \"Liming\"&#125;, &#123;\"pet\": \"Xiaohei\", \"age\": 33, \"name\": \"Lifang\"&#125;], \"name\": \"Limei\", \"places_lived\": [\"China\", \"UK\", \"Germany\"]&#125;' 如何将（一个或一组）JSON 对象转换为DataFrame 或其他便于分析的数据结构呢？最简单方便的方式是：向 DataFrame 构造器传入一组 JSON 对象，并选取数据字段的子集。 1234567In [80]: siblings = DataFrame(result['siblings'],columns = ['name','age'])In [81]: siblingsOut[81]: name age0 Liming 231 Lifang 33 五、XML 和 HTML Python 有许多可以读写 HTML 和 XML 格式数据的库。lxml 就是其中之一，它能够高效且可靠地解析大文件。lxml 有多个编程接口。首先我要用 lxml.html 处理 HTML，然后再用 lxml.objectify 做一些 XML 处理。 许多网站都将数据放到 HTML 表格中以便在浏览器中查看，但不能以一种更易于机器阅读的格式（如 JSON、HTML 或 XML）进行下载 首先，找到我们希望获取数据的 URL，利用 urllib2 将其打开，然后用 lxml 解析得到的数据流，如下所示： 1234567In [4]: from lxml.html import parseIn [5]: from urllib2 import urlopenIn [6]: parsed = parse(urlopen('http://finance.yahoo.com/q/hp?s=AAPL+Historical+Prices'))In [7]: doc = parsed.getroot() 通过这个对象，我们可以获取特定类型的所有 HTML 标签，比如含有所需数据的 table 标签。给这个简单的例子加点启发性，假设我们想得到该文档中所有的 URL 链接。HTML 中的链接是a标签。使用文档根节点的 findall 方法以及一个 XPath（对文档的“查询”的一种表示手段）： 1234567links = doc.findall('.//a')In [9]: links[:3]Out[9]: [&lt;Element a at 0x10db065d0&gt;, &lt;Element a at 0x10db06628&gt;, &lt;Element a at 0x10db06680&gt;] 但是这些是表示 HTML 元素的对象。要得到 URL 和链接文本，你必须使用各对象的 get 方法（针对 URL）和 text_content 方法（针对显示文本）：12345678910In [11]: lnk = links[5]In [12]: lnkOut[12]: &lt;Element a at 0x10db06788&gt;`In [13]: lnk.get('href')Out[13]: 'http://finance.yahoo.com/'In [14]: lnk.text_content()Out[14]: 'Finance' 因此编写下面这条列表推导式即可获取文档中的全部 URL：1234567In [16]: urls = [lnk.get('href') for lnk in doc.findall('.//a')]In [17]: urls[:3]Out[17]: ['https://www.yahoo.com/', 'https://mail.yahoo.com/?.intl=us&amp;.lang=en-US&amp;.src=ym', 'https://search.yahoo.com/search'] 现在，从文档中找出正确表格的办法就是反复试验了。有些网站会给目标表格加上一个id属性。试验了好久，终于在这个网站上找到了我们需要的表格：1234567891011121314151617181920212223242526272829303132333435363738In [70]: tables = doc.findall('.//table')In [121]: #从70行试到了121行也是不容易的啊In [122]: calls = tables[13]In [123]: puts = tables[14]In [124]: from pandas.io.parsers import TextParserIn [125]: #由于数值型数据任然是字符串格式In [126]: #所以我们希望将部分列转换为浮点数格式In [127]: #pandas 恰好就有一个 TextParser 类In [128]: #用于自动类型转换In [129]: def parse_options_data(table): .....: rows = table.findall('.//tr') .....: #每个表格都有一个标题行，然后才是数据行 .....: header = _unpack(rows[0],kind='th') .....: #对于标题行，就是 th 单元格 .....: #数据行，就是 td 单元格 .....: data = [_unpack(r) for r in rows[1:]] .....: return TextParser(data,names=header).get_chunk()In [130]: call_data = parse_options_data(calls)In [131]: put_data = parse_options_data(puts)In [132]: call_data[:3]Out[132]: Date Open High Low Close Volume \\0 Jul 29, 2015 123.15 123.50 122.27 122.99 35,914,200 1 Jul 28, 2015 123.38 123.91 122.55 123.38 33,448,900 2 Jul 27, 2015 123.09 123.61 122.12 122.77 44,274,800 Adj Close*\\n 0 122.99 1 123.38 2 122.77 XML 是另一种常见的支持分层、嵌套数据以及元数据的结构化数据格式。现在我们来学习另一种用于操作 XML 数据的接口，即 lxml.objectify 这里m 给同学们一个 xml 文件的内容，同学们用文本编译器生成一个 xml 文件即可：123456789101112131415161718192021`&lt;INDICATOR&gt; &lt;INDICATOR_SEQ&gt;373889&lt;/INDICATOR_SEQ&gt; &lt;PARENT_SEQ&gt;&lt;/PARENT_SEQ&gt; &lt;AGENCY_NAME&gt;Metro-North Railroad&lt;/AGENCY_NAME&gt; &lt;INDICATOR_NAME&gt;Escalator Availability&lt;/INDICATOR_NAME&gt; &lt;DESCRIPTION&gt;Percent of the time that escalators are operational` `systemwide. The availability rate is based on physical observations performed the morning of regular business days only. This is a new indicator the agency began reporting in 2009.&lt;/DESCRIPTION&gt; &lt;PERIOD_YEAR&gt;2011&lt;/PERIOD_YEAR&gt; &lt;PERIOD_MONTH&gt;12&lt;/PERIOD_MONTH&gt; &lt;CATEGORY&gt;Service Indicators&lt;/CATEGORY&gt; &lt;FREQUENCY&gt;M&lt;/FREQUENCY&gt; &lt;DESIRED_CHANGE&gt;U&lt;/DESIRED_CHANGE&gt; &lt;INDICATOR_UNIT&gt;%&lt;/INDICATOR_UNIT&gt; &lt;DECIMAL_PLACES&gt;1&lt;/DECIMAL_PLACES&gt; &lt;YTD_TARGET&gt;97.00&lt;/YTD_TARGET&gt; &lt;YTD_ACTUAL&gt;&lt;/YTD_ACTUAL&gt; &lt;MONTHLY_TARGET&gt;97.00&lt;/MONTHLY_TARGET&gt; &lt;MONTHLY_ACTUAL&gt;&lt;/MONTHLY_ACTUAL&gt; &lt;/INDICATOR&gt;` 好了，接下来我们就进入 lxml.objectify 的学习吧123456789101112131415161718192021222324252627282930313233343536373839404142434445464748In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用：In [4]: from lxml import objectifyIn [5]: path = 'Performance_MNR.xml'In [6]: parsed = objectify.parse(open(path))In [7]: root = parsed.getroot()In [3]: # 我们先用 lxml.objectify 解析该文件，然后通过getroot得到该XML文件的根节点的引用：In [4]: from lxml import objectifyIn [5]: path = 'Performance_MNR.xml'In [6]: parsed = objectify.parse(open(path))In [7]: root = parsed.getroot()In [8]: data = []In [9]: skip_fields = ['PARENT_SEQ''PARENT_SEQ', INDICATOR_SEQ', 'DESIRED_CHANGE', 'DECIMAL_PLACES']In [10]: for elt in root: ....: el_data = &#123;&#125; ....: for child in elt.getchildren(): ....: if child.tag in skip_fields: ....: continue ....: el_data[child.tag] = child.pyval ....: data.append(el_data)In [11]: dataOut[11]: [&#123;'AGENCY_NAME': 'Metro-North Railroad', 'CATEGORY': 'Service Indicators', 'DESCRIPTION': 'Percent of the time that escalators are operational \\nsystemwide. The availability rate is based on physical observations performed \\nthe morning of regular business days only. This is a new indicator the agency \\nbegan reporting in 2009.', 'FREQUENCY': 'M', 'INDICATOR_NAME': 'Escalator Availability', 'INDICATOR_UNIT': '%', 'MONTHLY_ACTUAL': u'', 'MONTHLY_TARGET': 97.0, 'PARENT_SEQ': u'', 'PERIOD_MONTH': 12, 'PERIOD_YEAR': 2011, 'YTD_ACTUAL': u'', 'YTD_TARGET': 97.0&#125;]","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"PANDAS常用手册 V -- 重塑和轴向旋转","slug":"PANDAS常用手册-II-使用-HTML-和-Web-API","date":"2016-05-18T15:40:15.569Z","updated":"2016-06-19T07:13:36.791Z","comments":true,"path":"2016/05/18/PANDAS常用手册-II-使用-HTML-和-Web-API/","link":"","permalink":"http://yoursite.com/2016/05/18/PANDAS常用手册-II-使用-HTML-和-Web-API/","excerpt":"","text":"转载自实验楼作者 木木同学 的课程[《PYTHON数据分析》][1] 一、实验简介 今天的课程比较简单，主要是对上一节课的一些补充。许多网站都有一些通过 JSON 或其他格式提供数据的公共 API。通过 Python 访问这些 API 的办法不少。一个简单易用的办法是 request 包。 二、使用 HTML 和 Web API GeoNames是一个免费的全球地理数据库。我们可以发送一个 HTTP GET 请求（关于Web API的数据下载，需要在联网情况才能进行。成功购买实验楼会员服务的用户，便可直接在环境中链接外网），如下所示：12345678In [15]: import requestsIn [16]: url = 'http://api.geonames.org/citiesJSON?north=44.1&amp;south=-9.9&amp;east=-22.4&amp;west=55.2&amp;lang=de&amp;username=demo'In [17]: resp = requests.get(url)In [18]: respOut[18]: &lt;Response [200]&gt; Response 对象的 text 属性含有 GET 请求的内容。许多 Web API 返回的都是 JSON 字符串，我们必须将其加载到一个 Python 对象中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146In [19]: import jsonIn [20]: data = json.loads(resp.text)In [21]: data.keys()Out[21]: [u'geonames']In [22]: dataOut[22]: &#123;u'geonames': [&#123;u'countrycode': u'MX', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 3530597, u'lat': 19.428472427036, u'lng': -99.12766456604, u'name': u'Mexiko-Stadt', u'population': 12294193, u'toponymName': u'Mexico City', u'wikipedia': u'en.wikipedia.org/wiki/Mexico_City'&#125;, &#123;u'countrycode': u'CN', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1816670, u'lat': 39.9074977414405, u'lng': 116.397228240967, u'name': u'Peking', u'population': 11716620, u'toponymName': u'Beijing', u'wikipedia': u'en.wikipedia.org/wiki/Beijing'&#125;, &#123;u'countrycode': u'PH', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1701668, u'lat': 14.6042, u'lng': 120.9822, u'name': u'Manila', u'population': 10444527, u'toponymName': u'Manila', u'wikipedia': u'en.wikipedia.org/wiki/Manila'&#125;, &#123;u'countrycode': u'BD', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1185241, u'lat': 23.710395616597037, u'lng': 90.40743827819824, u'name': u'Dhaka', u'population': 10356500, u'toponymName': u'Dhaka', u'wikipedia': u'en.wikipedia.org/wiki/Dhaka'&#125;, &#123;u'countrycode': u'KR', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1835848, u'lat': 37.566, u'lng': 126.9784, u'name': u'Seoul', u'population': 10349312, u'toponymName': u'Seoul', u'wikipedia': u'en.wikipedia.org/wiki/Seoul'&#125;, &#123;u'countrycode': u'ID', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1642911, u'lat': -6.214623197035775, u'lng': 106.84513092041016, u'name': u'Jakarta', u'population': 8540121, u'toponymName': u'Jakarta', u'wikipedia': u'en.wikipedia.org/wiki/Jakarta'&#125;, &#123;u'countrycode': u'JP', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1850147, u'lat': 35.6895, u'lng': 139.69171, u'name': u'Tokio', u'population': 8336599, u'toponymName': u'Tokyo', u'wikipedia': u'de.wikipedia.org/wiki/Tokyo'&#125;, &#123;u'countrycode': u'TW', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1668341, u'lat': 25.047763, u'lng': 121.531846, u'name': u'Taipeh', u'population': 7871900, u'toponymName': u'Taipei', u'wikipedia': u'de.wikipedia.org/wiki/Taipei'&#125;, &#123;u'countrycode': u'CO', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 3688689, u'lat': 4.609705849789108, u'lng': -74.08175468444824, u'name': u'Bogot\\xe1', u'population': 7674366, u'toponymName': u'Bogot\\xe1', u'wikipedia': u'en.wikipedia.org/wiki/Bogot%C3%A1'&#125;, &#123;u'countrycode': u'HK', u'fcl': u'P', u'fclName': u'city, village,...', u'fcode': u'PPLC', u'fcodeName': u'capital of a political entity', u'geonameId': 1819729, u'lat': 22.2855225817732, u'lng': 114.157691001892, u'name': u'Hong Kong', u'population': 7012738, u'toponymName': u'Hong Kong', u'wikipedia': u'en.wikipedia.org/wiki/Hong_Kong'&#125;]&#125;In [23]: city_fields = ['countrycode','name','fcode','population']In [24]: citys = DataFrame(data['geonames'],columns=city_fields)In [25]: citysOut[25]: countrycode name fcode population0 MX Mexiko-Stadt PPLC 122941931 CN Peking PPLC 117166202 PH Manila PPLC 104445273 BD Dhaka PPLC 103565004 KR Seoul PPLC 103493125 ID Jakarta PPLC 85401216 JP Tokio PPLC 83365997 TW Taipeh PPLC 78719008 CO Bogotá PPLC 76743669 HK Hong Kong PPLC 7012738 现在，DataFrame 中的每一行就有了来自 GeoNames 的数据：1234567In [26]: citys.ix[5]Out[26]: countrycode IDname Jakartafcode PPLCpopulation 8540121Name: 5, dtype: object 要想能够直接得到便于分析的 DataFrame 对象，只需要再多费些精力创建出对常见 Web API 的更高级接口即可。","categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://yoursite.com/categories/数据处理/"},{"name":"PYTHON","slug":"数据处理/PYTHON","permalink":"http://yoursite.com/categories/数据处理/PYTHON/"}],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2016-04-12T11:46:46.162Z","updated":"2016-05-24T14:32:07.913Z","comments":true,"path":"2016/04/12/hello-world/","link":"","permalink":"http://yoursite.com/2016/04/12/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}